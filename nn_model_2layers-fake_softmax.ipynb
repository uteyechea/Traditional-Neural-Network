{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn_model-2layers",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPzYAV22R2GdlVBWeols5KX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uteyechea/neural-network-from-scratch/blob/main/nn_model_2layers-fake_softmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNNViSwxzY6L"
      },
      "source": [
        "#Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJAruNzIzdmQ"
      },
      "source": [
        "##1.1 Import all necessary dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wcjk1FvAKALE",
        "outputId": "d6bdeff0-0743-4085-ca62-dfd726b6df46"
      },
      "source": [
        "!pip install scikit-multilearn #Data split train/test sets"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.6/dist-packages (0.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yFTkYMizUDp"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from skmultilearn.model_selection import iterative_train_test_split"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1M8Zj3Z0Bi3"
      },
      "source": [
        "##1.2 Prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPn7AM_Iz3nj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "outputId": "4c382cf2-6290-4a10-8968-f7167f504d39"
      },
      "source": [
        "#Binary classifier toy data, could be XOR, AND, OR, ...\n",
        "\n",
        "# Three training examples with two features x1 and x2.\n",
        "X=np.array([\n",
        "    [0,0,1,1],\n",
        "    [0,1,0,1]\n",
        "])\n",
        "\n",
        "print('# Training examples, m: {}'.format(X.shape[1]))\n",
        "print('# of features, n_x: {}'.format(X.shape[0]))\n",
        "#Labels for each of the three training examples.\n",
        "Y=np.array([\n",
        "            [1,0,0,1],\n",
        "])\n",
        "\n",
        "# Visualize the data\n",
        "fig=go.Figure(data=go.Scatter(x=X[0,:],\n",
        "                                y=X[1,:],\n",
        "                                mode='markers',\n",
        "                                marker_color=Y[0,:],\n",
        "                                marker_size=25,\n",
        "                                text=Y[0,:]))\n",
        "fig.update_layout(title='Binary classification training samples')\n",
        "fig.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Training examples, m: 4\n",
            "# of features, n_x: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"af1f6066-9948-4cfa-9ac9-a01d4a7eca14\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"af1f6066-9948-4cfa-9ac9-a01d4a7eca14\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'af1f6066-9948-4cfa-9ac9-a01d4a7eca14',\n",
              "                        [{\"marker\": {\"color\": [1, 0, 0, 1], \"size\": 25}, \"mode\": \"markers\", \"text\": [1.0, 0.0, 0.0, 1.0], \"type\": \"scatter\", \"x\": [0, 0, 1, 1], \"y\": [0, 1, 0, 1]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Binary classification training samples\"}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('af1f6066-9948-4cfa-9ac9-a01d4a7eca14');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVVmrvJhI6ki"
      },
      "source": [
        "## 1.3 ECG data\n",
        "1.   Download data\n",
        "2.   Split data into train and test sets\n",
        "3.   Verify balanced distribution of data labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffVnDPZzJm2t"
      },
      "source": [
        "### 1.3.1 Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8txPZk3I54B"
      },
      "source": [
        "# Download ECG data\n",
        "data_url='https://github.com/uteyechea/neural-network-from-scratch/blob/main/ecg_data/ecg.csv'\n",
        "\n",
        "def read_cvs_with_html_tags(data_url): \n",
        "  # This function only works for my very peculiar data structure.\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  data_url -- csv file url\n",
        "\n",
        "  Returns:\n",
        "  X -- Training data\n",
        "  y -- Labels\n",
        "  \"\"\" \n",
        "  raw_data=pd.read_html(data_url) # Looking for <table> tag\n",
        "  unformatted_data=raw_data[0][1].str.split(\";\",expand=True) #read_html return need a little work\n",
        "  #By now you have a standard pandas DataFrame, but still needs some work...\n",
        "  #make first row new header, (optional, depending on your csv file)\n",
        "  new_header=unformatted_data.iloc[0,:]\n",
        "  data=unformatted_data[1:]\n",
        "  data.columns=new_header\n",
        "\n",
        "  X=data.iloc[:,0:-5]\n",
        "  y=data.loc[:,'Clase']\n",
        "  y=y.values.reshape(y.shape[0],1)\n",
        "\n",
        "  X=np.array(X)\n",
        "  y=np.array(y)\n",
        "\n",
        "  X=X.astype(np.float)\n",
        "  y=y.astype(np.float)\n",
        "\n",
        "  y=y/max(y)-(1/max(y))\n",
        "\n",
        "  return X,y"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wQ7aHfHJu14"
      },
      "source": [
        "###1.3.2 Split train/test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy9qCHXf46Eq"
      },
      "source": [
        "#data_url='https://github.com/uteyechea/neural-network-from-scratch/blob/main/ecg_data/ecg.csv'\n",
        "#X,y=read_cvs_with_html_tags(data_url)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50glSHoo40bw"
      },
      "source": [
        "#X_train, y_train, X_test, y_test = iterative_train_test_split(X, y, test_size = 0.5)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXhmSfu2KVd3"
      },
      "source": [
        "###1.3.3 Verify balanced distribution of labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2A4D8tcKd1_"
      },
      "source": [
        "def count_train_test_split(y_test,y_train):\n",
        "  #Assumed y_train, y_test data shape (n,1)\n",
        "\n",
        "  y_test_distribution={}\n",
        "  y_train_distribution={}\n",
        "\n",
        "  for label in np.unique(y_test):\n",
        "    y_test_distribution['class='+str(label)]= np.count_nonzero(y_test == label, axis=0)\n",
        "  \n",
        "  for label in np.unique(y_train):\n",
        "    y_train_distribution['class='+str(label)]= np.count_nonzero(y_train == label, axis=0)\n",
        "  \n",
        "  return y_train_distribution,y_test_distribution"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx91sPitKuvE"
      },
      "source": [
        "###1.3.4 Get data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATIYMakeGfPX"
      },
      "source": [
        "def get_ecg_data():\n",
        "  \"\"\"\n",
        "  Agruments(None):\n",
        "  data_url -- csv file location\n",
        "  test_size -- Test/train data split, ex. test_size=0.7 distributes 70% to the test set and 30% to the training set. \n",
        "  Returns:\n",
        "  X_train -- train data features\n",
        "  y_train -- train data labels\n",
        "  X_test -- test data features\n",
        "  y_test -- test data labels\n",
        "  \"\"\"\n",
        "  data_url='https://github.com/uteyechea/neural-network-from-scratch/blob/main/ecg_data/ecg.csv'\n",
        "  X,y=read_cvs_with_html_tags(data_url)\n",
        "\n",
        "  print(y.shape)\n",
        "  X_train, y_train, X_test, y_test = iterative_train_test_split(X, y, test_size = 0.5) \n",
        "  y_train_distribution,y_test_distribution=count_train_test_split(y_test,y_train)\n",
        "  print(y_train_distribution)\n",
        "  print(y_test_distribution)\n",
        "\n",
        "  #Reshape data to fit expected data structure for nn model\n",
        "  X_train=X_train.reshape(X_train.shape[1],X_train.shape[0])\n",
        "  X_test=X_test.reshape(X_test.shape[1],X_test.shape[0])\n",
        "  y_train=y_train.reshape(y_train.shape[1],y_train.shape[0])\n",
        "  y_test=y_test.reshape(y_test.shape[1],y_test.shape[0])\n",
        "\n",
        "  N=len(np.unique(y_test))\n",
        "  print('distinct labels: ',N )\n",
        "\n",
        "  return X_train, y_train, X_test, y_test,N"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpue8bIELNTb",
        "outputId": "23c3269c-60f9-4c94-9d9a-f1bad7f95a5e"
      },
      "source": [
        "X_train, y_train, X_test, y_test,N = get_ecg_data()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(460, 1)\n",
            "{'class=0.0': array([96]), 'class=0.25': array([46]), 'class=0.5': array([46]), 'class=0.75': array([42])}\n",
            "{'class=0.0': array([96]), 'class=0.25': array([45]), 'class=0.5': array([46]), 'class=0.75': array([43])}\n",
            "distinct labels:  4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwDQzp1XRZIo",
        "outputId": "21576b9d-13a4-40a1-9cec-7360384d16c7"
      },
      "source": [
        "print(y_test.shape)\n",
        "print(X_train.shape)\n",
        "print('# Training examples, m: {}'.format(X_train.shape[1]))\n",
        "print('# of features, n_x: {}'.format(X_train.shape[0]))\n",
        "\n",
        "\n",
        "print(np.sum(X_train),\n",
        "np.sum(X_test),\n",
        "np.sum(y_train),\n",
        "np.sum(y_test)\n",
        ")\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 230)\n",
            "(39, 230)\n",
            "# Training examples, m: 230\n",
            "# of features, n_x: 39\n",
            "3826.0886028808795 3638.479131882022 66.0 66.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyCx5hQ14RxH"
      },
      "source": [
        "#Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z8cHU0m5bYC"
      },
      "source": [
        "#2.1 Define layer sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr84nZec2Dj9"
      },
      "source": [
        "def layer_sizes(X, Y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- input dataset of shape (number of features, number of training examples)\n",
        "    Y -- labels of shape (output size, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    n_x -- the size of the input layer, i. e., number of features\n",
        "    n_h -- the size of the hidden layer, i. e., number of neurons\n",
        "    n_y -- the size of the output layer, i. e., label\n",
        "    \"\"\"\n",
        "    \n",
        "    n_x = X.shape[0] # size of input layer, i. e. number of features\n",
        "    n_h = 3 # user defined number of neurons inside the one and only layer\n",
        "    n_y = Y.shape[0] # size of output layer, i. e. number of possible labels\n",
        "    \n",
        "    return (n_x, n_h, n_y)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ77xQR2NWJ6"
      },
      "source": [
        "#2.2 Initialize model parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz2Ok1FCKWD9"
      },
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    n_x -- size of the input layer, i. e., number of features\n",
        "    n_h -- size of the hidden layer, i. e., number of neurons\n",
        "    n_y -- size of the output layer, i. e., label\n",
        "    \n",
        "    Returns:\n",
        "    params -- python dictionary containing your parameters:\n",
        "                    W1 -- weight matrix of shape (n_h, n_x)\n",
        "                    b1 -- bias vector of shape (n_h, 1)\n",
        "                    W2 -- weight matrix of shape (n_y, n_h) #if Second layer is the output ot the NN.\n",
        "                    b2 -- bias vector of shape (n_y, 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    W1 = np.random.randn(n_h,n_x) * np.sqrt(1./ (n_x))#0.01 it is usually recommended to start with small weights, as long as they are different than zero\n",
        "    b1 = np.zeros((n_h,1))\n",
        "    W2 = np.random.randn(n_y,n_h) * np.sqrt(1./ (n_h))\n",
        "    b2 = np.zeros((n_y,1))\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_dJIXaVOF7u"
      },
      "source": [
        "#2.3 Implement forward propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98wNSc_mNjcA"
      },
      "source": [
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    X -- input data of size (n_x, m) #where m is the number of training examples\n",
        "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
        "    \n",
        "    Returns:\n",
        "    A2 -- The sigmoid output of the second activation\n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    \n",
        "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
        "    Z1 = W1 @ X + b1 #(n_h,n_x) @ (n_x,m) + (n_h,1) = (n_h,m)\n",
        "    A1 = np.tanh(Z1) #(n_h,m) selected activation function for the hidden layer neurons\n",
        "    Z2 = W2 @ A1 + b2 \n",
        "    A2 = 1.0 / (1.0 + np.exp(-Z2)) #NN output, later we will round this value to 0/1, thus the sigmoid function fits the expected output map just fine\n",
        "    \n",
        "    cache = {\"Z1\": Z1,\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "    \n",
        "    return A2, cache"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b0LKy91POdl"
      },
      "source": [
        "### 2.2.1 Compute Cost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV669td9PECH"
      },
      "source": [
        "def compute_cost(A2, Y):\n",
        "    \"\"\"\n",
        "    Computes the cross-entropy cost. It is prefered to use this cost function given it is stricly convex \n",
        "    \n",
        "    Arguments:\n",
        "    A2 -- The sigmoid output of the second activation, of shape (output size, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (output size, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1] # get the number of training examples\n",
        "\n",
        "    # Compute the cross-entropy cost\n",
        "    logprobs = Y*np.log(A2) + (1.0-Y) * np.log(1.0-A2) # (n_y,m) * (n_h,m) + ... = (n_h,m)\n",
        "    cost = -(1/m)*np.sum(logprobs,axis=1,keepdims=True) # sum the cost over all training examples\n",
        "    \n",
        "    #cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect                               \n",
        "    #assert(isinstance(cost, float))\n",
        "\n",
        "    return cost"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KFaemZu-Zl7"
      },
      "source": [
        "#Part 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiUi3FriP-V5"
      },
      "source": [
        "#3.1 Implement Backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EEGh6F1P4g8"
      },
      "source": [
        "def backward_propagation(parameters, cache, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing our parameters \n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
        "    X -- input data of shape (2, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    grads -- python dictionary containing your gradients with respect to different parameters\n",
        "    \"\"\"\n",
        "    m = X.shape[1] #get number of training examples\n",
        "    \n",
        "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
        "    W1 = parameters['W1']\n",
        "    W2 = parameters['W2']\n",
        "        \n",
        "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
        "    A1 = cache['A1']\n",
        "    A2 = cache['A2']\n",
        "    \n",
        "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
        "    dZ2 = A2-Y\n",
        "    dW2 = (1/m)*dZ2@A1.T #(n_h,m) @ (n_h,m) = (n_h,n_h)\n",
        "    db2 = (1/m)*np.sum(dZ2,axis=1,keepdims=True)\n",
        "    dZ1 = W2.T@dZ2 * (1 - np.power(A1, 2)) \n",
        "    # (n_h,n_x).T @ (n_h,m) * (n_h,m) - (n_y,m) = (n_x,m) * (n_h,m) Broadcasting\n",
        "    dW1 = (1/m)*dZ1@X.T #(n_h,m) * (n_h,m) = (n_h,m)\n",
        "    db1 = (1/m)*np.sum(dZ1,axis=1,keepdims=True)\n",
        "    \n",
        "    grads = {\"dW1\": dW1,\n",
        "             \"db1\": db1,\n",
        "             \"dW2\": dW2,\n",
        "             \"db2\": db2}\n",
        "    \n",
        "    return grads"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K58cJrCzB3jr"
      },
      "source": [
        "#Part 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BPIjsUoQ7-5"
      },
      "source": [
        "#4.1 Update parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YY5krwk-Q3oY"
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Updates parameters using the gradient descent update rule. Ex: w= w-alpha*dw, b= b-alpha*db\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients \n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    \n",
        "    # Retrieve each gradient from the dictionary \"grads\"\n",
        "    dW1 = grads['dW1']\n",
        "    db1 = grads['db1']\n",
        "    dW2 = grads['dW2']\n",
        "    db2 = grads['db2']\n",
        "    \n",
        "    # Update rule for each parameter\n",
        "    W1 = W1-learning_rate*dW1\n",
        "    b1 = b1-learning_rate*db1\n",
        "    W2 = W2-learning_rate*dW2\n",
        "    b2 = b2-learning_rate*db2\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5Sx569mB8SA"
      },
      "source": [
        "#Part 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JFPF7UzRapr"
      },
      "source": [
        "##5.1 Build NN model - 2 layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6ZDczLwRZ63"
      },
      "source": [
        "def nn_model(X, Y, n_h, num_iterations = 10000, learning_rate=0.01, print_cost=False):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- dataset of shape (2, number of examples)\n",
        "    Y -- labels of shape (1, number of examples)\n",
        "    n_h -- size of the hidden layer\n",
        "    num_iterations -- Number of iterations in gradient descent loop\n",
        "    learning_rate -- step size while implementing gradient descent\n",
        "    print_cost -- if True, print the cost every 1000 iterations\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "    \n",
        "    n_x = layer_sizes(X, Y)[0]\n",
        "    n_y = layer_sizes(X, Y)[2]\n",
        "    \n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "         \n",
        "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A, cache\".\n",
        "        A, cache = forward_propagation(X, parameters)\n",
        "        \n",
        "        #print(A)\n",
        "        # Cost function. Inputs: \"A, Y, parameters\". Outputs: \"cost\".\n",
        "        cost = compute_cost(A, Y)\n",
        " \n",
        "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
        "        grads = backward_propagation(parameters, cache, X, Y)\n",
        " \n",
        "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "        \n",
        "        # Print the cost every 1000 iterations\n",
        "        if print_cost and i % 1000 == 0:\n",
        "            #print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "            print('cost',i,cost)\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OXC1T8VSNxw"
      },
      "source": [
        "#5.2 Make function to generate predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMfPeeoXSElq"
      },
      "source": [
        "def predict(parameters, X, N):\n",
        "    \"\"\"\n",
        "    Using the learned parameters, predicts a class for each example in X\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    X -- input data of size (n_x, m), could be test set, validation set or new observations\n",
        "    \n",
        "    Returns\n",
        "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
        "    A, cache = forward_propagation(X, parameters)\n",
        "    #predictions = np.around(A,0) # integer class labels 0/1\n",
        "    \n",
        "    print(A.shape)\n",
        "\n",
        "    Y=A.reshape(A.shape[1],A.shape[0])\n",
        "\n",
        "    predictions=[]\n",
        "    for element in Y:\n",
        "      for n in range(0,int(N)+1):\n",
        "        if n/N < element <= (n+1)/N:\n",
        "          prediction=(n)/N#np.around( n+1/N, 2)\n",
        "          predictions.append(prediction)\n",
        "          #print(scaled_y) \n",
        "          #break \n",
        "\n",
        "    predictions=np.array(predictions)\n",
        "    predictions=predictions.reshape(A.shape[0],A.shape[1])\n",
        "\n",
        "    print(predictions.shape)\n",
        "    \n",
        "    return predictions"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA5oEkVIbVOe",
        "outputId": "3794d10e-ec1d-48bf-b542-22db5d208ff2"
      },
      "source": [
        "\n",
        "for n in range(0,int(N)+1):\n",
        "  print(n)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJGmfLzhSgcP"
      },
      "source": [
        "# Part 6: Run model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEp3hLhmSX2o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b2054be-65d7-4be5-b87e-029cdf242bfb"
      },
      "source": [
        "# Build a model with a n_h-dimensional hidden layer\n",
        "parameters = nn_model(X_train, y_train, n_h = 128, num_iterations = 30000, learning_rate=0.1, print_cost=True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cost 0 [[0.78066212]]\n",
            "cost 1000 [[0.50080836]]\n",
            "cost 2000 [[0.42755274]]\n",
            "cost 3000 [[0.39591756]]\n",
            "cost 4000 [[0.38227131]]\n",
            "cost 5000 [[0.37491526]]\n",
            "cost 6000 [[0.37028789]]\n",
            "cost 7000 [[0.36708907]]\n",
            "cost 8000 [[0.36474733]]\n",
            "cost 9000 [[0.36297744]]\n",
            "cost 10000 [[0.36160606]]\n",
            "cost 11000 [[0.36052234]]\n",
            "cost 12000 [[0.35965221]]\n",
            "cost 13000 [[0.35894812]]\n",
            "cost 14000 [[0.35838653]]\n",
            "cost 15000 [[0.35818914]]\n",
            "cost 16000 [[0.35841346]]\n",
            "cost 17000 [[0.35863634]]\n",
            "cost 18000 [[0.35859229]]\n",
            "cost 19000 [[0.35841153]]\n",
            "cost 20000 [[0.35819529]]\n",
            "cost 21000 [[0.35798002]]\n",
            "cost 22000 [[0.35777613]]\n",
            "cost 23000 [[0.35758618]]\n",
            "cost 24000 [[0.35741024]]\n",
            "cost 25000 [[0.35724753]]\n",
            "cost 26000 [[0.357097]]\n",
            "cost 27000 [[0.35695757]]\n",
            "cost 28000 [[0.35682818]]\n",
            "cost 29000 [[0.35670787]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ0mc36hTcVH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c587ce9c-4980-467a-b8b4-27717af2175b"
      },
      "source": [
        "# Print accuracy\n",
        "predictions = predict(parameters, X_train, N)\n",
        "print ('Accuracy: %d' % float((np.dot(y_train,predictions.T) + np.dot(1-y_train,1-predictions.T))/float(y_train.size)*100) + '%')\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 230)\n",
            "(1, 230)\n",
            "Accuracy: 75%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lI7NDDZbRgj1",
        "outputId": "d634c086-85b2-43b8-ac7c-17c3c0bd1579"
      },
      "source": [
        "print(predictions.shape,\n",
        "y_train.shape)\n",
        "y_train-predictions"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 230) (1, 230)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.25, 0.  , 0.  , 0.  , 0.  ,\n",
              "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "        0.  , 0.  , 0.  , 0.  , 0.  , 0.25, 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YVlqv2Vz8kY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c57f2628-3fa3-4998-9dbb-3290b163cdbf"
      },
      "source": [
        "# Confusion matrix\n",
        "#sklearn.metrics.confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)\n",
        "confusion_matrix(y_train.T*N,predictions.T*N)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[96,  0,  0,  0],\n",
              "       [ 2, 44,  0,  0],\n",
              "       [ 0,  0, 46,  0],\n",
              "       [ 0,  0,  0, 42]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2o7Yo5XON0Yt"
      },
      "source": [
        "#Part 7: Verify over/under fitting by the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKq5NGj-5kK4"
      },
      "source": [
        "def nn_model(X_train, y_train, X_test, y_test, n_h, num_iterations = 10000, learning_rate=0.1, print_cost=False):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- dataset of shape (2, number of examples)\n",
        "    Y -- labels of shape (1, number of examples)\n",
        "    n_h -- size of the hidden layer\n",
        "    num_iterations -- Number of iterations in gradient descent loop\n",
        "    learning_rate -- step size while implementing gradient descent\n",
        "    print_cost -- if True, print the cost every 1000 iterations\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "    \n",
        "    n_x = layer_sizes(X_train, y_train)[0]\n",
        "    n_y = layer_sizes(X_train, y_train)[2]\n",
        "    \n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "\n",
        "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A, cache\".\n",
        "        A, cache = forward_propagation(X_train, parameters)\n",
        "        # Cost function. Inputs: \"A, Y, parameters\". Outputs: \"cost\".\n",
        "        cost = compute_cost(A, y_test)\n",
        "\n",
        "        # Forward propagation. Inputs: \"X_test, parameters\". Outputs: \"A_validation, cache_validation\".\n",
        "        A_validation,cache_validation=forward_propagation(X_test, parameters)\n",
        "        # Cost function. Inputs: \"A_validation, Y_test, parameters\". Outputs: \"cost_validation\".\n",
        "        cost_validation = compute_cost(A_validation, y_test)\n",
        "\n",
        "\n",
        "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
        "        grads = backward_propagation(parameters, cache, X_train, y_train)\n",
        " \n",
        "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "        \n",
        "        # Print the cost every 1000 iterations\n",
        "        if print_cost and i % 1000 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost),'Validation cost %i:%f'%(i,cost_validation) )\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbQEX613QZne",
        "outputId": "163fd6a6-e32b-4999-9ef5-751b16d51f78"
      },
      "source": [
        "parameters = nn_model(X_train, y_train,X_test,y_test, n_h = 128, num_iterations = 30000, learning_rate=0.1, print_cost=True)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.743041 Validation cost 0:0.743780\n",
            "Cost after iteration 1000: 0.511184 Validation cost 1000:0.588599\n",
            "Cost after iteration 2000: 0.432031 Validation cost 2000:0.564453\n",
            "Cost after iteration 3000: 0.398656 Validation cost 3000:0.570238\n",
            "Cost after iteration 4000: 0.384467 Validation cost 4000:0.580373\n",
            "Cost after iteration 5000: 0.376560 Validation cost 5000:0.591831\n",
            "Cost after iteration 6000: 0.371640 Validation cost 6000:0.603662\n",
            "Cost after iteration 7000: 0.368301 Validation cost 7000:0.615351\n",
            "Cost after iteration 8000: 0.365942 Validation cost 8000:0.626844\n",
            "Cost after iteration 9000: 0.364101 Validation cost 9000:0.638007\n",
            "Cost after iteration 10000: 0.362736 Validation cost 10000:0.648112\n",
            "Cost after iteration 11000: 0.361663 Validation cost 11000:0.657638\n",
            "Cost after iteration 12000: 0.360829 Validation cost 12000:0.666452\n",
            "Cost after iteration 13000: 0.360210 Validation cost 13000:0.674638\n",
            "Cost after iteration 14000: 0.359744 Validation cost 14000:0.682577\n",
            "Cost after iteration 15000: 0.359512 Validation cost 15000:0.689999\n",
            "Cost after iteration 16000: 0.359694 Validation cost 16000:0.696912\n",
            "Cost after iteration 17000: 0.359981 Validation cost 17000:0.703360\n",
            "Cost after iteration 18000: 0.359991 Validation cost 18000:0.709117\n",
            "Cost after iteration 19000: 0.359836 Validation cost 19000:0.714387\n",
            "Cost after iteration 20000: 0.359633 Validation cost 20000:0.719357\n",
            "Cost after iteration 21000: 0.359425 Validation cost 21000:0.724096\n",
            "Cost after iteration 22000: 0.359224 Validation cost 22000:0.728634\n",
            "Cost after iteration 23000: 0.359034 Validation cost 23000:0.732989\n",
            "Cost after iteration 24000: 0.358858 Validation cost 24000:0.737172\n",
            "Cost after iteration 25000: 0.358694 Validation cost 25000:0.741195\n",
            "Cost after iteration 26000: 0.358542 Validation cost 26000:0.745069\n",
            "Cost after iteration 27000: 0.358400 Validation cost 27000:0.748802\n",
            "Cost after iteration 28000: 0.358269 Validation cost 28000:0.752404\n",
            "Cost after iteration 29000: 0.358146 Validation cost 29000:0.755881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmBmmUIxTcCf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab48aafe-2099-4f05-c4f1-ccfb843f30f0"
      },
      "source": [
        "# Print accuracy\n",
        "predictions = predict(parameters, X_test, N)\n",
        "print ('Accuracy: %d' % float((np.dot(y_test,predictions.T) + np.dot(1-y_test,1-predictions.T))/float(y_test.size)*100) + '%')\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 230)\n",
            "(1, 230)\n",
            "Accuracy: 69%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sN2MR1dke-wl",
        "outputId": "6ef0ff0f-80fa-45e1-c456-6c06f328245c"
      },
      "source": [
        "print(predictions.shape,\n",
        "y_test.shape)\n",
        "\n",
        "y_test-predictions"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 230) (1, 230)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.25,  0.  ,  0.  ,  0.  , -0.75,  0.  ,  0.  ,  0.  ,  0.  ,\n",
              "         0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
              "         0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  , -0.75,  0.  ,\n",
              "         0.  , -0.75,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
              "        -0.75,  0.  , -0.5 ,  0.  ,  0.  , -0.75,  0.  , -0.25,  0.  ,\n",
              "        -0.25,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
              "         0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  , -0.25,  0.  ,\n",
              "        -0.25, -0.5 , -0.25, -0.5 , -0.75,  0.  ,  0.  ,  0.  ,  0.  ,\n",
              "         0.  ,  0.  ,  0.  , -0.25, -0.25, -0.25,  0.  ,  0.  ,  0.  ,\n",
              "         0.  , -0.5 ,  0.  ,  0.  ,  0.  , -0.25,  0.  ,  0.  ,  0.  ,\n",
              "         0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.25,  0.25,  0.25,\n",
              "         0.25,  0.25, -0.5 ,  0.  , -0.25,  0.  ,  0.  , -0.25,  0.25,\n",
              "         0.25,  0.  ,  0.25,  0.25,  0.25,  0.25, -0.5 ,  0.  ,  0.  ,\n",
              "         0.25,  0.25,  0.25,  0.25,  0.  ,  0.25,  0.  ,  0.25,  0.25,\n",
              "        -0.25,  0.25,  0.25,  0.25,  0.  ,  0.25,  0.25,  0.25,  0.25,\n",
              "         0.25,  0.25,  0.25,  0.25,  0.25,  0.  ,  0.  ,  0.  , -0.25,\n",
              "        -0.25,  0.  ,  0.  , -0.25,  0.5 ,  0.5 ,  0.5 ,  0.25,  0.25,\n",
              "        -0.25,  0.  , -0.25,  0.5 ,  0.5 ,  0.  ,  0.25,  0.25,  0.5 ,\n",
              "         0.5 ,  0.5 ,  0.5 , -0.25,  0.5 ,  0.  , -0.25, -0.25,  0.  ,\n",
              "         0.  ,  0.5 ,  0.25, -0.25,  0.5 ,  0.5 ,  0.5 ,  0.25,  0.  ,\n",
              "         0.  ,  0.  ,  0.5 , -0.25, -0.25,  0.  , -0.25,  0.5 ,  0.75,\n",
              "         0.25,  0.25,  0.5 ,  0.  ,  0.75,  0.  ,  0.75,  0.75,  0.25,\n",
              "         0.5 ,  0.  ,  0.75,  0.  ,  0.75,  0.75,  0.5 ,  0.75,  0.5 ,\n",
              "         0.5 ,  0.25,  0.25,  0.5 ,  0.75,  0.25,  0.75,  0.5 ,  0.75,\n",
              "         0.75,  0.75,  0.5 ,  0.  ,  0.25,  0.25,  0.  ,  0.25,  0.75,\n",
              "         0.  ,  0.75,  0.  ,  0.25,  0.  ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4Yjdv_TfGFv",
        "outputId": "fac981f2-8a05-4c3f-c171-ba54052331d5"
      },
      "source": [
        "# Confusion matrix\n",
        "#sklearn.metrics.confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)\n",
        "confusion_matrix(y_test.T*N,predictions.T*N)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[76, 10,  4,  6],\n",
              "       [30, 10,  3,  2],\n",
              "       [15,  6, 13, 12],\n",
              "       [15,  9, 10,  9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq4ijodRfUYV"
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    }
  ]
}