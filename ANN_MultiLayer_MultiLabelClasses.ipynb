{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN_MultiLayer_MultiLabelClasses.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyND27KIZRhlDr3somGMYGuW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uteyechea/neural-network-from-scratch/blob/main/ANN_MultiLayer_MultiLabelClasses.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBVrFfdZKJMZ"
      },
      "source": [
        "#Part 1: Import all necessary dependecies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGhgnK0zJxTF",
        "outputId": "9b637779-d1fa-4a11-d79a-ba9692495772"
      },
      "source": [
        "!pip install scikit-multilearn #Data split train/test sets"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.6/dist-packages (0.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JXIWB9VLJjG"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from skmultilearn.model_selection import iterative_train_test_split"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Bd6BnGgLT1v"
      },
      "source": [
        "#Part 2: Import data and make train/test sets\n",
        "\n",
        "We wil be working with ECG data, looking to classify normal ECGs vs not-normal ECGs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82ytQqlBL86n"
      },
      "source": [
        "##2.1 Download ECG data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTM4u-bdLLua"
      },
      "source": [
        "def read_cvs_with_html_tags(data_url='https://github.com/uteyechea/neural-network-from-scratch/blob/main/ecg_data/ecg.csv'): \n",
        "  # This function only works for my very peculiar data structure.\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  data_url -- csv file url\n",
        "\n",
        "  Returns:\n",
        "  X -- Training data\n",
        "  y -- Labels\n",
        "  \"\"\" \n",
        "  raw_data=pd.read_html(data_url) # Looking for <table> tag\n",
        "  unformatted_data=raw_data[0][1].str.split(\";\",expand=True) #read_html return need a little work\n",
        "  #By now you have a standard pandas DataFrame, but still needs some work...\n",
        "  #make first row new header, (optional, depending on your csv file)\n",
        "  new_header=unformatted_data.iloc[0,:]\n",
        "  data=unformatted_data[1:]\n",
        "  data.columns=new_header\n",
        "\n",
        "  X=data.iloc[:,0:-5]\n",
        "  y=data.iloc[:,-4:] #'Clase' spanish for class label. OR use last 4 for one-hot representation\n",
        "  #y=y.values.reshape(y.shape[0],1)\n",
        "\n",
        "  X=np.array(X)\n",
        "  y=np.array(y)\n",
        "\n",
        "  X=X.astype(np.float)\n",
        "  y=y.astype(np.float)\n",
        "\n",
        "  #y=y/max(y)-(1/max(y)) Normalize class labels\n",
        "\n",
        "  return X,y"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK78lJOaMh-4"
      },
      "source": [
        "##2.2 Split train/test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXHSu-kBMnMY"
      },
      "source": [
        "def split_data(X,y,test_size=0.5):\n",
        "  \"\"\"\n",
        "  Agruments(None):\n",
        "  data_url -- csv file location\n",
        "  test_size -- Test/train data split, ex. test_size=0.7 distributes 70% to the test set and 30% to the training set. \n",
        "  Returns:\n",
        "  X_train -- train data features\n",
        "  y_train -- train data labels\n",
        "  X_test -- test data features\n",
        "  y_test -- test data labels\n",
        "  \"\"\"\n",
        "  data_url='https://github.com/uteyechea/neural-network-from-scratch/blob/main/ecg_data/ecg.csv'\n",
        "  X,y=read_cvs_with_html_tags(data_url)\n",
        "\n",
        "  X_train, y_train, X_test, y_test = iterative_train_test_split(X, y, test_size) \n",
        "  \n",
        "  #Reshape data to fit expected data structure for nn model\n",
        "  #X_train=X_train.reshape(X_train.shape[1],X_train.shape[0])\n",
        "  #X_test=X_test.reshape(X_test.shape[1],X_test.shape[0])\n",
        "  #y_train=y_train.reshape(y_train.shape[1],y_train.shape[0])\n",
        "  #y_test=y_test.reshape(y_test.shape[1],y_test.shape[0])\n",
        "\n",
        "  y_test=y_test.astype(int) #cross enctropy loss in NN model expects labels as ints\n",
        "  y_train=y_train.astype(int) #cross enctropy loss in NN model expects labels as ints\n",
        "\n",
        "  N=len(np.unique(y_test))\n",
        "\n",
        "  return X_train, y_train, X_test, y_test,N"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwwXcSRRMk0E"
      },
      "source": [
        "def get_data_distribution(y_test,y_train):\n",
        "  #Assumed y_train, y_test data shape (n,1)\n",
        "\n",
        "  y_test_distribution={}\n",
        "  y_train_distribution={}\n",
        "\n",
        "  for label in np.unique(y_test,axis=0): #fix to avoid only 0s and 1s\n",
        "    y_test_distribution['class='+str(label)]= np.count_nonzero(y_test == label, axis=0)\n",
        "  \n",
        "  for label in np.unique(y_train,axis=0):\n",
        "    y_train_distribution['class='+str(label)]= np.count_nonzero(y_train == label, axis=0)\n",
        "\n",
        "  #plot\n",
        "  y_test=pd.DataFrame(y_test_distribution)\n",
        "  y_train=pd.DataFrame(y_train_distribution)\n",
        "  fig = go.Figure(data=[\n",
        "  go.Bar(name='Train', x=y_train.columns, y=y_train.iloc[0,:]),\n",
        "  go.Bar(name='Test', x=y_test.columns, y=y_test.iloc[0,:])\n",
        "  ])\n",
        "  # Change the bar mode\n",
        "  fig.update_layout(barmode='stack',\n",
        "                    title_text='Train/Test data distribution')\n",
        "  fig.show()    \n",
        "  \n",
        "  return y_train_distribution,y_test_distribution"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpryNRkTNORG"
      },
      "source": [
        "##2.3 Get ECG data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "UBTVxbTnNS3G",
        "outputId": "215a8c20-7f51-4c06-e39c-a1ccfc57c4b9"
      },
      "source": [
        "X,y=read_cvs_with_html_tags()\n",
        "X_train, y_train, X_test, y_test,N=split_data(X,y,test_size=0.3)\n",
        "y_train_distribution,y_test_distribution=get_data_distribution(y_test,y_train)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"6a8de4fe-2869-4bf7-9e7c-8b70fc3f5693\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"6a8de4fe-2869-4bf7-9e7c-8b70fc3f5693\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '6a8de4fe-2869-4bf7-9e7c-8b70fc3f5693',\n",
              "                        [{\"name\": \"Train\", \"type\": \"bar\", \"x\": [\"class=[0 0 0 1]\", \"class=[0 0 1 0]\", \"class=[0 1 0 0]\", \"class=[1 0 0 0]\"], \"y\": [187, 187, 187, 134]}, {\"name\": \"Test\", \"type\": \"bar\", \"x\": [\"class=[0 0 0 1]\", \"class=[0 0 1 0]\", \"class=[0 1 0 0]\", \"class=[1 0 0 0]\"], \"y\": [81, 81, 81, 58]}],\n",
              "                        {\"barmode\": \"stack\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Train/Test data distribution\"}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('6a8de4fe-2869-4bf7-9e7c-8b70fc3f5693');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX5koctDUPjM"
      },
      "source": [
        "#Part 3: ANN model\n",
        "\n",
        "We have a small amount of data, which is further reduced due to splitting the data into a training and testing set. It could be said that due to the small amount of presented data in this dataset, we must be careful to not create an overly complex model, which could lead to overfitting our data. But in order to confirm wheter your model has too many parameters you need to do some experimentation.\n",
        "\n",
        "We are going to use an architecture based on a single Dense layers with 128 neurons using a ReLU (Rectified Linear Unit) activation function. A dense layer with a softmax activation function will be used as output layer.\n",
        "\n",
        "In order to allow us to know if our model is properly learning, we will use a categorical cross entropy loss function because we have four labels and to report the performance of it we will adopt the mean squared error as metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwZ762dPVYj0"
      },
      "source": [
        "##3.1 Model arquitecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-jbqWPvORhs",
        "outputId": "9a7186c8-fcc1-4651-cf3a-ca234130fd7b"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(X_train.shape[1],) ,activation='relu', name='dense_1'))\n",
        "#model.add(Dense(64, activation='relu', name='dense_2'))\n",
        "#model.add(Dense(128, activation='relu', name='dense_3'))\n",
        "model.add(Dense(4, activation='softmax', name='dense_output'))\n",
        "\n",
        "\n",
        "#loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['mse'])\n",
        "\n",
        "#Switch to one-hot representation and use loss=categegorical_crossentropy\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 128)               5120      \n",
            "_________________________________________________________________\n",
            "dense_output (Dense)         (None, 4)                 516       \n",
            "=================================================================\n",
            "Total params: 5,636\n",
            "Trainable params: 5,636\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKkYloKTVdP4"
      },
      "source": [
        "##3.2 Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-Pl_uFGOn2F",
        "outputId": "b5ace8bf-e548-4ae1-cf8e-370ae66e2aaa"
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=1000, validation_split=0.05)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "10/10 [==============================] - 0s 15ms/step - loss: 1.5235 - mse: 0.1966 - val_loss: 1.8325 - val_mse: 0.2363\n",
            "Epoch 2/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.2719 - mse: 0.1728 - val_loss: 2.1076 - val_mse: 0.2736\n",
            "Epoch 3/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 1.2251 - mse: 0.1682 - val_loss: 1.8902 - val_mse: 0.2470\n",
            "Epoch 4/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 1.1502 - mse: 0.1563 - val_loss: 1.7625 - val_mse: 0.2296\n",
            "Epoch 5/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.1035 - mse: 0.1495 - val_loss: 1.7239 - val_mse: 0.2260\n",
            "Epoch 6/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 1.0577 - mse: 0.1434 - val_loss: 1.7278 - val_mse: 0.2273\n",
            "Epoch 7/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 1.0210 - mse: 0.1388 - val_loss: 1.5726 - val_mse: 0.2118\n",
            "Epoch 8/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.9750 - mse: 0.1319 - val_loss: 1.6116 - val_mse: 0.2152\n",
            "Epoch 9/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.9350 - mse: 0.1262 - val_loss: 1.4562 - val_mse: 0.1974\n",
            "Epoch 10/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.9021 - mse: 0.1226 - val_loss: 1.4417 - val_mse: 0.1964\n",
            "Epoch 11/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.8654 - mse: 0.1169 - val_loss: 1.3939 - val_mse: 0.1950\n",
            "Epoch 12/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.8289 - mse: 0.1121 - val_loss: 1.4343 - val_mse: 0.1968\n",
            "Epoch 13/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.8058 - mse: 0.1081 - val_loss: 1.1836 - val_mse: 0.1649\n",
            "Epoch 14/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7687 - mse: 0.1034 - val_loss: 1.3130 - val_mse: 0.1852\n",
            "Epoch 15/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7358 - mse: 0.0984 - val_loss: 1.2587 - val_mse: 0.1773\n",
            "Epoch 16/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7064 - mse: 0.0936 - val_loss: 1.1324 - val_mse: 0.1593\n",
            "Epoch 17/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6830 - mse: 0.0904 - val_loss: 1.0921 - val_mse: 0.1559\n",
            "Epoch 18/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6557 - mse: 0.0866 - val_loss: 1.1742 - val_mse: 0.1656\n",
            "Epoch 19/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6353 - mse: 0.0841 - val_loss: 1.1569 - val_mse: 0.1648\n",
            "Epoch 20/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6155 - mse: 0.0806 - val_loss: 0.9565 - val_mse: 0.1341\n",
            "Epoch 21/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5925 - mse: 0.0775 - val_loss: 1.0952 - val_mse: 0.1569\n",
            "Epoch 22/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5746 - mse: 0.0755 - val_loss: 1.0155 - val_mse: 0.1433\n",
            "Epoch 23/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.5561 - mse: 0.0727 - val_loss: 1.0153 - val_mse: 0.1449\n",
            "Epoch 24/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5438 - mse: 0.0699 - val_loss: 0.9511 - val_mse: 0.1350\n",
            "Epoch 25/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5233 - mse: 0.0674 - val_loss: 0.9130 - val_mse: 0.1254\n",
            "Epoch 26/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5096 - mse: 0.0660 - val_loss: 0.9317 - val_mse: 0.1309\n",
            "Epoch 27/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4953 - mse: 0.0632 - val_loss: 0.9794 - val_mse: 0.1393\n",
            "Epoch 28/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4811 - mse: 0.0617 - val_loss: 0.8587 - val_mse: 0.1159\n",
            "Epoch 29/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4609 - mse: 0.0584 - val_loss: 0.9648 - val_mse: 0.1362\n",
            "Epoch 30/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4522 - mse: 0.0572 - val_loss: 0.9277 - val_mse: 0.1271\n",
            "Epoch 31/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4367 - mse: 0.0547 - val_loss: 0.8337 - val_mse: 0.1126\n",
            "Epoch 32/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4238 - mse: 0.0523 - val_loss: 0.8269 - val_mse: 0.1132\n",
            "Epoch 33/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4149 - mse: 0.0521 - val_loss: 0.8783 - val_mse: 0.1198\n",
            "Epoch 34/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3985 - mse: 0.0489 - val_loss: 0.8690 - val_mse: 0.1190\n",
            "Epoch 35/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3866 - mse: 0.0472 - val_loss: 0.7937 - val_mse: 0.1066\n",
            "Epoch 36/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3771 - mse: 0.0457 - val_loss: 0.8124 - val_mse: 0.1092\n",
            "Epoch 37/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3690 - mse: 0.0442 - val_loss: 0.7567 - val_mse: 0.1013\n",
            "Epoch 38/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3551 - mse: 0.0426 - val_loss: 0.8418 - val_mse: 0.1133\n",
            "Epoch 39/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3502 - mse: 0.0424 - val_loss: 0.7842 - val_mse: 0.1051\n",
            "Epoch 40/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3394 - mse: 0.0401 - val_loss: 0.7673 - val_mse: 0.1029\n",
            "Epoch 41/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3296 - mse: 0.0385 - val_loss: 0.7688 - val_mse: 0.1029\n",
            "Epoch 42/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3193 - mse: 0.0369 - val_loss: 0.7286 - val_mse: 0.0956\n",
            "Epoch 43/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3137 - mse: 0.0370 - val_loss: 0.8166 - val_mse: 0.1100\n",
            "Epoch 44/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3033 - mse: 0.0348 - val_loss: 0.7591 - val_mse: 0.1018\n",
            "Epoch 45/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2955 - mse: 0.0336 - val_loss: 0.7078 - val_mse: 0.0934\n",
            "Epoch 46/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2913 - mse: 0.0334 - val_loss: 0.7032 - val_mse: 0.0924\n",
            "Epoch 47/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2813 - mse: 0.0316 - val_loss: 0.7295 - val_mse: 0.0961\n",
            "Epoch 48/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2760 - mse: 0.0311 - val_loss: 0.7638 - val_mse: 0.1014\n",
            "Epoch 49/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2678 - mse: 0.0297 - val_loss: 0.6712 - val_mse: 0.0878\n",
            "Epoch 50/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2599 - mse: 0.0286 - val_loss: 0.6975 - val_mse: 0.0919\n",
            "Epoch 51/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2574 - mse: 0.0282 - val_loss: 0.6745 - val_mse: 0.0889\n",
            "Epoch 52/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2490 - mse: 0.0274 - val_loss: 0.7148 - val_mse: 0.0938\n",
            "Epoch 53/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2444 - mse: 0.0264 - val_loss: 0.7261 - val_mse: 0.0964\n",
            "Epoch 54/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2377 - mse: 0.0257 - val_loss: 0.6149 - val_mse: 0.0802\n",
            "Epoch 55/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2298 - mse: 0.0244 - val_loss: 0.6177 - val_mse: 0.0814\n",
            "Epoch 56/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2249 - mse: 0.0239 - val_loss: 0.6741 - val_mse: 0.0888\n",
            "Epoch 57/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2212 - mse: 0.0234 - val_loss: 0.6537 - val_mse: 0.0859\n",
            "Epoch 58/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2136 - mse: 0.0223 - val_loss: 0.6290 - val_mse: 0.0824\n",
            "Epoch 59/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2097 - mse: 0.0219 - val_loss: 0.6307 - val_mse: 0.0828\n",
            "Epoch 60/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2079 - mse: 0.0220 - val_loss: 0.6625 - val_mse: 0.0880\n",
            "Epoch 61/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2011 - mse: 0.0206 - val_loss: 0.5891 - val_mse: 0.0772\n",
            "Epoch 62/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1947 - mse: 0.0199 - val_loss: 0.6245 - val_mse: 0.0823\n",
            "Epoch 63/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1905 - mse: 0.0194 - val_loss: 0.6084 - val_mse: 0.0801\n",
            "Epoch 64/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1864 - mse: 0.0189 - val_loss: 0.6051 - val_mse: 0.0798\n",
            "Epoch 65/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1803 - mse: 0.0179 - val_loss: 0.6388 - val_mse: 0.0850\n",
            "Epoch 66/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1781 - mse: 0.0178 - val_loss: 0.5711 - val_mse: 0.0754\n",
            "Epoch 67/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1745 - mse: 0.0173 - val_loss: 0.6322 - val_mse: 0.0836\n",
            "Epoch 68/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1714 - mse: 0.0170 - val_loss: 0.5649 - val_mse: 0.0747\n",
            "Epoch 69/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1662 - mse: 0.0163 - val_loss: 0.5686 - val_mse: 0.0753\n",
            "Epoch 70/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1636 - mse: 0.0162 - val_loss: 0.6092 - val_mse: 0.0809\n",
            "Epoch 71/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1590 - mse: 0.0155 - val_loss: 0.5897 - val_mse: 0.0785\n",
            "Epoch 72/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1578 - mse: 0.0154 - val_loss: 0.4989 - val_mse: 0.0661\n",
            "Epoch 73/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1514 - mse: 0.0146 - val_loss: 0.6150 - val_mse: 0.0820\n",
            "Epoch 74/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1497 - mse: 0.0145 - val_loss: 0.5898 - val_mse: 0.0789\n",
            "Epoch 75/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1453 - mse: 0.0138 - val_loss: 0.5087 - val_mse: 0.0677\n",
            "Epoch 76/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1425 - mse: 0.0134 - val_loss: 0.5235 - val_mse: 0.0699\n",
            "Epoch 77/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1386 - mse: 0.0131 - val_loss: 0.5626 - val_mse: 0.0755\n",
            "Epoch 78/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1371 - mse: 0.0130 - val_loss: 0.5532 - val_mse: 0.0741\n",
            "Epoch 79/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1348 - mse: 0.0126 - val_loss: 0.4984 - val_mse: 0.0664\n",
            "Epoch 80/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1301 - mse: 0.0121 - val_loss: 0.5515 - val_mse: 0.0741\n",
            "Epoch 81/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1283 - mse: 0.0120 - val_loss: 0.5397 - val_mse: 0.0725\n",
            "Epoch 82/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1268 - mse: 0.0118 - val_loss: 0.5035 - val_mse: 0.0677\n",
            "Epoch 83/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1228 - mse: 0.0113 - val_loss: 0.5153 - val_mse: 0.0699\n",
            "Epoch 84/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1219 - mse: 0.0114 - val_loss: 0.5670 - val_mse: 0.0764\n",
            "Epoch 85/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1197 - mse: 0.0110 - val_loss: 0.4387 - val_mse: 0.0587\n",
            "Epoch 86/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1154 - mse: 0.0104 - val_loss: 0.5023 - val_mse: 0.0679\n",
            "Epoch 87/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1134 - mse: 0.0103 - val_loss: 0.5192 - val_mse: 0.0702\n",
            "Epoch 88/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1136 - mse: 0.0103 - val_loss: 0.4407 - val_mse: 0.0599\n",
            "Epoch 89/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1122 - mse: 0.0102 - val_loss: 0.6018 - val_mse: 0.0816\n",
            "Epoch 90/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1069 - mse: 0.0095 - val_loss: 0.4299 - val_mse: 0.0579\n",
            "Epoch 91/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1064 - mse: 0.0095 - val_loss: 0.4680 - val_mse: 0.0631\n",
            "Epoch 92/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1034 - mse: 0.0092 - val_loss: 0.5051 - val_mse: 0.0696\n",
            "Epoch 93/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0998 - mse: 0.0087 - val_loss: 0.5122 - val_mse: 0.0697\n",
            "Epoch 94/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0978 - mse: 0.0084 - val_loss: 0.4401 - val_mse: 0.0597\n",
            "Epoch 95/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0979 - mse: 0.0085 - val_loss: 0.4360 - val_mse: 0.0594\n",
            "Epoch 96/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0956 - mse: 0.0082 - val_loss: 0.4676 - val_mse: 0.0636\n",
            "Epoch 97/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0930 - mse: 0.0079 - val_loss: 0.4738 - val_mse: 0.0652\n",
            "Epoch 98/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0920 - mse: 0.0079 - val_loss: 0.4994 - val_mse: 0.0686\n",
            "Epoch 99/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0890 - mse: 0.0075 - val_loss: 0.4255 - val_mse: 0.0579\n",
            "Epoch 100/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0876 - mse: 0.0072 - val_loss: 0.3948 - val_mse: 0.0536\n",
            "Epoch 101/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0861 - mse: 0.0072 - val_loss: 0.4696 - val_mse: 0.0645\n",
            "Epoch 102/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0847 - mse: 0.0072 - val_loss: 0.4742 - val_mse: 0.0652\n",
            "Epoch 103/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0838 - mse: 0.0070 - val_loss: 0.4032 - val_mse: 0.0554\n",
            "Epoch 104/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0814 - mse: 0.0066 - val_loss: 0.4619 - val_mse: 0.0634\n",
            "Epoch 105/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0803 - mse: 0.0066 - val_loss: 0.4502 - val_mse: 0.0619\n",
            "Epoch 106/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0790 - mse: 0.0065 - val_loss: 0.4727 - val_mse: 0.0669\n",
            "Epoch 107/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0769 - mse: 0.0062 - val_loss: 0.4308 - val_mse: 0.0591\n",
            "Epoch 108/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0761 - mse: 0.0061 - val_loss: 0.4202 - val_mse: 0.0578\n",
            "Epoch 109/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0736 - mse: 0.0057 - val_loss: 0.4286 - val_mse: 0.0600\n",
            "Epoch 110/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0728 - mse: 0.0058 - val_loss: 0.4442 - val_mse: 0.0611\n",
            "Epoch 111/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0707 - mse: 0.0056 - val_loss: 0.4326 - val_mse: 0.0602\n",
            "Epoch 112/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0696 - mse: 0.0054 - val_loss: 0.4465 - val_mse: 0.0629\n",
            "Epoch 113/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0691 - mse: 0.0054 - val_loss: 0.4103 - val_mse: 0.0563\n",
            "Epoch 114/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0669 - mse: 0.0051 - val_loss: 0.3806 - val_mse: 0.0526\n",
            "Epoch 115/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0655 - mse: 0.0049 - val_loss: 0.4121 - val_mse: 0.0577\n",
            "Epoch 116/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0649 - mse: 0.0050 - val_loss: 0.4377 - val_mse: 0.0613\n",
            "Epoch 117/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0638 - mse: 0.0048 - val_loss: 0.4128 - val_mse: 0.0573\n",
            "Epoch 118/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0637 - mse: 0.0048 - val_loss: 0.3795 - val_mse: 0.0522\n",
            "Epoch 119/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0611 - mse: 0.0044 - val_loss: 0.4405 - val_mse: 0.0618\n",
            "Epoch 120/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0607 - mse: 0.0045 - val_loss: 0.3939 - val_mse: 0.0548\n",
            "Epoch 121/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0601 - mse: 0.0045 - val_loss: 0.3871 - val_mse: 0.0541\n",
            "Epoch 122/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0589 - mse: 0.0043 - val_loss: 0.3861 - val_mse: 0.0535\n",
            "Epoch 123/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0576 - mse: 0.0043 - val_loss: 0.4089 - val_mse: 0.0567\n",
            "Epoch 124/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0567 - mse: 0.0041 - val_loss: 0.3595 - val_mse: 0.0503\n",
            "Epoch 125/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0558 - mse: 0.0040 - val_loss: 0.3826 - val_mse: 0.0529\n",
            "Epoch 126/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0541 - mse: 0.0039 - val_loss: 0.3416 - val_mse: 0.0470\n",
            "Epoch 127/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0540 - mse: 0.0038 - val_loss: 0.3727 - val_mse: 0.0518\n",
            "Epoch 128/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0527 - mse: 0.0036 - val_loss: 0.3571 - val_mse: 0.0490\n",
            "Epoch 129/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0520 - mse: 0.0036 - val_loss: 0.3798 - val_mse: 0.0527\n",
            "Epoch 130/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0513 - mse: 0.0035 - val_loss: 0.3519 - val_mse: 0.0489\n",
            "Epoch 131/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0511 - mse: 0.0034 - val_loss: 0.3135 - val_mse: 0.0425\n",
            "Epoch 132/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0490 - mse: 0.0032 - val_loss: 0.3893 - val_mse: 0.0540\n",
            "Epoch 133/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0490 - mse: 0.0034 - val_loss: 0.3730 - val_mse: 0.0522\n",
            "Epoch 134/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0483 - mse: 0.0034 - val_loss: 0.3731 - val_mse: 0.0525\n",
            "Epoch 135/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0471 - mse: 0.0031 - val_loss: 0.2792 - val_mse: 0.0375\n",
            "Epoch 136/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0468 - mse: 0.0028 - val_loss: 0.3348 - val_mse: 0.0459\n",
            "Epoch 137/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0452 - mse: 0.0029 - val_loss: 0.3758 - val_mse: 0.0524\n",
            "Epoch 138/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0445 - mse: 0.0030 - val_loss: 0.3608 - val_mse: 0.0505\n",
            "Epoch 139/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0441 - mse: 0.0029 - val_loss: 0.3194 - val_mse: 0.0441\n",
            "Epoch 140/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0432 - mse: 0.0028 - val_loss: 0.3388 - val_mse: 0.0466\n",
            "Epoch 141/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0427 - mse: 0.0027 - val_loss: 0.3616 - val_mse: 0.0508\n",
            "Epoch 142/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0424 - mse: 0.0028 - val_loss: 0.3469 - val_mse: 0.0481\n",
            "Epoch 143/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0416 - mse: 0.0026 - val_loss: 0.2617 - val_mse: 0.0356\n",
            "Epoch 144/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0406 - mse: 0.0023 - val_loss: 0.3429 - val_mse: 0.0476\n",
            "Epoch 145/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0401 - mse: 0.0025 - val_loss: 0.3715 - val_mse: 0.0515\n",
            "Epoch 146/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0399 - mse: 0.0025 - val_loss: 0.3028 - val_mse: 0.0421\n",
            "Epoch 147/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0386 - mse: 0.0023 - val_loss: 0.3249 - val_mse: 0.0453\n",
            "Epoch 148/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0378 - mse: 0.0023 - val_loss: 0.3532 - val_mse: 0.0492\n",
            "Epoch 149/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0377 - mse: 0.0024 - val_loss: 0.3183 - val_mse: 0.0439\n",
            "Epoch 150/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0372 - mse: 0.0022 - val_loss: 0.3229 - val_mse: 0.0456\n",
            "Epoch 151/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0365 - mse: 0.0021 - val_loss: 0.2948 - val_mse: 0.0402\n",
            "Epoch 152/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0359 - mse: 0.0020 - val_loss: 0.3027 - val_mse: 0.0417\n",
            "Epoch 153/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0357 - mse: 0.0020 - val_loss: 0.3153 - val_mse: 0.0439\n",
            "Epoch 154/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0347 - mse: 0.0021 - val_loss: 0.3519 - val_mse: 0.0493\n",
            "Epoch 155/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0343 - mse: 0.0021 - val_loss: 0.3277 - val_mse: 0.0458\n",
            "Epoch 156/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0335 - mse: 0.0019 - val_loss: 0.2893 - val_mse: 0.0398\n",
            "Epoch 157/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0332 - mse: 0.0018 - val_loss: 0.2709 - val_mse: 0.0368\n",
            "Epoch 158/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0329 - mse: 0.0017 - val_loss: 0.2795 - val_mse: 0.0382\n",
            "Epoch 159/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0321 - mse: 0.0017 - val_loss: 0.2928 - val_mse: 0.0404\n",
            "Epoch 160/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0320 - mse: 0.0017 - val_loss: 0.2842 - val_mse: 0.0395\n",
            "Epoch 161/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0312 - mse: 0.0017 - val_loss: 0.2953 - val_mse: 0.0403\n",
            "Epoch 162/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0308 - mse: 0.0017 - val_loss: 0.3011 - val_mse: 0.0412\n",
            "Epoch 163/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0300 - mse: 0.0016 - val_loss: 0.2723 - val_mse: 0.0377\n",
            "Epoch 164/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0298 - mse: 0.0015 - val_loss: 0.2936 - val_mse: 0.0406\n",
            "Epoch 165/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0294 - mse: 0.0015 - val_loss: 0.2783 - val_mse: 0.0378\n",
            "Epoch 166/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0289 - mse: 0.0015 - val_loss: 0.2829 - val_mse: 0.0389\n",
            "Epoch 167/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0287 - mse: 0.0015 - val_loss: 0.2780 - val_mse: 0.0380\n",
            "Epoch 168/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0279 - mse: 0.0014 - val_loss: 0.2917 - val_mse: 0.0405\n",
            "Epoch 169/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0280 - mse: 0.0015 - val_loss: 0.3000 - val_mse: 0.0417\n",
            "Epoch 170/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0274 - mse: 0.0015 - val_loss: 0.2723 - val_mse: 0.0373\n",
            "Epoch 171/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0271 - mse: 0.0013 - val_loss: 0.2536 - val_mse: 0.0344\n",
            "Epoch 172/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0266 - mse: 0.0014 - val_loss: 0.2969 - val_mse: 0.0411\n",
            "Epoch 173/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0265 - mse: 0.0014 - val_loss: 0.2551 - val_mse: 0.0346\n",
            "Epoch 174/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0259 - mse: 0.0013 - val_loss: 0.2836 - val_mse: 0.0391\n",
            "Epoch 175/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0262 - mse: 0.0015 - val_loss: 0.2940 - val_mse: 0.0411\n",
            "Epoch 176/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0258 - mse: 0.0011 - val_loss: 0.1997 - val_mse: 0.0258\n",
            "Epoch 177/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0257 - mse: 0.0011 - val_loss: 0.2530 - val_mse: 0.0342\n",
            "Epoch 178/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0245 - mse: 0.0011 - val_loss: 0.2774 - val_mse: 0.0383\n",
            "Epoch 179/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0242 - mse: 0.0011 - val_loss: 0.2727 - val_mse: 0.0374\n",
            "Epoch 180/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0237 - mse: 0.0011 - val_loss: 0.2732 - val_mse: 0.0379\n",
            "Epoch 181/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0235 - mse: 0.0011 - val_loss: 0.2577 - val_mse: 0.0353\n",
            "Epoch 182/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0232 - mse: 0.0010 - val_loss: 0.2502 - val_mse: 0.0339\n",
            "Epoch 183/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0228 - mse: 9.9173e-04 - val_loss: 0.2718 - val_mse: 0.0375\n",
            "Epoch 184/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0224 - mse: 9.9229e-04 - val_loss: 0.2621 - val_mse: 0.0360\n",
            "Epoch 185/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0222 - mse: 9.8463e-04 - val_loss: 0.2636 - val_mse: 0.0361\n",
            "Epoch 186/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0221 - mse: 0.0010 - val_loss: 0.2752 - val_mse: 0.0380\n",
            "Epoch 187/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0219 - mse: 9.5457e-04 - val_loss: 0.2293 - val_mse: 0.0309\n",
            "Epoch 188/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0215 - mse: 9.1392e-04 - val_loss: 0.2671 - val_mse: 0.0367\n",
            "Epoch 189/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0215 - mse: 0.0011 - val_loss: 0.2724 - val_mse: 0.0377\n",
            "Epoch 190/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0210 - mse: 8.8095e-04 - val_loss: 0.2177 - val_mse: 0.0293\n",
            "Epoch 191/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0209 - mse: 8.0999e-04 - val_loss: 0.2510 - val_mse: 0.0340\n",
            "Epoch 192/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0205 - mse: 8.8753e-04 - val_loss: 0.2567 - val_mse: 0.0351\n",
            "Epoch 193/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0202 - mse: 8.9838e-04 - val_loss: 0.2411 - val_mse: 0.0329\n",
            "Epoch 194/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0197 - mse: 8.1186e-04 - val_loss: 0.2358 - val_mse: 0.0319\n",
            "Epoch 195/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0197 - mse: 8.6996e-04 - val_loss: 0.2553 - val_mse: 0.0351\n",
            "Epoch 196/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0194 - mse: 8.8030e-04 - val_loss: 0.2552 - val_mse: 0.0352\n",
            "Epoch 197/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0190 - mse: 6.9264e-04 - val_loss: 0.1920 - val_mse: 0.0254\n",
            "Epoch 198/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0193 - mse: 6.6828e-04 - val_loss: 0.2378 - val_mse: 0.0321\n",
            "Epoch 199/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0186 - mse: 6.8067e-04 - val_loss: 0.2351 - val_mse: 0.0317\n",
            "Epoch 200/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0184 - mse: 7.1179e-04 - val_loss: 0.2327 - val_mse: 0.0316\n",
            "Epoch 201/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0178 - mse: 6.5731e-04 - val_loss: 0.2411 - val_mse: 0.0329\n",
            "Epoch 202/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0179 - mse: 6.7796e-04 - val_loss: 0.2259 - val_mse: 0.0307\n",
            "Epoch 203/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0178 - mse: 7.0382e-04 - val_loss: 0.2373 - val_mse: 0.0326\n",
            "Epoch 204/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0174 - mse: 6.8333e-04 - val_loss: 0.2463 - val_mse: 0.0337\n",
            "Epoch 205/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0174 - mse: 6.2360e-04 - val_loss: 0.2063 - val_mse: 0.0273\n",
            "Epoch 206/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0170 - mse: 5.4159e-04 - val_loss: 0.2329 - val_mse: 0.0317\n",
            "Epoch 207/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0170 - mse: 6.9524e-04 - val_loss: 0.2529 - val_mse: 0.0349\n",
            "Epoch 208/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0164 - mse: 6.1387e-04 - val_loss: 0.2317 - val_mse: 0.0315\n",
            "Epoch 209/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0164 - mse: 5.5818e-04 - val_loss: 0.2140 - val_mse: 0.0288\n",
            "Epoch 210/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0163 - mse: 6.0031e-04 - val_loss: 0.2411 - val_mse: 0.0328\n",
            "Epoch 211/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0160 - mse: 5.1772e-04 - val_loss: 0.1996 - val_mse: 0.0266\n",
            "Epoch 212/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0158 - mse: 4.9404e-04 - val_loss: 0.2232 - val_mse: 0.0303\n",
            "Epoch 213/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0155 - mse: 5.2564e-04 - val_loss: 0.2243 - val_mse: 0.0306\n",
            "Epoch 214/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0154 - mse: 5.3357e-04 - val_loss: 0.2321 - val_mse: 0.0318\n",
            "Epoch 215/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0153 - mse: 5.2709e-04 - val_loss: 0.2256 - val_mse: 0.0306\n",
            "Epoch 216/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0153 - mse: 5.6784e-04 - val_loss: 0.2329 - val_mse: 0.0321\n",
            "Epoch 217/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0148 - mse: 5.1899e-04 - val_loss: 0.2235 - val_mse: 0.0305\n",
            "Epoch 218/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0147 - mse: 4.2438e-04 - val_loss: 0.1849 - val_mse: 0.0245\n",
            "Epoch 219/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0149 - mse: 4.8445e-04 - val_loss: 0.2282 - val_mse: 0.0310\n",
            "Epoch 220/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0144 - mse: 4.4942e-04 - val_loss: 0.1918 - val_mse: 0.0259\n",
            "Epoch 221/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0142 - mse: 4.3575e-04 - val_loss: 0.2126 - val_mse: 0.0291\n",
            "Epoch 222/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0140 - mse: 4.4752e-04 - val_loss: 0.2155 - val_mse: 0.0291\n",
            "Epoch 223/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0138 - mse: 4.4857e-04 - val_loss: 0.2259 - val_mse: 0.0308\n",
            "Epoch 224/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0138 - mse: 4.7762e-04 - val_loss: 0.2060 - val_mse: 0.0281\n",
            "Epoch 225/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0136 - mse: 4.4260e-04 - val_loss: 0.2066 - val_mse: 0.0281\n",
            "Epoch 226/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0135 - mse: 4.3058e-04 - val_loss: 0.2142 - val_mse: 0.0291\n",
            "Epoch 227/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0133 - mse: 3.6406e-04 - val_loss: 0.1870 - val_mse: 0.0248\n",
            "Epoch 228/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0135 - mse: 4.3800e-04 - val_loss: 0.2242 - val_mse: 0.0307\n",
            "Epoch 229/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0129 - mse: 4.0026e-04 - val_loss: 0.2101 - val_mse: 0.0285\n",
            "Epoch 230/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0127 - mse: 3.3134e-04 - val_loss: 0.1759 - val_mse: 0.0233\n",
            "Epoch 231/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0130 - mse: 3.2105e-04 - val_loss: 0.1749 - val_mse: 0.0231\n",
            "Epoch 232/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0124 - mse: 3.0227e-04 - val_loss: 0.2227 - val_mse: 0.0304\n",
            "Epoch 233/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0123 - mse: 3.4522e-04 - val_loss: 0.2287 - val_mse: 0.0315\n",
            "Epoch 234/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0123 - mse: 3.8551e-04 - val_loss: 0.2043 - val_mse: 0.0279\n",
            "Epoch 235/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0122 - mse: 3.5971e-04 - val_loss: 0.2063 - val_mse: 0.0279\n",
            "Epoch 236/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0119 - mse: 3.5152e-04 - val_loss: 0.2029 - val_mse: 0.0276\n",
            "Epoch 237/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 3.1982e-04 - val_loss: 0.1893 - val_mse: 0.0255\n",
            "Epoch 238/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0117 - mse: 2.9260e-04 - val_loss: 0.1970 - val_mse: 0.0266\n",
            "Epoch 239/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0116 - mse: 3.2864e-04 - val_loss: 0.2038 - val_mse: 0.0278\n",
            "Epoch 240/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 3.2227e-04 - val_loss: 0.2091 - val_mse: 0.0284\n",
            "Epoch 241/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0113 - mse: 2.9835e-04 - val_loss: 0.2046 - val_mse: 0.0279\n",
            "Epoch 242/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0112 - mse: 2.8947e-04 - val_loss: 0.1859 - val_mse: 0.0252\n",
            "Epoch 243/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0112 - mse: 3.2446e-04 - val_loss: 0.2108 - val_mse: 0.0288\n",
            "Epoch 244/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0113 - mse: 3.1946e-04 - val_loss: 0.1694 - val_mse: 0.0227\n",
            "Epoch 245/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0108 - mse: 2.4554e-04 - val_loss: 0.1976 - val_mse: 0.0268\n",
            "Epoch 246/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0108 - mse: 3.0601e-04 - val_loss: 0.2150 - val_mse: 0.0294\n",
            "Epoch 247/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0107 - mse: 2.7610e-04 - val_loss: 0.1827 - val_mse: 0.0247\n",
            "Epoch 248/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0104 - mse: 2.6398e-04 - val_loss: 0.1982 - val_mse: 0.0271\n",
            "Epoch 249/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0104 - mse: 2.7919e-04 - val_loss: 0.1851 - val_mse: 0.0251\n",
            "Epoch 250/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0103 - mse: 2.5050e-04 - val_loss: 0.1877 - val_mse: 0.0253\n",
            "Epoch 251/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0101 - mse: 2.5427e-04 - val_loss: 0.1849 - val_mse: 0.0251\n",
            "Epoch 252/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0101 - mse: 2.8076e-04 - val_loss: 0.1988 - val_mse: 0.0272\n",
            "Epoch 253/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0100 - mse: 2.7350e-04 - val_loss: 0.1876 - val_mse: 0.0254\n",
            "Epoch 254/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0098 - mse: 2.4037e-04 - val_loss: 0.1755 - val_mse: 0.0237\n",
            "Epoch 255/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0098 - mse: 2.0980e-04 - val_loss: 0.1637 - val_mse: 0.0216\n",
            "Epoch 256/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0096 - mse: 2.0298e-04 - val_loss: 0.1859 - val_mse: 0.0251\n",
            "Epoch 257/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0096 - mse: 2.3120e-04 - val_loss: 0.1892 - val_mse: 0.0258\n",
            "Epoch 258/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0094 - mse: 2.2756e-04 - val_loss: 0.1757 - val_mse: 0.0237\n",
            "Epoch 259/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0094 - mse: 2.2104e-04 - val_loss: 0.1836 - val_mse: 0.0249\n",
            "Epoch 260/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0093 - mse: 2.0805e-04 - val_loss: 0.1660 - val_mse: 0.0223\n",
            "Epoch 261/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0091 - mse: 1.9276e-04 - val_loss: 0.1695 - val_mse: 0.0227\n",
            "Epoch 262/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0091 - mse: 2.0653e-04 - val_loss: 0.1842 - val_mse: 0.0249\n",
            "Epoch 263/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0090 - mse: 2.0413e-04 - val_loss: 0.1678 - val_mse: 0.0226\n",
            "Epoch 264/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0089 - mse: 2.0281e-04 - val_loss: 0.1738 - val_mse: 0.0236\n",
            "Epoch 265/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0089 - mse: 2.1151e-04 - val_loss: 0.1815 - val_mse: 0.0246\n",
            "Epoch 266/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0087 - mse: 1.8837e-04 - val_loss: 0.1594 - val_mse: 0.0212\n",
            "Epoch 267/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0086 - mse: 1.6586e-04 - val_loss: 0.1685 - val_mse: 0.0228\n",
            "Epoch 268/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0086 - mse: 1.9253e-04 - val_loss: 0.1780 - val_mse: 0.0242\n",
            "Epoch 269/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0084 - mse: 1.7942e-04 - val_loss: 0.1723 - val_mse: 0.0234\n",
            "Epoch 270/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0085 - mse: 1.7693e-04 - val_loss: 0.1640 - val_mse: 0.0220\n",
            "Epoch 271/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0084 - mse: 1.8928e-04 - val_loss: 0.1815 - val_mse: 0.0247\n",
            "Epoch 272/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0082 - mse: 1.7630e-04 - val_loss: 0.1730 - val_mse: 0.0236\n",
            "Epoch 273/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0082 - mse: 1.7518e-04 - val_loss: 0.1745 - val_mse: 0.0237\n",
            "Epoch 274/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0080 - mse: 1.7064e-04 - val_loss: 0.1631 - val_mse: 0.0220\n",
            "Epoch 275/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0080 - mse: 1.5657e-04 - val_loss: 0.1655 - val_mse: 0.0225\n",
            "Epoch 276/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0079 - mse: 1.7445e-04 - val_loss: 0.1801 - val_mse: 0.0246\n",
            "Epoch 277/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0078 - mse: 1.8302e-04 - val_loss: 0.1710 - val_mse: 0.0233\n",
            "Epoch 278/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0077 - mse: 1.4627e-04 - val_loss: 0.1499 - val_mse: 0.0200\n",
            "Epoch 279/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0076 - mse: 1.3227e-04 - val_loss: 0.1641 - val_mse: 0.0223\n",
            "Epoch 280/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0077 - mse: 1.5814e-04 - val_loss: 0.1778 - val_mse: 0.0244\n",
            "Epoch 281/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0075 - mse: 1.6474e-04 - val_loss: 0.1739 - val_mse: 0.0238\n",
            "Epoch 282/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0075 - mse: 1.4438e-04 - val_loss: 0.1602 - val_mse: 0.0216\n",
            "Epoch 283/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0074 - mse: 1.3289e-04 - val_loss: 0.1555 - val_mse: 0.0210\n",
            "Epoch 284/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0076 - mse: 1.7118e-04 - val_loss: 0.1862 - val_mse: 0.0257\n",
            "Epoch 285/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0072 - mse: 1.3869e-04 - val_loss: 0.1463 - val_mse: 0.0196\n",
            "Epoch 286/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0072 - mse: 1.2157e-04 - val_loss: 0.1531 - val_mse: 0.0206\n",
            "Epoch 287/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0071 - mse: 1.3237e-04 - val_loss: 0.1665 - val_mse: 0.0228\n",
            "Epoch 288/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0070 - mse: 1.3278e-04 - val_loss: 0.1668 - val_mse: 0.0229\n",
            "Epoch 289/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0069 - mse: 1.3625e-04 - val_loss: 0.1660 - val_mse: 0.0227\n",
            "Epoch 290/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0068 - mse: 1.3555e-04 - val_loss: 0.1671 - val_mse: 0.0229\n",
            "Epoch 291/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0068 - mse: 1.2309e-04 - val_loss: 0.1495 - val_mse: 0.0202\n",
            "Epoch 292/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0068 - mse: 1.1904e-04 - val_loss: 0.1567 - val_mse: 0.0213\n",
            "Epoch 293/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0067 - mse: 1.1790e-04 - val_loss: 0.1567 - val_mse: 0.0212\n",
            "Epoch 294/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0066 - mse: 1.1215e-04 - val_loss: 0.1650 - val_mse: 0.0227\n",
            "Epoch 295/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0066 - mse: 1.2067e-04 - val_loss: 0.1607 - val_mse: 0.0220\n",
            "Epoch 296/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0065 - mse: 1.1171e-04 - val_loss: 0.1594 - val_mse: 0.0218\n",
            "Epoch 297/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0065 - mse: 1.2459e-04 - val_loss: 0.1664 - val_mse: 0.0229\n",
            "Epoch 298/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0064 - mse: 1.1134e-04 - val_loss: 0.1476 - val_mse: 0.0201\n",
            "Epoch 299/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0063 - mse: 1.0247e-04 - val_loss: 0.1562 - val_mse: 0.0213\n",
            "Epoch 300/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0063 - mse: 9.9467e-05 - val_loss: 0.1503 - val_mse: 0.0203\n",
            "Epoch 301/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0062 - mse: 9.0316e-05 - val_loss: 0.1568 - val_mse: 0.0216\n",
            "Epoch 302/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0063 - mse: 1.2044e-04 - val_loss: 0.1718 - val_mse: 0.0239\n",
            "Epoch 303/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0061 - mse: 8.9384e-05 - val_loss: 0.1361 - val_mse: 0.0182\n",
            "Epoch 304/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0061 - mse: 8.5266e-05 - val_loss: 0.1452 - val_mse: 0.0197\n",
            "Epoch 305/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0060 - mse: 9.4038e-05 - val_loss: 0.1675 - val_mse: 0.0231\n",
            "Epoch 306/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0059 - mse: 9.5525e-05 - val_loss: 0.1569 - val_mse: 0.0217\n",
            "Epoch 307/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0059 - mse: 1.0257e-04 - val_loss: 0.1634 - val_mse: 0.0227\n",
            "Epoch 308/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0058 - mse: 9.6230e-05 - val_loss: 0.1539 - val_mse: 0.0212\n",
            "Epoch 309/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0057 - mse: 8.8958e-05 - val_loss: 0.1523 - val_mse: 0.0210\n",
            "Epoch 310/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0058 - mse: 9.6965e-05 - val_loss: 0.1523 - val_mse: 0.0210\n",
            "Epoch 311/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0056 - mse: 7.9796e-05 - val_loss: 0.1356 - val_mse: 0.0184\n",
            "Epoch 312/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0056 - mse: 7.1464e-05 - val_loss: 0.1403 - val_mse: 0.0191\n",
            "Epoch 313/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0056 - mse: 8.7244e-05 - val_loss: 0.1681 - val_mse: 0.0234\n",
            "Epoch 314/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0056 - mse: 8.7742e-05 - val_loss: 0.1498 - val_mse: 0.0209\n",
            "Epoch 315/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0055 - mse: 8.0413e-05 - val_loss: 0.1471 - val_mse: 0.0202\n",
            "Epoch 316/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0054 - mse: 8.8776e-05 - val_loss: 0.1606 - val_mse: 0.0224\n",
            "Epoch 317/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0055 - mse: 8.5797e-05 - val_loss: 0.1437 - val_mse: 0.0198\n",
            "Epoch 318/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0053 - mse: 7.1654e-05 - val_loss: 0.1474 - val_mse: 0.0204\n",
            "Epoch 319/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0053 - mse: 7.5773e-05 - val_loss: 0.1571 - val_mse: 0.0218\n",
            "Epoch 320/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0052 - mse: 7.5790e-05 - val_loss: 0.1475 - val_mse: 0.0204\n",
            "Epoch 321/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0052 - mse: 7.6805e-05 - val_loss: 0.1525 - val_mse: 0.0212\n",
            "Epoch 322/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0052 - mse: 8.0116e-05 - val_loss: 0.1456 - val_mse: 0.0202\n",
            "Epoch 323/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0051 - mse: 7.0123e-05 - val_loss: 0.1473 - val_mse: 0.0204\n",
            "Epoch 324/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0051 - mse: 7.7279e-05 - val_loss: 0.1548 - val_mse: 0.0216\n",
            "Epoch 325/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0050 - mse: 7.0501e-05 - val_loss: 0.1478 - val_mse: 0.0205\n",
            "Epoch 326/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0049 - mse: 6.4534e-05 - val_loss: 0.1428 - val_mse: 0.0198\n",
            "Epoch 327/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0049 - mse: 6.3442e-05 - val_loss: 0.1496 - val_mse: 0.0208\n",
            "Epoch 328/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0050 - mse: 8.5096e-05 - val_loss: 0.1679 - val_mse: 0.0236\n",
            "Epoch 329/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0048 - mse: 6.3994e-05 - val_loss: 0.1334 - val_mse: 0.0183\n",
            "Epoch 330/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0048 - mse: 6.4040e-05 - val_loss: 0.1467 - val_mse: 0.0204\n",
            "Epoch 331/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0048 - mse: 5.9258e-05 - val_loss: 0.1303 - val_mse: 0.0178\n",
            "Epoch 332/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0047 - mse: 5.4605e-05 - val_loss: 0.1480 - val_mse: 0.0206\n",
            "Epoch 333/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0047 - mse: 6.2906e-05 - val_loss: 0.1552 - val_mse: 0.0218\n",
            "Epoch 334/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0047 - mse: 5.8860e-05 - val_loss: 0.1328 - val_mse: 0.0181\n",
            "Epoch 335/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0047 - mse: 6.6250e-05 - val_loss: 0.1525 - val_mse: 0.0214\n",
            "Epoch 336/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0046 - mse: 5.8009e-05 - val_loss: 0.1312 - val_mse: 0.0181\n",
            "Epoch 337/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 4.7931e-05 - val_loss: 0.1421 - val_mse: 0.0197\n",
            "Epoch 338/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 5.3159e-05 - val_loss: 0.1514 - val_mse: 0.0213\n",
            "Epoch 339/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 5.9892e-05 - val_loss: 0.1492 - val_mse: 0.0210\n",
            "Epoch 340/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 5.9059e-05 - val_loss: 0.1530 - val_mse: 0.0215\n",
            "Epoch 341/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0043 - mse: 5.5832e-05 - val_loss: 0.1444 - val_mse: 0.0202\n",
            "Epoch 342/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0043 - mse: 5.2668e-05 - val_loss: 0.1452 - val_mse: 0.0204\n",
            "Epoch 343/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0043 - mse: 5.2091e-05 - val_loss: 0.1365 - val_mse: 0.0190\n",
            "Epoch 344/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0042 - mse: 4.8612e-05 - val_loss: 0.1405 - val_mse: 0.0196\n",
            "Epoch 345/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0042 - mse: 4.6979e-05 - val_loss: 0.1443 - val_mse: 0.0202\n",
            "Epoch 346/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0042 - mse: 5.0336e-05 - val_loss: 0.1454 - val_mse: 0.0204\n",
            "Epoch 347/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0041 - mse: 4.7471e-05 - val_loss: 0.1366 - val_mse: 0.0191\n",
            "Epoch 348/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0041 - mse: 4.5331e-05 - val_loss: 0.1391 - val_mse: 0.0196\n",
            "Epoch 349/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0041 - mse: 5.1484e-05 - val_loss: 0.1477 - val_mse: 0.0208\n",
            "Epoch 350/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0040 - mse: 4.5500e-05 - val_loss: 0.1334 - val_mse: 0.0186\n",
            "Epoch 351/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0040 - mse: 4.2472e-05 - val_loss: 0.1366 - val_mse: 0.0191\n",
            "Epoch 352/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0040 - mse: 4.4714e-05 - val_loss: 0.1351 - val_mse: 0.0189\n",
            "Epoch 353/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0039 - mse: 4.2435e-05 - val_loss: 0.1371 - val_mse: 0.0193\n",
            "Epoch 354/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0040 - mse: 4.4032e-05 - val_loss: 0.1399 - val_mse: 0.0198\n",
            "Epoch 355/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0038 - mse: 4.3610e-05 - val_loss: 0.1441 - val_mse: 0.0203\n",
            "Epoch 356/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0039 - mse: 4.7196e-05 - val_loss: 0.1491 - val_mse: 0.0210\n",
            "Epoch 357/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0038 - mse: 4.3314e-05 - val_loss: 0.1342 - val_mse: 0.0188\n",
            "Epoch 358/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0038 - mse: 3.9112e-05 - val_loss: 0.1361 - val_mse: 0.0192\n",
            "Epoch 359/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0038 - mse: 4.8238e-05 - val_loss: 0.1452 - val_mse: 0.0207\n",
            "Epoch 360/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0037 - mse: 4.3551e-05 - val_loss: 0.1327 - val_mse: 0.0186\n",
            "Epoch 361/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0037 - mse: 3.7317e-05 - val_loss: 0.1338 - val_mse: 0.0188\n",
            "Epoch 362/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0037 - mse: 4.0887e-05 - val_loss: 0.1377 - val_mse: 0.0195\n",
            "Epoch 363/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0036 - mse: 3.3932e-05 - val_loss: 0.1238 - val_mse: 0.0172\n",
            "Epoch 364/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0037 - mse: 3.2911e-05 - val_loss: 0.1188 - val_mse: 0.0164\n",
            "Epoch 365/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0036 - mse: 3.7340e-05 - val_loss: 0.1412 - val_mse: 0.0200\n",
            "Epoch 366/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0036 - mse: 3.6935e-05 - val_loss: 0.1288 - val_mse: 0.0182\n",
            "Epoch 367/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0035 - mse: 3.2463e-05 - val_loss: 0.1254 - val_mse: 0.0176\n",
            "Epoch 368/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0035 - mse: 3.4890e-05 - val_loss: 0.1394 - val_mse: 0.0197\n",
            "Epoch 369/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0035 - mse: 3.9195e-05 - val_loss: 0.1436 - val_mse: 0.0204\n",
            "Epoch 370/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0034 - mse: 3.4754e-05 - val_loss: 0.1207 - val_mse: 0.0170\n",
            "Epoch 371/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0034 - mse: 3.0419e-05 - val_loss: 0.1214 - val_mse: 0.0171\n",
            "Epoch 372/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0034 - mse: 3.1865e-05 - val_loss: 0.1300 - val_mse: 0.0185\n",
            "Epoch 373/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0035 - mse: 3.2786e-05 - val_loss: 0.1200 - val_mse: 0.0167\n",
            "Epoch 374/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0034 - mse: 2.7828e-05 - val_loss: 0.1394 - val_mse: 0.0199\n",
            "Epoch 375/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0033 - mse: 3.5163e-05 - val_loss: 0.1437 - val_mse: 0.0206\n",
            "Epoch 376/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0034 - mse: 3.3854e-05 - val_loss: 0.1199 - val_mse: 0.0168\n",
            "Epoch 377/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0032 - mse: 2.5638e-05 - val_loss: 0.1311 - val_mse: 0.0187\n",
            "Epoch 378/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0033 - mse: 3.7776e-05 - val_loss: 0.1579 - val_mse: 0.0228\n",
            "Epoch 379/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0031 - mse: 2.8094e-05 - val_loss: 0.1253 - val_mse: 0.0177\n",
            "Epoch 380/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0032 - mse: 2.5352e-05 - val_loss: 0.1177 - val_mse: 0.0166\n",
            "Epoch 381/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0031 - mse: 2.5787e-05 - val_loss: 0.1263 - val_mse: 0.0179\n",
            "Epoch 382/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0031 - mse: 2.5807e-05 - val_loss: 0.1331 - val_mse: 0.0189\n",
            "Epoch 383/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0031 - mse: 2.3092e-05 - val_loss: 0.1235 - val_mse: 0.0175\n",
            "Epoch 384/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0031 - mse: 2.2125e-05 - val_loss: 0.1199 - val_mse: 0.0169\n",
            "Epoch 385/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0030 - mse: 2.2943e-05 - val_loss: 0.1340 - val_mse: 0.0192\n",
            "Epoch 386/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0030 - mse: 2.8634e-05 - val_loss: 0.1429 - val_mse: 0.0205\n",
            "Epoch 387/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0030 - mse: 2.6304e-05 - val_loss: 0.1269 - val_mse: 0.0180\n",
            "Epoch 388/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0030 - mse: 2.3976e-05 - val_loss: 0.1266 - val_mse: 0.0181\n",
            "Epoch 389/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0030 - mse: 2.5847e-05 - val_loss: 0.1358 - val_mse: 0.0196\n",
            "Epoch 390/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0029 - mse: 2.6331e-05 - val_loss: 0.1342 - val_mse: 0.0192\n",
            "Epoch 391/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0029 - mse: 2.4762e-05 - val_loss: 0.1152 - val_mse: 0.0163\n",
            "Epoch 392/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0029 - mse: 2.1877e-05 - val_loss: 0.1293 - val_mse: 0.0185\n",
            "Epoch 393/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0029 - mse: 2.2423e-05 - val_loss: 0.1300 - val_mse: 0.0185\n",
            "Epoch 394/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0028 - mse: 2.3212e-05 - val_loss: 0.1342 - val_mse: 0.0194\n",
            "Epoch 395/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0028 - mse: 2.5792e-05 - val_loss: 0.1394 - val_mse: 0.0201\n",
            "Epoch 396/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0028 - mse: 2.3138e-05 - val_loss: 0.1236 - val_mse: 0.0176\n",
            "Epoch 397/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0028 - mse: 2.0463e-05 - val_loss: 0.1156 - val_mse: 0.0165\n",
            "Epoch 398/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0027 - mse: 1.9770e-05 - val_loss: 0.1220 - val_mse: 0.0174\n",
            "Epoch 399/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0027 - mse: 1.9882e-05 - val_loss: 0.1302 - val_mse: 0.0187\n",
            "Epoch 400/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0027 - mse: 2.4288e-05 - val_loss: 0.1339 - val_mse: 0.0193\n",
            "Epoch 401/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0027 - mse: 2.1643e-05 - val_loss: 0.1193 - val_mse: 0.0171\n",
            "Epoch 402/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0026 - mse: 1.8190e-05 - val_loss: 0.1232 - val_mse: 0.0177\n",
            "Epoch 403/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0026 - mse: 1.8034e-05 - val_loss: 0.1282 - val_mse: 0.0184\n",
            "Epoch 404/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0026 - mse: 2.0116e-05 - val_loss: 0.1306 - val_mse: 0.0190\n",
            "Epoch 405/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0026 - mse: 1.8965e-05 - val_loss: 0.1177 - val_mse: 0.0169\n",
            "Epoch 406/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0026 - mse: 1.9277e-05 - val_loss: 0.1264 - val_mse: 0.0182\n",
            "Epoch 407/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0026 - mse: 1.8735e-05 - val_loss: 0.1229 - val_mse: 0.0178\n",
            "Epoch 408/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0025 - mse: 1.9006e-05 - val_loss: 0.1271 - val_mse: 0.0184\n",
            "Epoch 409/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0025 - mse: 2.0663e-05 - val_loss: 0.1316 - val_mse: 0.0191\n",
            "Epoch 410/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0025 - mse: 1.9267e-05 - val_loss: 0.1276 - val_mse: 0.0185\n",
            "Epoch 411/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0025 - mse: 1.5218e-05 - val_loss: 0.1093 - val_mse: 0.0156\n",
            "Epoch 412/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0025 - mse: 1.4891e-05 - val_loss: 0.1179 - val_mse: 0.0171\n",
            "Epoch 413/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0025 - mse: 1.6039e-05 - val_loss: 0.1201 - val_mse: 0.0174\n",
            "Epoch 414/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0024 - mse: 1.6178e-05 - val_loss: 0.1247 - val_mse: 0.0181\n",
            "Epoch 415/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0024 - mse: 1.6140e-05 - val_loss: 0.1276 - val_mse: 0.0185\n",
            "Epoch 416/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0024 - mse: 1.5959e-05 - val_loss: 0.1279 - val_mse: 0.0185\n",
            "Epoch 417/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0024 - mse: 1.6189e-05 - val_loss: 0.1182 - val_mse: 0.0172\n",
            "Epoch 418/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0023 - mse: 1.6084e-05 - val_loss: 0.1204 - val_mse: 0.0175\n",
            "Epoch 419/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0023 - mse: 1.5134e-05 - val_loss: 0.1136 - val_mse: 0.0163\n",
            "Epoch 420/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0023 - mse: 1.5216e-05 - val_loss: 0.1286 - val_mse: 0.0187\n",
            "Epoch 421/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0023 - mse: 1.6031e-05 - val_loss: 0.1260 - val_mse: 0.0183\n",
            "Epoch 422/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0023 - mse: 1.4754e-05 - val_loss: 0.1197 - val_mse: 0.0173\n",
            "Epoch 423/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0023 - mse: 1.5599e-05 - val_loss: 0.1222 - val_mse: 0.0178\n",
            "Epoch 424/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0023 - mse: 1.4899e-05 - val_loss: 0.1122 - val_mse: 0.0162\n",
            "Epoch 425/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0022 - mse: 1.3694e-05 - val_loss: 0.1312 - val_mse: 0.0192\n",
            "Epoch 426/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0022 - mse: 1.6032e-05 - val_loss: 0.1313 - val_mse: 0.0192\n",
            "Epoch 427/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0022 - mse: 1.4822e-05 - val_loss: 0.1217 - val_mse: 0.0177\n",
            "Epoch 428/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0022 - mse: 1.4415e-05 - val_loss: 0.1148 - val_mse: 0.0167\n",
            "Epoch 429/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0022 - mse: 1.2723e-05 - val_loss: 0.1127 - val_mse: 0.0163\n",
            "Epoch 430/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0022 - mse: 1.1983e-05 - val_loss: 0.1121 - val_mse: 0.0161\n",
            "Epoch 431/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0021 - mse: 1.1383e-05 - val_loss: 0.1194 - val_mse: 0.0174\n",
            "Epoch 432/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0021 - mse: 1.5403e-05 - val_loss: 0.1344 - val_mse: 0.0197\n",
            "Epoch 433/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0021 - mse: 1.3573e-05 - val_loss: 0.1224 - val_mse: 0.0179\n",
            "Epoch 434/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0021 - mse: 1.3065e-05 - val_loss: 0.1119 - val_mse: 0.0162\n",
            "Epoch 435/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0021 - mse: 1.3633e-05 - val_loss: 0.1280 - val_mse: 0.0188\n",
            "Epoch 436/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0020 - mse: 1.2814e-05 - val_loss: 0.1182 - val_mse: 0.0173\n",
            "Epoch 437/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0020 - mse: 1.2238e-05 - val_loss: 0.1135 - val_mse: 0.0166\n",
            "Epoch 438/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0020 - mse: 1.1475e-05 - val_loss: 0.1208 - val_mse: 0.0177\n",
            "Epoch 439/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0020 - mse: 1.3418e-05 - val_loss: 0.1254 - val_mse: 0.0184\n",
            "Epoch 440/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0020 - mse: 1.1747e-05 - val_loss: 0.1048 - val_mse: 0.0150\n",
            "Epoch 441/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0020 - mse: 1.0376e-05 - val_loss: 0.1121 - val_mse: 0.0163\n",
            "Epoch 442/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0020 - mse: 1.1835e-05 - val_loss: 0.1241 - val_mse: 0.0181\n",
            "Epoch 443/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0020 - mse: 1.1396e-05 - val_loss: 0.1210 - val_mse: 0.0178\n",
            "Epoch 444/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0019 - mse: 1.1377e-05 - val_loss: 0.1174 - val_mse: 0.0172\n",
            "Epoch 445/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0019 - mse: 1.2985e-05 - val_loss: 0.1292 - val_mse: 0.0191\n",
            "Epoch 446/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0020 - mse: 1.1907e-05 - val_loss: 0.1008 - val_mse: 0.0144\n",
            "Epoch 447/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0019 - mse: 9.2063e-06 - val_loss: 0.1057 - val_mse: 0.0153\n",
            "Epoch 448/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0019 - mse: 9.9319e-06 - val_loss: 0.1249 - val_mse: 0.0184\n",
            "Epoch 449/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0019 - mse: 1.0718e-05 - val_loss: 0.1299 - val_mse: 0.0192\n",
            "Epoch 450/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0019 - mse: 1.2150e-05 - val_loss: 0.1242 - val_mse: 0.0184\n",
            "Epoch 451/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0018 - mse: 1.0154e-05 - val_loss: 0.1115 - val_mse: 0.0162\n",
            "Epoch 452/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0018 - mse: 8.7741e-06 - val_loss: 0.1107 - val_mse: 0.0162\n",
            "Epoch 453/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0018 - mse: 8.2840e-06 - val_loss: 0.1153 - val_mse: 0.0170\n",
            "Epoch 454/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0018 - mse: 9.2298e-06 - val_loss: 0.1254 - val_mse: 0.0187\n",
            "Epoch 455/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0018 - mse: 1.1400e-05 - val_loss: 0.1305 - val_mse: 0.0194\n",
            "Epoch 456/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0018 - mse: 9.6890e-06 - val_loss: 0.0983 - val_mse: 0.0142\n",
            "Epoch 457/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0018 - mse: 7.9927e-06 - val_loss: 0.1127 - val_mse: 0.0166\n",
            "Epoch 458/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0017 - mse: 8.3679e-06 - val_loss: 0.1227 - val_mse: 0.0182\n",
            "Epoch 459/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0018 - mse: 8.6810e-06 - val_loss: 0.1191 - val_mse: 0.0176\n",
            "Epoch 460/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0017 - mse: 9.1743e-06 - val_loss: 0.1237 - val_mse: 0.0184\n",
            "Epoch 461/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0017 - mse: 8.7133e-06 - val_loss: 0.1211 - val_mse: 0.0181\n",
            "Epoch 462/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0017 - mse: 9.2706e-06 - val_loss: 0.1277 - val_mse: 0.0190\n",
            "Epoch 463/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0017 - mse: 7.9161e-06 - val_loss: 0.1079 - val_mse: 0.0158\n",
            "Epoch 464/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0017 - mse: 7.4285e-06 - val_loss: 0.1120 - val_mse: 0.0165\n",
            "Epoch 465/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0017 - mse: 7.3840e-06 - val_loss: 0.1135 - val_mse: 0.0168\n",
            "Epoch 466/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0016 - mse: 7.2980e-06 - val_loss: 0.1193 - val_mse: 0.0178\n",
            "Epoch 467/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0017 - mse: 8.6190e-06 - val_loss: 0.1220 - val_mse: 0.0181\n",
            "Epoch 468/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0016 - mse: 8.6092e-06 - val_loss: 0.1236 - val_mse: 0.0185\n",
            "Epoch 469/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0016 - mse: 7.9517e-06 - val_loss: 0.1154 - val_mse: 0.0171\n",
            "Epoch 470/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0016 - mse: 7.2000e-06 - val_loss: 0.1126 - val_mse: 0.0167\n",
            "Epoch 471/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0016 - mse: 8.5938e-06 - val_loss: 0.1277 - val_mse: 0.0191\n",
            "Epoch 472/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0016 - mse: 7.7180e-06 - val_loss: 0.1196 - val_mse: 0.0178\n",
            "Epoch 473/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0016 - mse: 7.2450e-06 - val_loss: 0.1166 - val_mse: 0.0174\n",
            "Epoch 474/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0016 - mse: 7.1079e-06 - val_loss: 0.1157 - val_mse: 0.0172\n",
            "Epoch 475/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0016 - mse: 6.7574e-06 - val_loss: 0.1115 - val_mse: 0.0165\n",
            "Epoch 476/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0015 - mse: 6.3198e-06 - val_loss: 0.1248 - val_mse: 0.0187\n",
            "Epoch 477/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0015 - mse: 8.1685e-06 - val_loss: 0.1264 - val_mse: 0.0190\n",
            "Epoch 478/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0015 - mse: 7.2551e-06 - val_loss: 0.1153 - val_mse: 0.0172\n",
            "Epoch 479/1000\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0015 - mse: 7.7608e-06 - val_loss: 0.1210 - val_mse: 0.0181\n",
            "Epoch 480/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0015 - mse: 6.6762e-06 - val_loss: 0.1111 - val_mse: 0.0165\n",
            "Epoch 481/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0015 - mse: 6.0452e-06 - val_loss: 0.1095 - val_mse: 0.0163\n",
            "Epoch 482/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0015 - mse: 6.4772e-06 - val_loss: 0.1147 - val_mse: 0.0172\n",
            "Epoch 483/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0015 - mse: 6.7655e-06 - val_loss: 0.1179 - val_mse: 0.0176\n",
            "Epoch 484/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0015 - mse: 6.9964e-06 - val_loss: 0.1168 - val_mse: 0.0174\n",
            "Epoch 485/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0014 - mse: 6.1849e-06 - val_loss: 0.1136 - val_mse: 0.0170\n",
            "Epoch 486/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0014 - mse: 5.7672e-06 - val_loss: 0.1083 - val_mse: 0.0162\n",
            "Epoch 487/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0015 - mse: 7.9542e-06 - val_loss: 0.1249 - val_mse: 0.0189\n",
            "Epoch 488/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0014 - mse: 5.4451e-06 - val_loss: 0.0974 - val_mse: 0.0140\n",
            "Epoch 489/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0014 - mse: 5.7943e-06 - val_loss: 0.1024 - val_mse: 0.0150\n",
            "Epoch 490/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0014 - mse: 6.2861e-06 - val_loss: 0.1163 - val_mse: 0.0174\n",
            "Epoch 491/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0014 - mse: 6.0001e-06 - val_loss: 0.0957 - val_mse: 0.0139\n",
            "Epoch 492/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0014 - mse: 4.8606e-06 - val_loss: 0.1089 - val_mse: 0.0163\n",
            "Epoch 493/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0014 - mse: 5.2136e-06 - val_loss: 0.1206 - val_mse: 0.0182\n",
            "Epoch 494/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0014 - mse: 5.8817e-06 - val_loss: 0.1246 - val_mse: 0.0189\n",
            "Epoch 495/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0014 - mse: 5.5699e-06 - val_loss: 0.1105 - val_mse: 0.0166\n",
            "Epoch 496/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0013 - mse: 5.4214e-06 - val_loss: 0.1202 - val_mse: 0.0182\n",
            "Epoch 497/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 5.1351e-06 - val_loss: 0.1125 - val_mse: 0.0169\n",
            "Epoch 498/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0013 - mse: 4.8641e-06 - val_loss: 0.1176 - val_mse: 0.0178\n",
            "Epoch 499/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0013 - mse: 5.2303e-06 - val_loss: 0.1170 - val_mse: 0.0177\n",
            "Epoch 500/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0013 - mse: 4.9282e-06 - val_loss: 0.1117 - val_mse: 0.0168\n",
            "Epoch 501/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0013 - mse: 5.3307e-06 - val_loss: 0.1209 - val_mse: 0.0183\n",
            "Epoch 502/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0013 - mse: 5.0017e-06 - val_loss: 0.1168 - val_mse: 0.0176\n",
            "Epoch 503/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 4.7420e-06 - val_loss: 0.1035 - val_mse: 0.0156\n",
            "Epoch 504/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0013 - mse: 4.7527e-06 - val_loss: 0.1127 - val_mse: 0.0171\n",
            "Epoch 505/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0013 - mse: 4.6253e-06 - val_loss: 0.1139 - val_mse: 0.0172\n",
            "Epoch 506/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 4.9475e-06 - val_loss: 0.1176 - val_mse: 0.0178\n",
            "Epoch 507/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0012 - mse: 4.5187e-06 - val_loss: 0.1125 - val_mse: 0.0170\n",
            "Epoch 508/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 4.4731e-06 - val_loss: 0.1143 - val_mse: 0.0174\n",
            "Epoch 509/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0012 - mse: 4.4749e-06 - val_loss: 0.1161 - val_mse: 0.0175\n",
            "Epoch 510/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0012 - mse: 4.7013e-06 - val_loss: 0.1170 - val_mse: 0.0177\n",
            "Epoch 511/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0012 - mse: 4.2776e-06 - val_loss: 0.1040 - val_mse: 0.0156\n",
            "Epoch 512/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0012 - mse: 4.7821e-06 - val_loss: 0.1236 - val_mse: 0.0189\n",
            "Epoch 513/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0012 - mse: 4.1225e-06 - val_loss: 0.1109 - val_mse: 0.0168\n",
            "Epoch 514/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0012 - mse: 3.8105e-06 - val_loss: 0.1052 - val_mse: 0.0159\n",
            "Epoch 515/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0012 - mse: 4.5879e-06 - val_loss: 0.1172 - val_mse: 0.0179\n",
            "Epoch 516/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 4.1201e-06 - val_loss: 0.1155 - val_mse: 0.0175\n",
            "Epoch 517/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 4.1160e-06 - val_loss: 0.1139 - val_mse: 0.0174\n",
            "Epoch 518/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0011 - mse: 3.9718e-06 - val_loss: 0.1126 - val_mse: 0.0172\n",
            "Epoch 519/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0011 - mse: 3.6575e-06 - val_loss: 0.1076 - val_mse: 0.0164\n",
            "Epoch 520/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0011 - mse: 3.5121e-06 - val_loss: 0.1185 - val_mse: 0.0182\n",
            "Epoch 521/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0011 - mse: 4.1313e-06 - val_loss: 0.1154 - val_mse: 0.0176\n",
            "Epoch 522/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0011 - mse: 4.0472e-06 - val_loss: 0.1093 - val_mse: 0.0166\n",
            "Epoch 523/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0011 - mse: 4.5126e-06 - val_loss: 0.1166 - val_mse: 0.0178\n",
            "Epoch 524/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0011 - mse: 3.9181e-06 - val_loss: 0.0959 - val_mse: 0.0143\n",
            "Epoch 525/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 3.3671e-06 - val_loss: 0.1068 - val_mse: 0.0163\n",
            "Epoch 526/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0011 - mse: 3.1024e-06 - val_loss: 0.1030 - val_mse: 0.0156\n",
            "Epoch 527/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 3.0076e-06 - val_loss: 0.1148 - val_mse: 0.0176\n",
            "Epoch 528/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0011 - mse: 3.3716e-06 - val_loss: 0.1225 - val_mse: 0.0189\n",
            "Epoch 529/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0011 - mse: 3.5516e-06 - val_loss: 0.1183 - val_mse: 0.0182\n",
            "Epoch 530/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0011 - mse: 3.5467e-06 - val_loss: 0.1095 - val_mse: 0.0168\n",
            "Epoch 531/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0011 - mse: 3.1362e-06 - val_loss: 0.1120 - val_mse: 0.0172\n",
            "Epoch 532/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0010 - mse: 3.5489e-06 - val_loss: 0.1209 - val_mse: 0.0186\n",
            "Epoch 533/1000\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0010 - mse: 3.4137e-06 - val_loss: 0.1161 - val_mse: 0.0178\n",
            "Epoch 534/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0010 - mse: 3.4605e-06 - val_loss: 0.1148 - val_mse: 0.0177\n",
            "Epoch 535/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0010 - mse: 3.1072e-06 - val_loss: 0.1050 - val_mse: 0.0161\n",
            "Epoch 536/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0010 - mse: 2.9417e-06 - val_loss: 0.1121 - val_mse: 0.0172\n",
            "Epoch 537/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0010 - mse: 3.0269e-06 - val_loss: 0.1161 - val_mse: 0.0179\n",
            "Epoch 538/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0010 - mse: 3.1787e-06 - val_loss: 0.1139 - val_mse: 0.0176\n",
            "Epoch 539/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 9.9703e-04 - mse: 3.2265e-06 - val_loss: 0.1146 - val_mse: 0.0176\n",
            "Epoch 540/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.9853e-04 - mse: 3.0427e-06 - val_loss: 0.1070 - val_mse: 0.0165\n",
            "Epoch 541/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.9325e-04 - mse: 3.4182e-06 - val_loss: 0.1195 - val_mse: 0.0185\n",
            "Epoch 542/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.8014e-04 - mse: 3.1169e-06 - val_loss: 0.1103 - val_mse: 0.0170\n",
            "Epoch 543/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 9.7631e-04 - mse: 2.7903e-06 - val_loss: 0.1111 - val_mse: 0.0172\n",
            "Epoch 544/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.6676e-04 - mse: 2.9509e-06 - val_loss: 0.1187 - val_mse: 0.0184\n",
            "Epoch 545/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 9.5611e-04 - mse: 2.7722e-06 - val_loss: 0.1087 - val_mse: 0.0167\n",
            "Epoch 546/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.5730e-04 - mse: 2.6951e-06 - val_loss: 0.1095 - val_mse: 0.0169\n",
            "Epoch 547/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.4648e-04 - mse: 2.7685e-06 - val_loss: 0.1159 - val_mse: 0.0180\n",
            "Epoch 548/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.3959e-04 - mse: 2.8368e-06 - val_loss: 0.1151 - val_mse: 0.0179\n",
            "Epoch 549/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.3113e-04 - mse: 2.4685e-06 - val_loss: 0.1042 - val_mse: 0.0160\n",
            "Epoch 550/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 9.2250e-04 - mse: 2.3227e-06 - val_loss: 0.1086 - val_mse: 0.0168\n",
            "Epoch 551/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.2156e-04 - mse: 2.3995e-06 - val_loss: 0.1073 - val_mse: 0.0166\n",
            "Epoch 552/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.1273e-04 - mse: 2.4276e-06 - val_loss: 0.1128 - val_mse: 0.0176\n",
            "Epoch 553/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.0838e-04 - mse: 2.5355e-06 - val_loss: 0.1162 - val_mse: 0.0180\n",
            "Epoch 554/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.0264e-04 - mse: 2.5195e-06 - val_loss: 0.1133 - val_mse: 0.0176\n",
            "Epoch 555/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.9400e-04 - mse: 2.4719e-06 - val_loss: 0.1179 - val_mse: 0.0183\n",
            "Epoch 556/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.9868e-04 - mse: 2.8302e-06 - val_loss: 0.1142 - val_mse: 0.0178\n",
            "Epoch 557/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.8577e-04 - mse: 2.4102e-06 - val_loss: 0.1041 - val_mse: 0.0161\n",
            "Epoch 558/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.8276e-04 - mse: 2.1953e-06 - val_loss: 0.1092 - val_mse: 0.0169\n",
            "Epoch 559/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.6824e-04 - mse: 2.2310e-06 - val_loss: 0.1155 - val_mse: 0.0180\n",
            "Epoch 560/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.6997e-04 - mse: 2.5728e-06 - val_loss: 0.1164 - val_mse: 0.0182\n",
            "Epoch 561/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.5445e-04 - mse: 2.0885e-06 - val_loss: 0.1018 - val_mse: 0.0158\n",
            "Epoch 562/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 8.6920e-04 - mse: 2.0960e-06 - val_loss: 0.1034 - val_mse: 0.0161\n",
            "Epoch 563/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.4753e-04 - mse: 1.9874e-06 - val_loss: 0.1233 - val_mse: 0.0193\n",
            "Epoch 564/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.4925e-04 - mse: 2.4894e-06 - val_loss: 0.1189 - val_mse: 0.0186\n",
            "Epoch 565/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 8.4311e-04 - mse: 2.2341e-06 - val_loss: 0.1099 - val_mse: 0.0171\n",
            "Epoch 566/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.3228e-04 - mse: 2.0015e-06 - val_loss: 0.1075 - val_mse: 0.0168\n",
            "Epoch 567/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.2006e-04 - mse: 1.9941e-06 - val_loss: 0.1213 - val_mse: 0.0190\n",
            "Epoch 568/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 8.2188e-04 - mse: 2.2376e-06 - val_loss: 0.1195 - val_mse: 0.0187\n",
            "Epoch 569/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.1546e-04 - mse: 2.2173e-06 - val_loss: 0.1177 - val_mse: 0.0184\n",
            "Epoch 570/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.0882e-04 - mse: 1.9589e-06 - val_loss: 0.1080 - val_mse: 0.0168\n",
            "Epoch 571/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.1371e-04 - mse: 1.8440e-06 - val_loss: 0.1087 - val_mse: 0.0170\n",
            "Epoch 572/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.9776e-04 - mse: 1.8245e-06 - val_loss: 0.1139 - val_mse: 0.0178\n",
            "Epoch 573/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.9272e-04 - mse: 1.8614e-06 - val_loss: 0.1172 - val_mse: 0.0184\n",
            "Epoch 574/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 7.9343e-04 - mse: 2.1412e-06 - val_loss: 0.1202 - val_mse: 0.0189\n",
            "Epoch 575/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.8388e-04 - mse: 1.9789e-06 - val_loss: 0.1104 - val_mse: 0.0174\n",
            "Epoch 576/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.8203e-04 - mse: 2.0060e-06 - val_loss: 0.1185 - val_mse: 0.0187\n",
            "Epoch 577/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.7184e-04 - mse: 1.8485e-06 - val_loss: 0.1078 - val_mse: 0.0169\n",
            "Epoch 578/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.6947e-04 - mse: 1.7933e-06 - val_loss: 0.1102 - val_mse: 0.0173\n",
            "Epoch 579/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.5864e-04 - mse: 1.7543e-06 - val_loss: 0.1128 - val_mse: 0.0177\n",
            "Epoch 580/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 7.5787e-04 - mse: 1.7010e-06 - val_loss: 0.1152 - val_mse: 0.0181\n",
            "Epoch 581/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.5657e-04 - mse: 1.7249e-06 - val_loss: 0.1148 - val_mse: 0.0181\n",
            "Epoch 582/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.4906e-04 - mse: 1.7447e-06 - val_loss: 0.1105 - val_mse: 0.0173\n",
            "Epoch 583/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 7.4391e-04 - mse: 1.7809e-06 - val_loss: 0.1152 - val_mse: 0.0181\n",
            "Epoch 584/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.3982e-04 - mse: 1.6789e-06 - val_loss: 0.1087 - val_mse: 0.0171\n",
            "Epoch 585/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 7.3218e-04 - mse: 1.7102e-06 - val_loss: 0.1145 - val_mse: 0.0180\n",
            "Epoch 586/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.2702e-04 - mse: 1.6734e-06 - val_loss: 0.1105 - val_mse: 0.0174\n",
            "Epoch 587/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.2549e-04 - mse: 1.6898e-06 - val_loss: 0.1153 - val_mse: 0.0182\n",
            "Epoch 588/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.2211e-04 - mse: 1.6475e-06 - val_loss: 0.1093 - val_mse: 0.0172\n",
            "Epoch 589/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.2274e-04 - mse: 1.5813e-06 - val_loss: 0.1035 - val_mse: 0.0162\n",
            "Epoch 590/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 7.1394e-04 - mse: 1.4260e-06 - val_loss: 0.1097 - val_mse: 0.0172\n",
            "Epoch 591/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.0531e-04 - mse: 1.5992e-06 - val_loss: 0.1170 - val_mse: 0.0185\n",
            "Epoch 592/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.9933e-04 - mse: 1.5706e-06 - val_loss: 0.1159 - val_mse: 0.0184\n",
            "Epoch 593/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.9952e-04 - mse: 1.7665e-06 - val_loss: 0.1223 - val_mse: 0.0194\n",
            "Epoch 594/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.9048e-04 - mse: 1.3921e-06 - val_loss: 0.1008 - val_mse: 0.0158\n",
            "Epoch 595/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.9374e-04 - mse: 1.2888e-06 - val_loss: 0.1010 - val_mse: 0.0159\n",
            "Epoch 596/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 6.8652e-04 - mse: 1.3325e-06 - val_loss: 0.1113 - val_mse: 0.0176\n",
            "Epoch 597/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.8218e-04 - mse: 1.3118e-06 - val_loss: 0.1057 - val_mse: 0.0166\n",
            "Epoch 598/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.7248e-04 - mse: 1.3094e-06 - val_loss: 0.1134 - val_mse: 0.0180\n",
            "Epoch 599/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.7114e-04 - mse: 1.4619e-06 - val_loss: 0.1186 - val_mse: 0.0188\n",
            "Epoch 600/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.6737e-04 - mse: 1.3703e-06 - val_loss: 0.1047 - val_mse: 0.0166\n",
            "Epoch 601/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 6.6112e-04 - mse: 1.2690e-06 - val_loss: 0.1088 - val_mse: 0.0173\n",
            "Epoch 602/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 6.6832e-04 - mse: 1.5403e-06 - val_loss: 0.1190 - val_mse: 0.0189\n",
            "Epoch 603/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.5679e-04 - mse: 1.3264e-06 - val_loss: 0.1070 - val_mse: 0.0169\n",
            "Epoch 604/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.5375e-04 - mse: 1.3322e-06 - val_loss: 0.1166 - val_mse: 0.0185\n",
            "Epoch 605/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.4224e-04 - mse: 1.2825e-06 - val_loss: 0.1118 - val_mse: 0.0178\n",
            "Epoch 606/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.4069e-04 - mse: 1.2322e-06 - val_loss: 0.1076 - val_mse: 0.0171\n",
            "Epoch 607/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 6.3362e-04 - mse: 1.1696e-06 - val_loss: 0.1143 - val_mse: 0.0182\n",
            "Epoch 608/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.3278e-04 - mse: 1.2200e-06 - val_loss: 0.1215 - val_mse: 0.0193\n",
            "Epoch 609/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.4275e-04 - mse: 1.2726e-06 - val_loss: 0.1017 - val_mse: 0.0162\n",
            "Epoch 610/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.2664e-04 - mse: 1.1479e-06 - val_loss: 0.1128 - val_mse: 0.0179\n",
            "Epoch 611/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.1856e-04 - mse: 1.1622e-06 - val_loss: 0.1089 - val_mse: 0.0173\n",
            "Epoch 612/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.1297e-04 - mse: 1.1478e-06 - val_loss: 0.1145 - val_mse: 0.0182\n",
            "Epoch 613/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.1069e-04 - mse: 1.1365e-06 - val_loss: 0.1124 - val_mse: 0.0179\n",
            "Epoch 614/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.0564e-04 - mse: 1.1427e-06 - val_loss: 0.1121 - val_mse: 0.0179\n",
            "Epoch 615/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.0262e-04 - mse: 1.0835e-06 - val_loss: 0.1078 - val_mse: 0.0172\n",
            "Epoch 616/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.0499e-04 - mse: 1.0655e-06 - val_loss: 0.1075 - val_mse: 0.0171\n",
            "Epoch 617/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 5.9992e-04 - mse: 1.0493e-06 - val_loss: 0.1167 - val_mse: 0.0187\n",
            "Epoch 618/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 5.9327e-04 - mse: 1.1088e-06 - val_loss: 0.1119 - val_mse: 0.0179\n",
            "Epoch 619/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 5.8743e-04 - mse: 1.0370e-06 - val_loss: 0.1146 - val_mse: 0.0183\n",
            "Epoch 620/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 5.8471e-04 - mse: 1.1250e-06 - val_loss: 0.1154 - val_mse: 0.0185\n",
            "Epoch 621/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 5.8184e-04 - mse: 1.1506e-06 - val_loss: 0.1156 - val_mse: 0.0185\n",
            "Epoch 622/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 5.7849e-04 - mse: 1.0514e-06 - val_loss: 0.1072 - val_mse: 0.0172\n",
            "Epoch 623/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 5.7148e-04 - mse: 9.9040e-07 - val_loss: 0.1133 - val_mse: 0.0181\n",
            "Epoch 624/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 5.6926e-04 - mse: 1.0188e-06 - val_loss: 0.1133 - val_mse: 0.0181\n",
            "Epoch 625/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 5.6644e-04 - mse: 9.9531e-07 - val_loss: 0.1103 - val_mse: 0.0176\n",
            "Epoch 626/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 5.6424e-04 - mse: 1.0431e-06 - val_loss: 0.1104 - val_mse: 0.0177\n",
            "Epoch 627/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 5.6081e-04 - mse: 9.7783e-07 - val_loss: 0.1149 - val_mse: 0.0184\n",
            "Epoch 628/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 5.5724e-04 - mse: 1.0143e-06 - val_loss: 0.1132 - val_mse: 0.0181\n",
            "Epoch 629/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 5.5088e-04 - mse: 8.6191e-07 - val_loss: 0.0973 - val_mse: 0.0155\n",
            "Epoch 630/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 5.5207e-04 - mse: 8.3470e-07 - val_loss: 0.1013 - val_mse: 0.0163\n",
            "Epoch 631/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 5.4931e-04 - mse: 9.2649e-07 - val_loss: 0.1066 - val_mse: 0.0171\n",
            "Epoch 632/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 5.4202e-04 - mse: 8.9289e-07 - val_loss: 0.1095 - val_mse: 0.0175\n",
            "Epoch 633/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 5.3710e-04 - mse: 9.0726e-07 - val_loss: 0.1139 - val_mse: 0.0182\n",
            "Epoch 634/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 5.3481e-04 - mse: 9.2806e-07 - val_loss: 0.1173 - val_mse: 0.0188\n",
            "Epoch 635/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 5.3455e-04 - mse: 9.3379e-07 - val_loss: 0.1157 - val_mse: 0.0186\n",
            "Epoch 636/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 5.2979e-04 - mse: 8.7504e-07 - val_loss: 0.1088 - val_mse: 0.0174\n",
            "Epoch 637/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 5.2633e-04 - mse: 8.4021e-07 - val_loss: 0.1092 - val_mse: 0.0175\n",
            "Epoch 638/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 5.2087e-04 - mse: 8.3720e-07 - val_loss: 0.1168 - val_mse: 0.0188\n",
            "Epoch 639/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 5.1872e-04 - mse: 8.3908e-07 - val_loss: 0.1067 - val_mse: 0.0171\n",
            "Epoch 640/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 5.1527e-04 - mse: 7.6490e-07 - val_loss: 0.1054 - val_mse: 0.0169\n",
            "Epoch 641/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 5.1550e-04 - mse: 8.4090e-07 - val_loss: 0.1135 - val_mse: 0.0183\n",
            "Epoch 642/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 5.1017e-04 - mse: 8.3816e-07 - val_loss: 0.1093 - val_mse: 0.0176\n",
            "Epoch 643/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 5.1152e-04 - mse: 7.5674e-07 - val_loss: 0.0973 - val_mse: 0.0156\n",
            "Epoch 644/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 5.0868e-04 - mse: 6.8668e-07 - val_loss: 0.1094 - val_mse: 0.0176\n",
            "Epoch 645/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.9719e-04 - mse: 7.1785e-07 - val_loss: 0.1146 - val_mse: 0.0184\n",
            "Epoch 646/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.9906e-04 - mse: 8.4955e-07 - val_loss: 0.1185 - val_mse: 0.0191\n",
            "Epoch 647/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 4.9237e-04 - mse: 7.7759e-07 - val_loss: 0.1122 - val_mse: 0.0181\n",
            "Epoch 648/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.9179e-04 - mse: 7.5425e-07 - val_loss: 0.1079 - val_mse: 0.0174\n",
            "Epoch 649/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.9166e-04 - mse: 7.9105e-07 - val_loss: 0.1125 - val_mse: 0.0181\n",
            "Epoch 650/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.8456e-04 - mse: 7.0566e-07 - val_loss: 0.1024 - val_mse: 0.0165\n",
            "Epoch 651/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.8103e-04 - mse: 7.1362e-07 - val_loss: 0.1073 - val_mse: 0.0174\n",
            "Epoch 652/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 4.7738e-04 - mse: 7.1277e-07 - val_loss: 0.1099 - val_mse: 0.0177\n",
            "Epoch 653/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.7497e-04 - mse: 7.1510e-07 - val_loss: 0.1108 - val_mse: 0.0179\n",
            "Epoch 654/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.8010e-04 - mse: 8.3399e-07 - val_loss: 0.1142 - val_mse: 0.0184\n",
            "Epoch 655/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.6996e-04 - mse: 7.3144e-07 - val_loss: 0.1066 - val_mse: 0.0172\n",
            "Epoch 656/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.7114e-04 - mse: 7.2988e-07 - val_loss: 0.1098 - val_mse: 0.0178\n",
            "Epoch 657/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 4.6667e-04 - mse: 6.9658e-07 - val_loss: 0.1035 - val_mse: 0.0167\n",
            "Epoch 658/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 4.6124e-04 - mse: 6.2497e-07 - val_loss: 0.1103 - val_mse: 0.0178\n",
            "Epoch 659/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 4.5815e-04 - mse: 6.7108e-07 - val_loss: 0.1122 - val_mse: 0.0181\n",
            "Epoch 660/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.6024e-04 - mse: 7.6055e-07 - val_loss: 0.1142 - val_mse: 0.0184\n",
            "Epoch 661/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.5074e-04 - mse: 6.7160e-07 - val_loss: 0.1047 - val_mse: 0.0169\n",
            "Epoch 662/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.5458e-04 - mse: 6.2276e-07 - val_loss: 0.1023 - val_mse: 0.0164\n",
            "Epoch 663/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.4744e-04 - mse: 6.1182e-07 - val_loss: 0.1090 - val_mse: 0.0177\n",
            "Epoch 664/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.4536e-04 - mse: 6.0993e-07 - val_loss: 0.1055 - val_mse: 0.0171\n",
            "Epoch 665/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 4.4211e-04 - mse: 5.8806e-07 - val_loss: 0.1171 - val_mse: 0.0190\n",
            "Epoch 666/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.4208e-04 - mse: 6.8090e-07 - val_loss: 0.1159 - val_mse: 0.0188\n",
            "Epoch 667/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.3723e-04 - mse: 6.1731e-07 - val_loss: 0.1027 - val_mse: 0.0166\n",
            "Epoch 668/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.3651e-04 - mse: 6.3275e-07 - val_loss: 0.1118 - val_mse: 0.0181\n",
            "Epoch 669/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.3161e-04 - mse: 5.9537e-07 - val_loss: 0.1082 - val_mse: 0.0176\n",
            "Epoch 670/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.2703e-04 - mse: 5.6450e-07 - val_loss: 0.1096 - val_mse: 0.0178\n",
            "Epoch 671/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.2435e-04 - mse: 5.6757e-07 - val_loss: 0.1085 - val_mse: 0.0176\n",
            "Epoch 672/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.2108e-04 - mse: 5.6567e-07 - val_loss: 0.1161 - val_mse: 0.0189\n",
            "Epoch 673/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.2277e-04 - mse: 6.4768e-07 - val_loss: 0.1102 - val_mse: 0.0179\n",
            "Epoch 674/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 4.1629e-04 - mse: 5.2164e-07 - val_loss: 0.0989 - val_mse: 0.0159\n",
            "Epoch 675/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.1603e-04 - mse: 5.0162e-07 - val_loss: 0.1033 - val_mse: 0.0168\n",
            "Epoch 676/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 4.1699e-04 - mse: 5.8322e-07 - val_loss: 0.1135 - val_mse: 0.0184\n",
            "Epoch 677/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.1660e-04 - mse: 5.2994e-07 - val_loss: 0.0930 - val_mse: 0.0150\n",
            "Epoch 678/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.1523e-04 - mse: 5.0436e-07 - val_loss: 0.1093 - val_mse: 0.0179\n",
            "Epoch 679/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.0768e-04 - mse: 5.3670e-07 - val_loss: 0.1107 - val_mse: 0.0180\n",
            "Epoch 680/1000\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 4.0097e-04 - mse: 5.3016e-07 - val_loss: 0.1077 - val_mse: 0.0175\n",
            "Epoch 681/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 4.0289e-04 - mse: 4.6889e-07 - val_loss: 0.0995 - val_mse: 0.0161\n",
            "Epoch 682/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 4.0018e-04 - mse: 4.5452e-07 - val_loss: 0.1066 - val_mse: 0.0174\n",
            "Epoch 683/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.9446e-04 - mse: 4.8262e-07 - val_loss: 0.1110 - val_mse: 0.0181\n",
            "Epoch 684/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.9164e-04 - mse: 4.9844e-07 - val_loss: 0.1086 - val_mse: 0.0177\n",
            "Epoch 685/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.8933e-04 - mse: 4.8309e-07 - val_loss: 0.1051 - val_mse: 0.0171\n",
            "Epoch 686/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.8769e-04 - mse: 4.7111e-07 - val_loss: 0.1098 - val_mse: 0.0179\n",
            "Epoch 687/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.8468e-04 - mse: 4.6983e-07 - val_loss: 0.1090 - val_mse: 0.0178\n",
            "Epoch 688/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.8359e-04 - mse: 4.7491e-07 - val_loss: 0.1072 - val_mse: 0.0175\n",
            "Epoch 689/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.8221e-04 - mse: 4.8574e-07 - val_loss: 0.1076 - val_mse: 0.0176\n",
            "Epoch 690/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.7876e-04 - mse: 4.5184e-07 - val_loss: 0.1025 - val_mse: 0.0166\n",
            "Epoch 691/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.7595e-04 - mse: 4.3452e-07 - val_loss: 0.1073 - val_mse: 0.0175\n",
            "Epoch 692/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.7399e-04 - mse: 4.4729e-07 - val_loss: 0.1053 - val_mse: 0.0172\n",
            "Epoch 693/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.7237e-04 - mse: 4.3358e-07 - val_loss: 0.1040 - val_mse: 0.0170\n",
            "Epoch 694/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.7144e-04 - mse: 4.4605e-07 - val_loss: 0.1065 - val_mse: 0.0174\n",
            "Epoch 695/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.7177e-04 - mse: 4.1766e-07 - val_loss: 0.1039 - val_mse: 0.0169\n",
            "Epoch 696/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 3.6717e-04 - mse: 3.9610e-07 - val_loss: 0.1080 - val_mse: 0.0178\n",
            "Epoch 697/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.6513e-04 - mse: 4.1069e-07 - val_loss: 0.1095 - val_mse: 0.0179\n",
            "Epoch 698/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.6010e-04 - mse: 4.2913e-07 - val_loss: 0.1069 - val_mse: 0.0175\n",
            "Epoch 699/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.5838e-04 - mse: 4.2594e-07 - val_loss: 0.1079 - val_mse: 0.0176\n",
            "Epoch 700/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.5819e-04 - mse: 4.6413e-07 - val_loss: 0.1138 - val_mse: 0.0186\n",
            "Epoch 701/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.5511e-04 - mse: 4.5384e-07 - val_loss: 0.1079 - val_mse: 0.0177\n",
            "Epoch 702/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.5607e-04 - mse: 3.8400e-07 - val_loss: 0.0850 - val_mse: 0.0137\n",
            "Epoch 703/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.6080e-04 - mse: 3.6077e-07 - val_loss: 0.0992 - val_mse: 0.0163\n",
            "Epoch 704/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.5527e-04 - mse: 3.6972e-07 - val_loss: 0.1016 - val_mse: 0.0167\n",
            "Epoch 705/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.4519e-04 - mse: 3.2541e-07 - val_loss: 0.0950 - val_mse: 0.0155\n",
            "Epoch 706/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.4885e-04 - mse: 3.2779e-07 - val_loss: 0.1015 - val_mse: 0.0166\n",
            "Epoch 707/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.4703e-04 - mse: 3.2744e-07 - val_loss: 0.1001 - val_mse: 0.0164\n",
            "Epoch 708/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.4367e-04 - mse: 3.6324e-07 - val_loss: 0.1125 - val_mse: 0.0185\n",
            "Epoch 709/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.3702e-04 - mse: 3.2679e-07 - val_loss: 0.0961 - val_mse: 0.0158\n",
            "Epoch 710/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 3.3571e-04 - mse: 3.3057e-07 - val_loss: 0.1055 - val_mse: 0.0174\n",
            "Epoch 711/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.3383e-04 - mse: 3.6936e-07 - val_loss: 0.1116 - val_mse: 0.0184\n",
            "Epoch 712/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.2958e-04 - mse: 3.4105e-07 - val_loss: 0.1017 - val_mse: 0.0167\n",
            "Epoch 713/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.3093e-04 - mse: 3.1870e-07 - val_loss: 0.1049 - val_mse: 0.0171\n",
            "Epoch 714/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.2623e-04 - mse: 3.1201e-07 - val_loss: 0.1062 - val_mse: 0.0175\n",
            "Epoch 715/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 3.2343e-04 - mse: 3.3283e-07 - val_loss: 0.1147 - val_mse: 0.0188\n",
            "Epoch 716/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.2460e-04 - mse: 3.4070e-07 - val_loss: 0.1086 - val_mse: 0.0179\n",
            "Epoch 717/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.1844e-04 - mse: 3.1293e-07 - val_loss: 0.1074 - val_mse: 0.0177\n",
            "Epoch 718/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.1795e-04 - mse: 3.2662e-07 - val_loss: 0.1092 - val_mse: 0.0180\n",
            "Epoch 719/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.1491e-04 - mse: 3.2341e-07 - val_loss: 0.1110 - val_mse: 0.0183\n",
            "Epoch 720/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.1327e-04 - mse: 3.1271e-07 - val_loss: 0.1056 - val_mse: 0.0174\n",
            "Epoch 721/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.1134e-04 - mse: 2.9896e-07 - val_loss: 0.1028 - val_mse: 0.0169\n",
            "Epoch 722/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.1042e-04 - mse: 2.9547e-07 - val_loss: 0.1050 - val_mse: 0.0173\n",
            "Epoch 723/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.0792e-04 - mse: 3.0392e-07 - val_loss: 0.1086 - val_mse: 0.0179\n",
            "Epoch 724/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.0730e-04 - mse: 2.9646e-07 - val_loss: 0.1047 - val_mse: 0.0173\n",
            "Epoch 725/1000\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 3.0461e-04 - mse: 3.1099e-07 - val_loss: 0.1108 - val_mse: 0.0183\n",
            "Epoch 726/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.0377e-04 - mse: 3.2324e-07 - val_loss: 0.1072 - val_mse: 0.0177\n",
            "Epoch 727/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.0207e-04 - mse: 2.8796e-07 - val_loss: 0.1031 - val_mse: 0.0170\n",
            "Epoch 728/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 3.0103e-04 - mse: 2.7877e-07 - val_loss: 0.1034 - val_mse: 0.0170\n",
            "Epoch 729/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 2.9755e-04 - mse: 2.6736e-07 - val_loss: 0.1076 - val_mse: 0.0178\n",
            "Epoch 730/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.9581e-04 - mse: 2.8944e-07 - val_loss: 0.1079 - val_mse: 0.0178\n",
            "Epoch 731/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.9346e-04 - mse: 2.9310e-07 - val_loss: 0.1052 - val_mse: 0.0175\n",
            "Epoch 732/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.9195e-04 - mse: 2.6205e-07 - val_loss: 0.0949 - val_mse: 0.0157\n",
            "Epoch 733/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.9110e-04 - mse: 2.5256e-07 - val_loss: 0.1032 - val_mse: 0.0171\n",
            "Epoch 734/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.9009e-04 - mse: 2.6596e-07 - val_loss: 0.1047 - val_mse: 0.0173\n",
            "Epoch 735/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.8672e-04 - mse: 2.5099e-07 - val_loss: 0.1027 - val_mse: 0.0169\n",
            "Epoch 736/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.8609e-04 - mse: 2.7343e-07 - val_loss: 0.1054 - val_mse: 0.0175\n",
            "Epoch 737/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.8481e-04 - mse: 2.6369e-07 - val_loss: 0.0985 - val_mse: 0.0164\n",
            "Epoch 738/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 2.8352e-04 - mse: 2.5262e-07 - val_loss: 0.0998 - val_mse: 0.0165\n",
            "Epoch 739/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.8166e-04 - mse: 2.4872e-07 - val_loss: 0.0997 - val_mse: 0.0165\n",
            "Epoch 740/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.7865e-04 - mse: 2.4719e-07 - val_loss: 0.1074 - val_mse: 0.0177\n",
            "Epoch 741/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.7878e-04 - mse: 2.7067e-07 - val_loss: 0.1105 - val_mse: 0.0183\n",
            "Epoch 742/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.7731e-04 - mse: 2.5162e-07 - val_loss: 0.0976 - val_mse: 0.0161\n",
            "Epoch 743/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 2.7312e-04 - mse: 2.2615e-07 - val_loss: 0.1023 - val_mse: 0.0169\n",
            "Epoch 744/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 2.7118e-04 - mse: 2.3635e-07 - val_loss: 0.1028 - val_mse: 0.0170\n",
            "Epoch 745/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.7060e-04 - mse: 2.4460e-07 - val_loss: 0.1014 - val_mse: 0.0168\n",
            "Epoch 746/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 2.6950e-04 - mse: 2.3034e-07 - val_loss: 0.0976 - val_mse: 0.0162\n",
            "Epoch 747/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.6758e-04 - mse: 2.2119e-07 - val_loss: 0.1039 - val_mse: 0.0172\n",
            "Epoch 748/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 2.6594e-04 - mse: 2.2240e-07 - val_loss: 0.1053 - val_mse: 0.0175\n",
            "Epoch 749/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.7107e-04 - mse: 2.7910e-07 - val_loss: 0.1127 - val_mse: 0.0187\n",
            "Epoch 750/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.6391e-04 - mse: 2.2322e-07 - val_loss: 0.0934 - val_mse: 0.0155\n",
            "Epoch 751/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.6270e-04 - mse: 2.0777e-07 - val_loss: 0.0982 - val_mse: 0.0163\n",
            "Epoch 752/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.5987e-04 - mse: 1.8626e-07 - val_loss: 0.0898 - val_mse: 0.0148\n",
            "Epoch 753/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.6214e-04 - mse: 1.8814e-07 - val_loss: 0.0961 - val_mse: 0.0159\n",
            "Epoch 754/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.5711e-04 - mse: 2.0308e-07 - val_loss: 0.1033 - val_mse: 0.0172\n",
            "Epoch 755/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.5386e-04 - mse: 1.9994e-07 - val_loss: 0.1044 - val_mse: 0.0174\n",
            "Epoch 756/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.5225e-04 - mse: 1.9941e-07 - val_loss: 0.0996 - val_mse: 0.0166\n",
            "Epoch 757/1000\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 2.5093e-04 - mse: 2.0380e-07 - val_loss: 0.1028 - val_mse: 0.0171\n",
            "Epoch 758/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.5387e-04 - mse: 2.2651e-07 - val_loss: 0.1061 - val_mse: 0.0177\n",
            "Epoch 759/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 2.5456e-04 - mse: 2.0100e-07 - val_loss: 0.0840 - val_mse: 0.0138\n",
            "Epoch 760/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.5126e-04 - mse: 1.6658e-07 - val_loss: 0.0933 - val_mse: 0.0156\n",
            "Epoch 761/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.4526e-04 - mse: 1.7124e-07 - val_loss: 0.1028 - val_mse: 0.0171\n",
            "Epoch 762/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 2.4539e-04 - mse: 2.0880e-07 - val_loss: 0.1084 - val_mse: 0.0180\n",
            "Epoch 763/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.4304e-04 - mse: 1.9886e-07 - val_loss: 0.0990 - val_mse: 0.0165\n",
            "Epoch 764/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 2.4041e-04 - mse: 1.8504e-07 - val_loss: 0.1021 - val_mse: 0.0171\n",
            "Epoch 765/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.3910e-04 - mse: 1.9057e-07 - val_loss: 0.1070 - val_mse: 0.0179\n",
            "Epoch 766/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.3884e-04 - mse: 1.9234e-07 - val_loss: 0.1035 - val_mse: 0.0173\n",
            "Epoch 767/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.3684e-04 - mse: 1.8351e-07 - val_loss: 0.1024 - val_mse: 0.0171\n",
            "Epoch 768/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.3366e-04 - mse: 1.7255e-07 - val_loss: 0.0991 - val_mse: 0.0166\n",
            "Epoch 769/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 2.3284e-04 - mse: 1.7023e-07 - val_loss: 0.0974 - val_mse: 0.0163\n",
            "Epoch 770/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.3306e-04 - mse: 1.8477e-07 - val_loss: 0.1023 - val_mse: 0.0171\n",
            "Epoch 771/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.3151e-04 - mse: 1.8245e-07 - val_loss: 0.1039 - val_mse: 0.0173\n",
            "Epoch 772/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 2.2903e-04 - mse: 1.5626e-07 - val_loss: 0.0877 - val_mse: 0.0145\n",
            "Epoch 773/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.3013e-04 - mse: 1.4916e-07 - val_loss: 0.0902 - val_mse: 0.0149\n",
            "Epoch 774/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.3439e-04 - mse: 1.9855e-07 - val_loss: 0.1112 - val_mse: 0.0186\n",
            "Epoch 775/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.2504e-04 - mse: 1.5141e-07 - val_loss: 0.0932 - val_mse: 0.0155\n",
            "Epoch 776/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 2.2546e-04 - mse: 1.4318e-07 - val_loss: 0.0955 - val_mse: 0.0159\n",
            "Epoch 777/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.2259e-04 - mse: 1.4425e-07 - val_loss: 0.0984 - val_mse: 0.0165\n",
            "Epoch 778/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.2529e-04 - mse: 1.6840e-07 - val_loss: 0.1026 - val_mse: 0.0172\n",
            "Epoch 779/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.2032e-04 - mse: 1.5790e-07 - val_loss: 0.1018 - val_mse: 0.0169\n",
            "Epoch 780/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.1942e-04 - mse: 1.5454e-07 - val_loss: 0.1021 - val_mse: 0.0170\n",
            "Epoch 781/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 2.1751e-04 - mse: 1.5902e-07 - val_loss: 0.1036 - val_mse: 0.0173\n",
            "Epoch 782/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.1727e-04 - mse: 1.5226e-07 - val_loss: 0.0952 - val_mse: 0.0160\n",
            "Epoch 783/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.1439e-04 - mse: 1.3846e-07 - val_loss: 0.0979 - val_mse: 0.0164\n",
            "Epoch 784/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.1262e-04 - mse: 1.3987e-07 - val_loss: 0.1025 - val_mse: 0.0172\n",
            "Epoch 785/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.1369e-04 - mse: 1.6207e-07 - val_loss: 0.1062 - val_mse: 0.0178\n",
            "Epoch 786/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 2.1161e-04 - mse: 1.6161e-07 - val_loss: 0.1029 - val_mse: 0.0172\n",
            "Epoch 787/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.1014e-04 - mse: 1.2491e-07 - val_loss: 0.0844 - val_mse: 0.0140\n",
            "Epoch 788/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.1132e-04 - mse: 1.1911e-07 - val_loss: 0.0860 - val_mse: 0.0143\n",
            "Epoch 789/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.1189e-04 - mse: 1.2829e-07 - val_loss: 0.0952 - val_mse: 0.0160\n",
            "Epoch 790/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.0626e-04 - mse: 1.2479e-07 - val_loss: 0.0969 - val_mse: 0.0162\n",
            "Epoch 791/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.0538e-04 - mse: 1.2245e-07 - val_loss: 0.0938 - val_mse: 0.0156\n",
            "Epoch 792/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.0377e-04 - mse: 1.2507e-07 - val_loss: 0.0992 - val_mse: 0.0166\n",
            "Epoch 793/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.0302e-04 - mse: 1.4210e-07 - val_loss: 0.1063 - val_mse: 0.0179\n",
            "Epoch 794/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.0154e-04 - mse: 1.3457e-07 - val_loss: 0.1009 - val_mse: 0.0170\n",
            "Epoch 795/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 1.9868e-04 - mse: 1.1486e-07 - val_loss: 0.0939 - val_mse: 0.0157\n",
            "Epoch 796/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.9945e-04 - mse: 1.1596e-07 - val_loss: 0.0951 - val_mse: 0.0160\n",
            "Epoch 797/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.9702e-04 - mse: 1.1365e-07 - val_loss: 0.0960 - val_mse: 0.0162\n",
            "Epoch 798/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.9811e-04 - mse: 1.1539e-07 - val_loss: 0.0943 - val_mse: 0.0159\n",
            "Epoch 799/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 1.9998e-04 - mse: 1.3210e-07 - val_loss: 0.1091 - val_mse: 0.0183\n",
            "Epoch 800/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.9577e-04 - mse: 1.2919e-07 - val_loss: 0.0984 - val_mse: 0.0165\n",
            "Epoch 801/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.9308e-04 - mse: 1.1892e-07 - val_loss: 0.0969 - val_mse: 0.0163\n",
            "Epoch 802/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.9129e-04 - mse: 1.1643e-07 - val_loss: 0.0985 - val_mse: 0.0165\n",
            "Epoch 803/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.9060e-04 - mse: 1.1527e-07 - val_loss: 0.0987 - val_mse: 0.0166\n",
            "Epoch 804/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.9023e-04 - mse: 1.1266e-07 - val_loss: 0.0959 - val_mse: 0.0163\n",
            "Epoch 805/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.8713e-04 - mse: 1.1013e-07 - val_loss: 0.1027 - val_mse: 0.0173\n",
            "Epoch 806/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.8667e-04 - mse: 1.1114e-07 - val_loss: 0.0994 - val_mse: 0.0167\n",
            "Epoch 807/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.8669e-04 - mse: 1.1516e-07 - val_loss: 0.1010 - val_mse: 0.0170\n",
            "Epoch 808/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.8615e-04 - mse: 1.1811e-07 - val_loss: 0.1010 - val_mse: 0.0171\n",
            "Epoch 809/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.8439e-04 - mse: 1.1250e-07 - val_loss: 0.0991 - val_mse: 0.0167\n",
            "Epoch 810/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.8297e-04 - mse: 1.1315e-07 - val_loss: 0.0989 - val_mse: 0.0167\n",
            "Epoch 811/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.8108e-04 - mse: 1.0680e-07 - val_loss: 0.0982 - val_mse: 0.0166\n",
            "Epoch 812/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.8051e-04 - mse: 1.0706e-07 - val_loss: 0.0975 - val_mse: 0.0165\n",
            "Epoch 813/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.8004e-04 - mse: 1.0549e-07 - val_loss: 0.1004 - val_mse: 0.0170\n",
            "Epoch 814/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.8005e-04 - mse: 1.0301e-07 - val_loss: 0.0958 - val_mse: 0.0161\n",
            "Epoch 815/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 1.7758e-04 - mse: 9.6391e-08 - val_loss: 0.0961 - val_mse: 0.0163\n",
            "Epoch 816/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.7883e-04 - mse: 1.1634e-07 - val_loss: 0.1033 - val_mse: 0.0174\n",
            "Epoch 817/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.7586e-04 - mse: 1.0705e-07 - val_loss: 0.0954 - val_mse: 0.0161\n",
            "Epoch 818/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.7514e-04 - mse: 1.0313e-07 - val_loss: 0.1003 - val_mse: 0.0169\n",
            "Epoch 819/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.7522e-04 - mse: 9.4414e-08 - val_loss: 0.0878 - val_mse: 0.0148\n",
            "Epoch 820/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.7291e-04 - mse: 8.6774e-08 - val_loss: 0.0948 - val_mse: 0.0160\n",
            "Epoch 821/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.7195e-04 - mse: 8.8979e-08 - val_loss: 0.0946 - val_mse: 0.0160\n",
            "Epoch 822/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.7035e-04 - mse: 8.8285e-08 - val_loss: 0.0989 - val_mse: 0.0168\n",
            "Epoch 823/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.6897e-04 - mse: 9.3069e-08 - val_loss: 0.1005 - val_mse: 0.0170\n",
            "Epoch 824/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.6953e-04 - mse: 9.4507e-08 - val_loss: 0.1028 - val_mse: 0.0173\n",
            "Epoch 825/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.6769e-04 - mse: 9.5398e-08 - val_loss: 0.0961 - val_mse: 0.0162\n",
            "Epoch 826/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.6699e-04 - mse: 9.0508e-08 - val_loss: 0.0890 - val_mse: 0.0149\n",
            "Epoch 827/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.6513e-04 - mse: 8.0621e-08 - val_loss: 0.0930 - val_mse: 0.0157\n",
            "Epoch 828/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.6419e-04 - mse: 8.2155e-08 - val_loss: 0.0975 - val_mse: 0.0166\n",
            "Epoch 829/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.6336e-04 - mse: 8.3288e-08 - val_loss: 0.0955 - val_mse: 0.0162\n",
            "Epoch 830/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 1.6246e-04 - mse: 9.1348e-08 - val_loss: 0.0997 - val_mse: 0.0169\n",
            "Epoch 831/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.6164e-04 - mse: 9.1347e-08 - val_loss: 0.0983 - val_mse: 0.0166\n",
            "Epoch 832/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.6076e-04 - mse: 8.5830e-08 - val_loss: 0.0951 - val_mse: 0.0161\n",
            "Epoch 833/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.5998e-04 - mse: 8.5190e-08 - val_loss: 0.0969 - val_mse: 0.0164\n",
            "Epoch 834/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 1.5829e-04 - mse: 8.1411e-08 - val_loss: 0.0966 - val_mse: 0.0164\n",
            "Epoch 835/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.5981e-04 - mse: 8.9863e-08 - val_loss: 0.0999 - val_mse: 0.0170\n",
            "Epoch 836/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.5834e-04 - mse: 8.1535e-08 - val_loss: 0.0905 - val_mse: 0.0152\n",
            "Epoch 837/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.5582e-04 - mse: 7.5721e-08 - val_loss: 0.0958 - val_mse: 0.0162\n",
            "Epoch 838/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.5519e-04 - mse: 7.6996e-08 - val_loss: 0.0985 - val_mse: 0.0167\n",
            "Epoch 839/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 1.5499e-04 - mse: 8.2041e-08 - val_loss: 0.1010 - val_mse: 0.0172\n",
            "Epoch 840/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.5392e-04 - mse: 8.2066e-08 - val_loss: 0.0988 - val_mse: 0.0167\n",
            "Epoch 841/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.5234e-04 - mse: 7.8368e-08 - val_loss: 0.0972 - val_mse: 0.0165\n",
            "Epoch 842/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.5127e-04 - mse: 7.6710e-08 - val_loss: 0.0953 - val_mse: 0.0162\n",
            "Epoch 843/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 1.5088e-04 - mse: 7.3612e-08 - val_loss: 0.0960 - val_mse: 0.0163\n",
            "Epoch 844/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 1.5058e-04 - mse: 7.6582e-08 - val_loss: 0.0973 - val_mse: 0.0165\n",
            "Epoch 845/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.5035e-04 - mse: 7.7262e-08 - val_loss: 0.0986 - val_mse: 0.0167\n",
            "Epoch 846/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.4890e-04 - mse: 7.5183e-08 - val_loss: 0.0957 - val_mse: 0.0162\n",
            "Epoch 847/1000\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 1.4752e-04 - mse: 7.3963e-08 - val_loss: 0.0926 - val_mse: 0.0158\n",
            "Epoch 848/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.4632e-04 - mse: 6.9136e-08 - val_loss: 0.0939 - val_mse: 0.0160\n",
            "Epoch 849/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.4545e-04 - mse: 7.1912e-08 - val_loss: 0.0975 - val_mse: 0.0166\n",
            "Epoch 850/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.4486e-04 - mse: 7.2009e-08 - val_loss: 0.0960 - val_mse: 0.0163\n",
            "Epoch 851/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 1.4412e-04 - mse: 6.8101e-08 - val_loss: 0.0966 - val_mse: 0.0163\n",
            "Epoch 852/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.4344e-04 - mse: 6.8956e-08 - val_loss: 0.0968 - val_mse: 0.0165\n",
            "Epoch 853/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.4320e-04 - mse: 6.7375e-08 - val_loss: 0.0901 - val_mse: 0.0153\n",
            "Epoch 854/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.4110e-04 - mse: 6.3337e-08 - val_loss: 0.0947 - val_mse: 0.0161\n",
            "Epoch 855/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.4055e-04 - mse: 6.6445e-08 - val_loss: 0.0956 - val_mse: 0.0163\n",
            "Epoch 856/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.4018e-04 - mse: 6.3870e-08 - val_loss: 0.0913 - val_mse: 0.0154\n",
            "Epoch 857/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.3882e-04 - mse: 5.9027e-08 - val_loss: 0.0843 - val_mse: 0.0142\n",
            "Epoch 858/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.4287e-04 - mse: 5.9240e-08 - val_loss: 0.0812 - val_mse: 0.0136\n",
            "Epoch 859/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.3746e-04 - mse: 5.4120e-08 - val_loss: 0.0971 - val_mse: 0.0166\n",
            "Epoch 860/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.3749e-04 - mse: 6.5464e-08 - val_loss: 0.1008 - val_mse: 0.0172\n",
            "Epoch 861/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.3633e-04 - mse: 6.3492e-08 - val_loss: 0.0933 - val_mse: 0.0158\n",
            "Epoch 862/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.3511e-04 - mse: 5.9293e-08 - val_loss: 0.0887 - val_mse: 0.0151\n",
            "Epoch 863/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.3406e-04 - mse: 5.7812e-08 - val_loss: 0.0933 - val_mse: 0.0159\n",
            "Epoch 864/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.3455e-04 - mse: 6.0185e-08 - val_loss: 0.0920 - val_mse: 0.0156\n",
            "Epoch 865/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.3374e-04 - mse: 6.3524e-08 - val_loss: 0.0972 - val_mse: 0.0167\n",
            "Epoch 866/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.3247e-04 - mse: 5.4156e-08 - val_loss: 0.0855 - val_mse: 0.0145\n",
            "Epoch 867/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.3264e-04 - mse: 5.3972e-08 - val_loss: 0.0883 - val_mse: 0.0151\n",
            "Epoch 868/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 1.3174e-04 - mse: 5.2415e-08 - val_loss: 0.0888 - val_mse: 0.0150\n",
            "Epoch 869/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.3099e-04 - mse: 5.5743e-08 - val_loss: 0.0931 - val_mse: 0.0159\n",
            "Epoch 870/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.2923e-04 - mse: 5.4316e-08 - val_loss: 0.0894 - val_mse: 0.0153\n",
            "Epoch 871/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.2906e-04 - mse: 5.2903e-08 - val_loss: 0.0921 - val_mse: 0.0156\n",
            "Epoch 872/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 1.2721e-04 - mse: 5.2246e-08 - val_loss: 0.0908 - val_mse: 0.0155\n",
            "Epoch 873/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 1.2738e-04 - mse: 5.4049e-08 - val_loss: 0.0943 - val_mse: 0.0161\n",
            "Epoch 874/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.2720e-04 - mse: 5.1910e-08 - val_loss: 0.0854 - val_mse: 0.0145\n",
            "Epoch 875/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 1.2529e-04 - mse: 4.6632e-08 - val_loss: 0.0879 - val_mse: 0.0150\n",
            "Epoch 876/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 1.2590e-04 - mse: 4.8945e-08 - val_loss: 0.0978 - val_mse: 0.0166\n",
            "Epoch 877/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.2443e-04 - mse: 5.4080e-08 - val_loss: 0.0921 - val_mse: 0.0158\n",
            "Epoch 878/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.2398e-04 - mse: 4.8101e-08 - val_loss: 0.0784 - val_mse: 0.0132\n",
            "Epoch 879/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.2292e-04 - mse: 4.6163e-08 - val_loss: 0.0924 - val_mse: 0.0157\n",
            "Epoch 880/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.2208e-04 - mse: 5.0081e-08 - val_loss: 0.0977 - val_mse: 0.0166\n",
            "Epoch 881/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 1.2243e-04 - mse: 5.0133e-08 - val_loss: 0.0871 - val_mse: 0.0148\n",
            "Epoch 882/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.2239e-04 - mse: 5.4836e-08 - val_loss: 0.0974 - val_mse: 0.0167\n",
            "Epoch 883/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.1916e-04 - mse: 5.2089e-08 - val_loss: 0.0922 - val_mse: 0.0158\n",
            "Epoch 884/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 1.1995e-04 - mse: 4.7017e-08 - val_loss: 0.0873 - val_mse: 0.0149\n",
            "Epoch 885/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.1971e-04 - mse: 5.0162e-08 - val_loss: 0.0945 - val_mse: 0.0162\n",
            "Epoch 886/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.1722e-04 - mse: 4.7292e-08 - val_loss: 0.0899 - val_mse: 0.0153\n",
            "Epoch 887/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.1607e-04 - mse: 4.2686e-08 - val_loss: 0.0867 - val_mse: 0.0147\n",
            "Epoch 888/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 1.1595e-04 - mse: 4.2026e-08 - val_loss: 0.0898 - val_mse: 0.0153\n",
            "Epoch 889/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.1509e-04 - mse: 4.1767e-08 - val_loss: 0.0928 - val_mse: 0.0159\n",
            "Epoch 890/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.1491e-04 - mse: 4.4098e-08 - val_loss: 0.0929 - val_mse: 0.0159\n",
            "Epoch 891/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.1364e-04 - mse: 4.3509e-08 - val_loss: 0.0927 - val_mse: 0.0159\n",
            "Epoch 892/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 1.1411e-04 - mse: 4.7320e-08 - val_loss: 0.0928 - val_mse: 0.0159\n",
            "Epoch 893/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.1264e-04 - mse: 4.4108e-08 - val_loss: 0.0915 - val_mse: 0.0157\n",
            "Epoch 894/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.1167e-04 - mse: 4.0985e-08 - val_loss: 0.0884 - val_mse: 0.0151\n",
            "Epoch 895/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.1082e-04 - mse: 3.9357e-08 - val_loss: 0.0896 - val_mse: 0.0153\n",
            "Epoch 896/1000\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 1.0998e-04 - mse: 3.9338e-08 - val_loss: 0.0926 - val_mse: 0.0159\n",
            "Epoch 897/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.0987e-04 - mse: 4.2621e-08 - val_loss: 0.0924 - val_mse: 0.0159\n",
            "Epoch 898/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.0994e-04 - mse: 4.1096e-08 - val_loss: 0.0829 - val_mse: 0.0142\n",
            "Epoch 899/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.0898e-04 - mse: 3.8974e-08 - val_loss: 0.0874 - val_mse: 0.0149\n",
            "Epoch 900/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 1.0820e-04 - mse: 3.7759e-08 - val_loss: 0.0904 - val_mse: 0.0155\n",
            "Epoch 901/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.0726e-04 - mse: 3.8410e-08 - val_loss: 0.0923 - val_mse: 0.0158\n",
            "Epoch 902/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.0767e-04 - mse: 4.1092e-08 - val_loss: 0.0913 - val_mse: 0.0157\n",
            "Epoch 903/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.0628e-04 - mse: 3.6969e-08 - val_loss: 0.0837 - val_mse: 0.0142\n",
            "Epoch 904/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.0669e-04 - mse: 3.6121e-08 - val_loss: 0.0861 - val_mse: 0.0148\n",
            "Epoch 905/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.0501e-04 - mse: 3.4766e-08 - val_loss: 0.0883 - val_mse: 0.0151\n",
            "Epoch 906/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 1.0467e-04 - mse: 3.7069e-08 - val_loss: 0.0921 - val_mse: 0.0158\n",
            "Epoch 907/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.0364e-04 - mse: 3.5662e-08 - val_loss: 0.0901 - val_mse: 0.0155\n",
            "Epoch 908/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 1.0399e-04 - mse: 3.8889e-08 - val_loss: 0.0924 - val_mse: 0.0159\n",
            "Epoch 909/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.0256e-04 - mse: 3.5695e-08 - val_loss: 0.0890 - val_mse: 0.0153\n",
            "Epoch 910/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.0195e-04 - mse: 3.3641e-08 - val_loss: 0.0895 - val_mse: 0.0153\n",
            "Epoch 911/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.0311e-04 - mse: 3.3613e-08 - val_loss: 0.0821 - val_mse: 0.0140\n",
            "Epoch 912/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 1.0095e-04 - mse: 3.4609e-08 - val_loss: 0.0916 - val_mse: 0.0157\n",
            "Epoch 913/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.0130e-04 - mse: 3.6968e-08 - val_loss: 0.0999 - val_mse: 0.0172\n",
            "Epoch 914/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.0113e-04 - mse: 3.7887e-08 - val_loss: 0.0905 - val_mse: 0.0156\n",
            "Epoch 915/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 9.9270e-05 - mse: 3.4171e-08 - val_loss: 0.0914 - val_mse: 0.0157\n",
            "Epoch 916/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.0075e-04 - mse: 3.4081e-08 - val_loss: 0.0816 - val_mse: 0.0139\n",
            "Epoch 917/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.8471e-05 - mse: 3.0830e-08 - val_loss: 0.0894 - val_mse: 0.0154\n",
            "Epoch 918/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.7459e-05 - mse: 3.0060e-08 - val_loss: 0.0893 - val_mse: 0.0153\n",
            "Epoch 919/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.7275e-05 - mse: 3.4647e-08 - val_loss: 0.0958 - val_mse: 0.0165\n",
            "Epoch 920/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.7142e-05 - mse: 3.5401e-08 - val_loss: 0.0923 - val_mse: 0.0159\n",
            "Epoch 921/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.6479e-05 - mse: 3.1960e-08 - val_loss: 0.0838 - val_mse: 0.0143\n",
            "Epoch 922/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.6875e-05 - mse: 3.2601e-08 - val_loss: 0.0919 - val_mse: 0.0158\n",
            "Epoch 923/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.6075e-05 - mse: 3.0252e-08 - val_loss: 0.0821 - val_mse: 0.0140\n",
            "Epoch 924/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.5209e-05 - mse: 3.1704e-08 - val_loss: 0.0926 - val_mse: 0.0160\n",
            "Epoch 925/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.4139e-05 - mse: 3.1228e-08 - val_loss: 0.0908 - val_mse: 0.0156\n",
            "Epoch 926/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.3125e-05 - mse: 2.9522e-08 - val_loss: 0.0901 - val_mse: 0.0154\n",
            "Epoch 927/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 9.2531e-05 - mse: 2.6743e-08 - val_loss: 0.0809 - val_mse: 0.0138\n",
            "Epoch 928/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.2966e-05 - mse: 2.8311e-08 - val_loss: 0.0840 - val_mse: 0.0145\n",
            "Epoch 929/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.1421e-05 - mse: 2.5409e-08 - val_loss: 0.0811 - val_mse: 0.0139\n",
            "Epoch 930/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.1600e-05 - mse: 2.4658e-08 - val_loss: 0.0799 - val_mse: 0.0137\n",
            "Epoch 931/1000\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 9.1066e-05 - mse: 2.6494e-08 - val_loss: 0.0920 - val_mse: 0.0158\n",
            "Epoch 932/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 9.0088e-05 - mse: 2.5782e-08 - val_loss: 0.0860 - val_mse: 0.0147\n",
            "Epoch 933/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.9188e-05 - mse: 2.4682e-08 - val_loss: 0.0887 - val_mse: 0.0153\n",
            "Epoch 934/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.8696e-05 - mse: 2.5803e-08 - val_loss: 0.0913 - val_mse: 0.0158\n",
            "Epoch 935/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 8.8796e-05 - mse: 2.8430e-08 - val_loss: 0.0915 - val_mse: 0.0158\n",
            "Epoch 936/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.8031e-05 - mse: 2.6452e-08 - val_loss: 0.0907 - val_mse: 0.0156\n",
            "Epoch 937/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.7716e-05 - mse: 2.4775e-08 - val_loss: 0.0862 - val_mse: 0.0148\n",
            "Epoch 938/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.6820e-05 - mse: 2.5973e-08 - val_loss: 0.0921 - val_mse: 0.0159\n",
            "Epoch 939/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.7448e-05 - mse: 2.8960e-08 - val_loss: 0.0933 - val_mse: 0.0161\n",
            "Epoch 940/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.5416e-05 - mse: 2.5529e-08 - val_loss: 0.0821 - val_mse: 0.0141\n",
            "Epoch 941/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.6229e-05 - mse: 2.3144e-08 - val_loss: 0.0813 - val_mse: 0.0140\n",
            "Epoch 942/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.4493e-05 - mse: 2.2477e-08 - val_loss: 0.0874 - val_mse: 0.0151\n",
            "Epoch 943/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 8.4605e-05 - mse: 2.3310e-08 - val_loss: 0.0873 - val_mse: 0.0151\n",
            "Epoch 944/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.4845e-05 - mse: 2.7682e-08 - val_loss: 0.0961 - val_mse: 0.0166\n",
            "Epoch 945/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.4278e-05 - mse: 2.4912e-08 - val_loss: 0.0791 - val_mse: 0.0135\n",
            "Epoch 946/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 8.3527e-05 - mse: 2.0970e-08 - val_loss: 0.0820 - val_mse: 0.0140\n",
            "Epoch 947/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.2523e-05 - mse: 2.1706e-08 - val_loss: 0.0866 - val_mse: 0.0149\n",
            "Epoch 948/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.1838e-05 - mse: 2.1362e-08 - val_loss: 0.0854 - val_mse: 0.0148\n",
            "Epoch 949/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.1420e-05 - mse: 2.0760e-08 - val_loss: 0.0835 - val_mse: 0.0144\n",
            "Epoch 950/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.0534e-05 - mse: 2.0907e-08 - val_loss: 0.0913 - val_mse: 0.0158\n",
            "Epoch 951/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.1486e-05 - mse: 2.3926e-08 - val_loss: 0.0940 - val_mse: 0.0162\n",
            "Epoch 952/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.0347e-05 - mse: 2.2096e-08 - val_loss: 0.0851 - val_mse: 0.0146\n",
            "Epoch 953/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 7.9791e-05 - mse: 2.1325e-08 - val_loss: 0.0877 - val_mse: 0.0151\n",
            "Epoch 954/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 7.9348e-05 - mse: 2.0755e-08 - val_loss: 0.0830 - val_mse: 0.0143\n",
            "Epoch 955/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.8667e-05 - mse: 1.9708e-08 - val_loss: 0.0865 - val_mse: 0.0150\n",
            "Epoch 956/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.8463e-05 - mse: 2.0841e-08 - val_loss: 0.0869 - val_mse: 0.0151\n",
            "Epoch 957/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.7959e-05 - mse: 1.9968e-08 - val_loss: 0.0878 - val_mse: 0.0151\n",
            "Epoch 958/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.7501e-05 - mse: 1.9257e-08 - val_loss: 0.0892 - val_mse: 0.0154\n",
            "Epoch 959/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.7407e-05 - mse: 1.9786e-08 - val_loss: 0.0942 - val_mse: 0.0163\n",
            "Epoch 960/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.6448e-05 - mse: 2.1021e-08 - val_loss: 0.0867 - val_mse: 0.0150\n",
            "Epoch 961/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.6251e-05 - mse: 1.9106e-08 - val_loss: 0.0836 - val_mse: 0.0144\n",
            "Epoch 962/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 7.5929e-05 - mse: 1.8644e-08 - val_loss: 0.0858 - val_mse: 0.0148\n",
            "Epoch 963/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.5431e-05 - mse: 2.0110e-08 - val_loss: 0.0898 - val_mse: 0.0156\n",
            "Epoch 964/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.4884e-05 - mse: 1.9758e-08 - val_loss: 0.0841 - val_mse: 0.0146\n",
            "Epoch 965/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.4438e-05 - mse: 1.9075e-08 - val_loss: 0.0861 - val_mse: 0.0149\n",
            "Epoch 966/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 7.3945e-05 - mse: 1.8147e-08 - val_loss: 0.0830 - val_mse: 0.0143\n",
            "Epoch 967/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.4359e-05 - mse: 1.7404e-08 - val_loss: 0.0796 - val_mse: 0.0137\n",
            "Epoch 968/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 7.4560e-05 - mse: 1.8202e-08 - val_loss: 0.0871 - val_mse: 0.0150\n",
            "Epoch 969/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.3208e-05 - mse: 1.7228e-08 - val_loss: 0.0820 - val_mse: 0.0141\n",
            "Epoch 970/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.3538e-05 - mse: 1.8710e-08 - val_loss: 0.0902 - val_mse: 0.0157\n",
            "Epoch 971/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 7.2488e-05 - mse: 1.7782e-08 - val_loss: 0.0857 - val_mse: 0.0148\n",
            "Epoch 972/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.1653e-05 - mse: 1.7215e-08 - val_loss: 0.0842 - val_mse: 0.0146\n",
            "Epoch 973/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.1141e-05 - mse: 1.6597e-08 - val_loss: 0.0903 - val_mse: 0.0158\n",
            "Epoch 974/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 7.0643e-05 - mse: 1.7172e-08 - val_loss: 0.0866 - val_mse: 0.0150\n",
            "Epoch 975/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.0469e-05 - mse: 1.6678e-08 - val_loss: 0.0839 - val_mse: 0.0145\n",
            "Epoch 976/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 7.0247e-05 - mse: 1.7597e-08 - val_loss: 0.0877 - val_mse: 0.0153\n",
            "Epoch 977/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 6.9752e-05 - mse: 1.6336e-08 - val_loss: 0.0806 - val_mse: 0.0139\n",
            "Epoch 978/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 6.9127e-05 - mse: 1.5554e-08 - val_loss: 0.0862 - val_mse: 0.0149\n",
            "Epoch 979/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.8917e-05 - mse: 1.7327e-08 - val_loss: 0.0889 - val_mse: 0.0154\n",
            "Epoch 980/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 6.8207e-05 - mse: 1.5444e-08 - val_loss: 0.0806 - val_mse: 0.0139\n",
            "Epoch 981/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.7935e-05 - mse: 1.4824e-08 - val_loss: 0.0813 - val_mse: 0.0140\n",
            "Epoch 982/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 6.8017e-05 - mse: 1.6966e-08 - val_loss: 0.0924 - val_mse: 0.0160\n",
            "Epoch 983/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.7414e-05 - mse: 1.6704e-08 - val_loss: 0.0885 - val_mse: 0.0154\n",
            "Epoch 984/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.7191e-05 - mse: 1.5299e-08 - val_loss: 0.0827 - val_mse: 0.0143\n",
            "Epoch 985/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.6800e-05 - mse: 1.4132e-08 - val_loss: 0.0857 - val_mse: 0.0148\n",
            "Epoch 986/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 6.6992e-05 - mse: 1.4312e-08 - val_loss: 0.0788 - val_mse: 0.0136\n",
            "Epoch 987/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.6075e-05 - mse: 1.5620e-08 - val_loss: 0.0883 - val_mse: 0.0154\n",
            "Epoch 988/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.4843e-05 - mse: 1.3502e-08 - val_loss: 0.0810 - val_mse: 0.0140\n",
            "Epoch 989/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.4943e-05 - mse: 1.2937e-08 - val_loss: 0.0810 - val_mse: 0.0141\n",
            "Epoch 990/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 6.4494e-05 - mse: 1.2753e-08 - val_loss: 0.0861 - val_mse: 0.0149\n",
            "Epoch 991/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.4786e-05 - mse: 1.4932e-08 - val_loss: 0.0940 - val_mse: 0.0164\n",
            "Epoch 992/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.3679e-05 - mse: 1.4524e-08 - val_loss: 0.0881 - val_mse: 0.0154\n",
            "Epoch 993/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.3455e-05 - mse: 1.3023e-08 - val_loss: 0.0810 - val_mse: 0.0140\n",
            "Epoch 994/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 6.3472e-05 - mse: 1.2804e-08 - val_loss: 0.0813 - val_mse: 0.0141\n",
            "Epoch 995/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.3824e-05 - mse: 1.3039e-08 - val_loss: 0.0798 - val_mse: 0.0138\n",
            "Epoch 996/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.2177e-05 - mse: 1.2543e-08 - val_loss: 0.0889 - val_mse: 0.0155\n",
            "Epoch 997/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.2778e-05 - mse: 1.5204e-08 - val_loss: 0.0913 - val_mse: 0.0159\n",
            "Epoch 998/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 6.2283e-05 - mse: 1.4126e-08 - val_loss: 0.0790 - val_mse: 0.0137\n",
            "Epoch 999/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.1800e-05 - mse: 1.3576e-08 - val_loss: 0.0878 - val_mse: 0.0153\n",
            "Epoch 1000/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 6.1104e-05 - mse: 1.2999e-08 - val_loss: 0.0816 - val_mse: 0.0142\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqNZ1szcW4O4"
      },
      "source": [
        "##3.3 Plot loss/validation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "6cWOmZsYVnv8",
        "outputId": "6cdd9f87-5578-4e24-a6f4-4ea0e2fd0579"
      },
      "source": [
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scattergl(y=history.history['loss'],\n",
        "                    name='Train'))\n",
        "fig.add_trace(go.Scattergl(y=history.history['val_loss'],\n",
        "                    name='Validation'))\n",
        "\n",
        "fig.update_layout(height=500, width=700,\n",
        "                  xaxis_title='Epoch',\n",
        "                  yaxis_title='Loss')\n",
        "fig.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"1929b219-2484-459c-87df-f3d03b0f0de6\" class=\"plotly-graph-div\" style=\"height:500px; width:700px;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"1929b219-2484-459c-87df-f3d03b0f0de6\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '1929b219-2484-459c-87df-f3d03b0f0de6',\n",
              "                        [{\"name\": \"Train\", \"type\": \"scattergl\", \"y\": [1.5234603881835938, 1.271854043006897, 1.2250547409057617, 1.1502084732055664, 1.1035406589508057, 1.0577377080917358, 1.020957112312317, 0.9749732613563538, 0.9350075721740723, 0.9020705819129944, 0.8654181957244873, 0.8289492130279541, 0.8058488368988037, 0.768700122833252, 0.7357683181762695, 0.706403911113739, 0.6829841732978821, 0.6556644439697266, 0.6353265047073364, 0.6155254244804382, 0.5925449728965759, 0.574563205242157, 0.5561379790306091, 0.5437858700752258, 0.5232794284820557, 0.5096071362495422, 0.495344877243042, 0.48106157779693604, 0.46093007922172546, 0.45221173763275146, 0.43666011095046997, 0.4237522780895233, 0.4149060845375061, 0.3984927237033844, 0.38663554191589355, 0.3770736753940582, 0.3690353333950043, 0.355059951543808, 0.3502456545829773, 0.33941057324409485, 0.32960081100463867, 0.31928372383117676, 0.31371167302131653, 0.3033473491668701, 0.29550081491470337, 0.2913314402103424, 0.2812967598438263, 0.2759503126144409, 0.2677513062953949, 0.2599097192287445, 0.25743699073791504, 0.2490490823984146, 0.2443949580192566, 0.23773451149463654, 0.22983510792255402, 0.2249007672071457, 0.22118645906448364, 0.21360459923744202, 0.20972542464733124, 0.20791374146938324, 0.20107482373714447, 0.19471953809261322, 0.19048330187797546, 0.1863556206226349, 0.1803491711616516, 0.17808933556079865, 0.17449256777763367, 0.17140750586986542, 0.16624608635902405, 0.16363757848739624, 0.15904276072978973, 0.15780416131019592, 0.151407390832901, 0.14971239864826202, 0.145302414894104, 0.142466738820076, 0.13861052691936493, 0.13705773651599884, 0.13482344150543213, 0.1301439106464386, 0.1282830685377121, 0.1267782598733902, 0.12277812510728836, 0.1219121441245079, 0.11967664211988449, 0.1154092475771904, 0.11340229213237762, 0.11362805217504501, 0.11222830414772034, 0.10689695179462433, 0.10635929554700851, 0.1034240871667862, 0.09983696788549423, 0.09779736399650574, 0.09789454936981201, 0.09563669562339783, 0.09300088882446289, 0.09197520464658737, 0.08903056383132935, 0.08764324337244034, 0.08606963604688644, 0.08465033024549484, 0.0838262140750885, 0.0813741460442543, 0.0803106352686882, 0.07898099720478058, 0.07685716450214386, 0.07605297863483429, 0.07361350953578949, 0.0727623999118805, 0.07068576663732529, 0.06958078593015671, 0.06905566155910492, 0.06694623827934265, 0.06553898751735687, 0.06485269963741302, 0.0637831911444664, 0.06372808665037155, 0.061138302087783813, 0.06072472408413887, 0.060058943927288055, 0.058909982442855835, 0.05759875476360321, 0.056693386286497116, 0.05582967400550842, 0.05411433055996895, 0.0540444478392601, 0.052735310047864914, 0.05203774571418762, 0.05131228640675545, 0.05111794173717499, 0.049040790647268295, 0.04896315559744835, 0.04829433560371399, 0.04710422456264496, 0.046826232224702835, 0.04519949108362198, 0.044505584985017776, 0.04414462670683861, 0.043213777244091034, 0.04271569103002548, 0.042395371943712234, 0.04160080850124359, 0.040638748556375504, 0.040066253393888474, 0.03986334055662155, 0.0386381633579731, 0.03781280294060707, 0.03774022310972214, 0.03717263415455818, 0.036470409482717514, 0.03593624010682106, 0.035711053758859634, 0.03466741740703583, 0.03425581753253937, 0.03345372527837753, 0.03318861126899719, 0.03286273404955864, 0.03207112476229668, 0.03196170926094055, 0.03123605251312256, 0.03081301413476467, 0.03003837540745735, 0.029808348044753075, 0.029363330453634262, 0.02891526371240616, 0.02870120108127594, 0.02794400416314602, 0.02803579717874527, 0.02738833986222744, 0.027101870626211166, 0.026618892326951027, 0.026546766981482506, 0.025871584191918373, 0.02619149722158909, 0.02575204148888588, 0.02570943534374237, 0.024498162791132927, 0.024203049018979073, 0.023711226880550385, 0.023508796468377113, 0.023208240047097206, 0.02278001420199871, 0.02237033285200596, 0.02224528230726719, 0.02205953560769558, 0.021868377923965454, 0.02147751860320568, 0.021474573761224747, 0.020966604351997375, 0.020912641659379005, 0.02048354782164097, 0.020222770050168037, 0.019685685634613037, 0.01973808743059635, 0.019448386505246162, 0.019023019820451736, 0.019265294075012207, 0.01864195056259632, 0.01839580200612545, 0.01783877983689308, 0.01792345941066742, 0.017783695831894875, 0.01743766851723194, 0.017438139766454697, 0.0170164555311203, 0.017024138942360878, 0.016421426087617874, 0.016389410942792892, 0.016280196607112885, 0.016032550483942032, 0.015779148787260056, 0.015505713410675526, 0.015371781773865223, 0.015308612026274204, 0.015265453606843948, 0.014788856729865074, 0.014656529761850834, 0.014905679039657116, 0.014411114156246185, 0.014225012622773647, 0.013962236233055592, 0.013831955380737782, 0.0138293681666255, 0.01359384786337614, 0.013464143499732018, 0.013304167427122593, 0.013534790836274624, 0.01288366038352251, 0.012742411345243454, 0.012955846264958382, 0.012433180585503578, 0.01226366963237524, 0.01228372287005186, 0.012150042690336704, 0.011928454972803593, 0.011736114509403706, 0.011651884764432907, 0.011642047204077244, 0.011521977372467518, 0.011305765248835087, 0.011203872971236706, 0.011158500798046589, 0.011297335848212242, 0.010831226594746113, 0.010802554897964, 0.01066169235855341, 0.010414552874863148, 0.010412145406007767, 0.010287664830684662, 0.010132597759366035, 0.010086355730891228, 0.009959693066775799, 0.009848082438111305, 0.009842842817306519, 0.009604211896657944, 0.009638513438403606, 0.009433656930923462, 0.009373431093990803, 0.009255246259272099, 0.00911769364029169, 0.009096192196011543, 0.009009722620248795, 0.008879474364221096, 0.00886540301144123, 0.008734109811484814, 0.008644687943160534, 0.008569372817873955, 0.008429001085460186, 0.008485953323543072, 0.008364922367036343, 0.008202063851058483, 0.00815539713948965, 0.008017199113965034, 0.00796943437308073, 0.007895226590335369, 0.00783361867070198, 0.007705997675657272, 0.007644134107977152, 0.007666122633963823, 0.007541814353317022, 0.007469988893717527, 0.007371536921709776, 0.00755575904622674, 0.007196040358394384, 0.007215137593448162, 0.007082614582031965, 0.006987243890762329, 0.006947801914066076, 0.006839594338089228, 0.006783541291952133, 0.006785162258893251, 0.006713071838021278, 0.006612907163798809, 0.006586161907762289, 0.006489328574389219, 0.006542047951370478, 0.006392350886017084, 0.0062716808170080185, 0.006289577577263117, 0.00616932800039649, 0.006335989106446505, 0.006119736470282078, 0.006134383380413055, 0.0060285162180662155, 0.005926790647208691, 0.0059000956825912, 0.005830326117575169, 0.005735398270189762, 0.00577319273725152, 0.005629441235214472, 0.005625445395708084, 0.005640423856675625, 0.005567384418100119, 0.00549642788246274, 0.005442282650619745, 0.005464903544634581, 0.005311749409884214, 0.00527411000803113, 0.0052150944247841835, 0.005174486432224512, 0.005186678376048803, 0.005085986107587814, 0.005056748632341623, 0.00497438944876194, 0.0049371798522770405, 0.00490905623883009, 0.004972901660948992, 0.004848260432481766, 0.004821715876460075, 0.004808944649994373, 0.004693709779530764, 0.004678197670727968, 0.004709560424089432, 0.004740637261420488, 0.004615041892975569, 0.0045111472718417645, 0.004459999036043882, 0.004483271390199661, 0.004388405475765467, 0.0043483623303473, 0.004301078151911497, 0.004289887845516205, 0.004234383814036846, 0.0041911001317203045, 0.0041725547052919865, 0.004111005924642086, 0.0040826452895998955, 0.004100386053323746, 0.004014977253973484, 0.004004870541393757, 0.0039935605600476265, 0.003910420462489128, 0.003955626394599676, 0.003849610686302185, 0.003856224939227104, 0.0038253178354352713, 0.003754884470254183, 0.003820661222562194, 0.003736451966688037, 0.0036776913329958916, 0.0036722898948937654, 0.0035951368045061827, 0.003671996295452118, 0.0036083213053643703, 0.003554212860763073, 0.003491399809718132, 0.00347850751131773, 0.0034844756592065096, 0.00342349149286747, 0.0033895710948854685, 0.003366433549672365, 0.0034695137292146683, 0.0033552234526723623, 0.0033134245313704014, 0.0033528872299939394, 0.0032173972576856613, 0.0032888702116906643, 0.0031450283713638783, 0.0031767161563038826, 0.003135896287858486, 0.0031235411297529936, 0.0030777074862271547, 0.003065137891098857, 0.0030326279811561108, 0.003035162575542927, 0.003008677391335368, 0.0029543053824454546, 0.00295143760740757, 0.002918446436524391, 0.0029282327741384506, 0.0028631221503019333, 0.002871895907446742, 0.002839331515133381, 0.0028139930218458176, 0.0027656264137476683, 0.002775398315861821, 0.0027423507999628782, 0.002715200884267688, 0.002715019043534994, 0.0026709050871431828, 0.0026483561377972364, 0.002631385810673237, 0.0026345013175159693, 0.002606690628454089, 0.0026094652712345123, 0.0025547363329678774, 0.002522907918319106, 0.00252885022200644, 0.002484722062945366, 0.00247558974660933, 0.002487560035660863, 0.002462026197463274, 0.002409872366115451, 0.0023822553921490908, 0.002371839713305235, 0.002369944006204605, 0.002345689106732607, 0.0023305201902985573, 0.002306923968717456, 0.0022973527666181326, 0.0022606104612350464, 0.0022616032510995865, 0.0022710387129336596, 0.0022170140873640776, 0.002222297713160515, 0.0021802072878926992, 0.002180789364501834, 0.002152635483071208, 0.002152391942217946, 0.002131326822564006, 0.0021417723037302494, 0.0020937493536621332, 0.0021086346823722124, 0.0021008693147450686, 0.0020436595659703016, 0.0020390034187585115, 0.002018885686993599, 0.002038750797510147, 0.0019998096395283937, 0.0019897278398275375, 0.0019735314417630434, 0.001964948372915387, 0.0019385473569855094, 0.0019452403066679835, 0.0019558826461434364, 0.0019058834295719862, 0.0018686350667849183, 0.0018679866334423423, 0.0018737788777798414, 0.0018374980427324772, 0.0018380150431767106, 0.0018152408301830292, 0.0017919803503900766, 0.0018077902495861053, 0.0018117517465725541, 0.0017855385085567832, 0.0017368426779285073, 0.001751425676047802, 0.0017338861944153905, 0.0017111341003328562, 0.0016996819758787751, 0.0016709913033992052, 0.0016770310467109084, 0.0016711975913494825, 0.0016419769963249564, 0.0016561076045036316, 0.0016403596382588148, 0.0016073168953880668, 0.0016060450579971075, 0.0016159521182999015, 0.0015739024383947253, 0.0015729607548564672, 0.0015544163761660457, 0.001550761517137289, 0.0015228254487738013, 0.0015450887149199843, 0.001510809874162078, 0.0015225168317556381, 0.0014850650914013386, 0.0014859569491818547, 0.0014712399570271373, 0.0014652853133156896, 0.0014580761780962348, 0.0014409542782232165, 0.0014350343262776732, 0.0014670026721432805, 0.0014070604229345918, 0.001422703149728477, 0.0014153642114251852, 0.0014428814174607396, 0.0013953791931271553, 0.001370520330965519, 0.0013571787858381867, 0.0013513282174244523, 0.0013410288374871016, 0.001334459870122373, 0.0013133168686181307, 0.001308623468503356, 0.001301268464885652, 0.0012931671226397157, 0.001278943382203579, 0.001282719080336392, 0.001272419816814363, 0.0012530835811048746, 0.0012493252288550138, 0.0012354736682027578, 0.0012351411860436201, 0.0012146863155066967, 0.0012172381393611431, 0.0012172478018328547, 0.0012027156772091985, 0.001181194675154984, 0.0011860178783535957, 0.0011976510286331177, 0.0011610902147367597, 0.001160881482064724, 0.0011455174535512924, 0.001141174929216504, 0.0011275048600509763, 0.001133113750256598, 0.0011246418580412865, 0.0011310350382700562, 0.001128669362515211, 0.0011269531678408384, 0.0010970188304781914, 0.0010813635308295488, 0.0010684258304536343, 0.0010621425462886691, 0.0010641422122716904, 0.0010512486333027482, 0.001046933582983911, 0.0010348238283768296, 0.0010316744446754456, 0.0010275844251736999, 0.0010143097024410963, 0.001004340243525803, 0.0010061095235869288, 0.0009970262181013823, 0.0009985283249989152, 0.000993245281279087, 0.0009801369160413742, 0.0009763080161064863, 0.0009667629492469132, 0.000956112053245306, 0.0009572978597134352, 0.0009464818867854774, 0.0009395857341587543, 0.0009311322937719524, 0.0009225009707733989, 0.00092156114988029, 0.000912730407435447, 0.0009083810728043318, 0.000902635685633868, 0.0008940000552684069, 0.0008986755274236202, 0.000885765824932605, 0.0008827589335851371, 0.000868236820679158, 0.0008699733880348504, 0.0008544450392946601, 0.0008691976545378566, 0.0008475337526760995, 0.0008492530323565006, 0.0008431125897914171, 0.0008322754292748868, 0.0008200578740797937, 0.0008218833245337009, 0.0008154623792506754, 0.0008088162867352366, 0.0008137092809192836, 0.0007977608474902809, 0.0007927173282951117, 0.0007934306049719453, 0.0007838772726245224, 0.0007820289465598762, 0.0007718401029706001, 0.000769470410887152, 0.0007586413994431496, 0.0007578670629300177, 0.0007565671694464982, 0.0007490604184567928, 0.000743913755286485, 0.00073982187313959, 0.0007321764715015888, 0.0007270220085047185, 0.0007254902157001197, 0.0007221073028631508, 0.0007227437454275787, 0.0007139405352063477, 0.0007053114823065698, 0.0006993293645791709, 0.0006995201692916453, 0.0006904750480316579, 0.0006937438738532364, 0.0006865152390673757, 0.0006821765564382076, 0.0006724784034304321, 0.000671139161568135, 0.0006673741154372692, 0.0006611240096390247, 0.0006683160318061709, 0.0006567906239069998, 0.0006537510780617595, 0.0006422426667995751, 0.0006406904431059957, 0.000633615127298981, 0.0006327830487862229, 0.0006427525077015162, 0.0006266431300900877, 0.0006185649544931948, 0.0006129745743237436, 0.0006106913206167519, 0.0006056426791474223, 0.0006026164628565311, 0.000604987028054893, 0.0005999174900352955, 0.0005932737840339541, 0.0005874267080798745, 0.0005847082938998938, 0.0005818381905555725, 0.0005784942768514156, 0.0005714803119190037, 0.0005692553240805864, 0.0005664383643306792, 0.000564242887776345, 0.0005608138744719326, 0.0005572444060817361, 0.0005508808535523713, 0.0005520685226656497, 0.0005493075586855412, 0.0005420220550149679, 0.0005371026345528662, 0.0005348149570636451, 0.0005345456302165985, 0.0005297937896102667, 0.0005263303173705935, 0.0005208727088756859, 0.0005187182687222958, 0.0005152749363332987, 0.0005154950777068734, 0.0005101721617393196, 0.0005115166422910988, 0.0005086776800453663, 0.0004971928428858519, 0.0004990589804947376, 0.0004923683591187, 0.0004917941405437887, 0.0004916588077321649, 0.00048455630894750357, 0.00048102904111146927, 0.00047738378634676337, 0.00047497230116277933, 0.00048010001773945987, 0.0004699645796790719, 0.0004711368237622082, 0.0004666656022891402, 0.00046123864012770355, 0.0004581546236295253, 0.0004602388944476843, 0.00045073768706060946, 0.00045457755913957953, 0.0004474371671676636, 0.00044536389759741724, 0.00044211186468601227, 0.0004420772602315992, 0.0004372314433567226, 0.0004365050117485225, 0.0004316149861551821, 0.00042703445069491863, 0.0004243458970449865, 0.00042107515037059784, 0.0004227741155773401, 0.0004162883560638875, 0.0004160256066825241, 0.0004169887106399983, 0.0004165951395407319, 0.0004152268811594695, 0.0004076758923474699, 0.00040096588782034814, 0.0004028852563351393, 0.0004001849447377026, 0.00039445949369110167, 0.00039163706242106855, 0.0003893258108291775, 0.0003876909031532705, 0.00038468322600238025, 0.00038359459722414613, 0.0003822107973974198, 0.0003787580062635243, 0.00037594634341076016, 0.00037398969288915396, 0.00037236910429783165, 0.00037143597728572786, 0.0003717665676958859, 0.0003671650483738631, 0.0003651255974546075, 0.00036009764880873263, 0.0003583758953027427, 0.00035819329787045717, 0.00035511123132891953, 0.0003560675831977278, 0.0003608008846640587, 0.0003552661510184407, 0.00034518851316533983, 0.00034884613705798984, 0.000347033201251179, 0.00034367022453807294, 0.0003370189806446433, 0.0003357110545039177, 0.0003338282695040107, 0.00032958018709905446, 0.0003309300809632987, 0.000326231267536059, 0.0003234294999856502, 0.000324600754538551, 0.00031844142358750105, 0.0003179487830493599, 0.0003149142430629581, 0.00031326827593147755, 0.00031134358141571283, 0.00031041764304973185, 0.0003079171874560416, 0.0003073038242291659, 0.0003046125639230013, 0.0003037656133528799, 0.0003020682488568127, 0.00030102848540991545, 0.0002975509560201317, 0.0002958097029477358, 0.00029346044175326824, 0.00029195149545557797, 0.0002911002084147185, 0.0002900899271480739, 0.0002867181319743395, 0.00028609298169612885, 0.000284811103483662, 0.00028352331719361246, 0.00028165875119157135, 0.00027865407173521817, 0.0002787805860862136, 0.00027731116279028356, 0.00027312274323776364, 0.0002711831184569746, 0.00027059754938818514, 0.00026949954917654395, 0.0002675775613170117, 0.0002659390738699585, 0.0002710665576159954, 0.0002639094309415668, 0.000262704910710454, 0.00025986513355746865, 0.00026214419631287456, 0.0002571114164311439, 0.00025385883054696023, 0.00025224898126907647, 0.0002509290352463722, 0.00025387239293195307, 0.00025455813738517463, 0.0002512602077331394, 0.00024525963817723095, 0.0002453869965393096, 0.0002430439490126446, 0.00024040612333919853, 0.00023910385789349675, 0.00023883821268100291, 0.00023684224288444966, 0.00023365770175587386, 0.00023284353665076196, 0.00023305769718717784, 0.00023150595370680094, 0.0002290318807354197, 0.00023013218014966697, 0.00023438585049007088, 0.0002250358957098797, 0.00022546030231751502, 0.0002225935459136963, 0.00022529355192091316, 0.00022032359265722334, 0.00021942268358543515, 0.0002175060799345374, 0.00021726719569414854, 0.00021438783733174205, 0.000212617072975263, 0.0002136871189577505, 0.00021160695177968591, 0.00021014190861023962, 0.0002113201335305348, 0.0002118870324920863, 0.0002062581479549408, 0.00020538450917229056, 0.0002037700469372794, 0.000203021802008152, 0.00020154209050815552, 0.0001986805145861581, 0.00019944869563914835, 0.00019701896235346794, 0.0001981139648705721, 0.00019997562048956752, 0.00019576551858335733, 0.000193084153579548, 0.0001912873995024711, 0.00019059893384110183, 0.00019022601190954447, 0.00018713042663875967, 0.0001866682869149372, 0.00018669482960831374, 0.0001861542259575799, 0.0001843945647124201, 0.00018296641064807773, 0.00018108298536390066, 0.00018050633661914617, 0.00018003991863224655, 0.0001800534810172394, 0.0001775811251718551, 0.00017882633255794644, 0.00017585535533726215, 0.00017514113278593868, 0.00017522257985547185, 0.0001729100476950407, 0.0001719512220006436, 0.0001703505840850994, 0.0001689740747679025, 0.000169529695995152, 0.00016768746718298644, 0.00016699018306098878, 0.00016512787260580808, 0.00016418559243902564, 0.00016335613327100873, 0.0001624600263312459, 0.00016163656255230308, 0.00016076322935987264, 0.00015998254821170121, 0.00015828662435524166, 0.00015981456090230495, 0.00015834449732210487, 0.00015581650950480253, 0.00015519022417720407, 0.00015499407891184092, 0.00015392426575999707, 0.0001523350947536528, 0.00015126826474443078, 0.00015087690553627908, 0.00015057979908306152, 0.00015034826355986297, 0.0001488950802013278, 0.00014752353308722377, 0.00014632353850174695, 0.00014544754230882972, 0.00014486264262814075, 0.00014412074233405292, 0.00014343579823616892, 0.00014319767069537193, 0.00014110053598415107, 0.00014055454812478274, 0.00014017941430211067, 0.00013882081839255989, 0.00014287146041169763, 0.0001374633575323969, 0.0001374944840790704, 0.00013632926857098937, 0.0001351068785879761, 0.00013405836944002658, 0.0001345451601082459, 0.00013373738329391927, 0.00013247034803498536, 0.00013263634173199534, 0.0001317375135840848, 0.0001309850485995412, 0.00012922640598844737, 0.00012905937910545617, 0.00012720595987048, 0.00012738436635117978, 0.00012720363156404346, 0.00012529366358648986, 0.00012590194819495082, 0.00012443409650586545, 0.00012398081889841706, 0.00012292373867239803, 0.00012207961117383093, 0.0001224261795869097, 0.00012238843191880733, 0.00011916457151528448, 0.00011995359091088176, 0.00011970815103268251, 0.00011721806367859244, 0.00011606650514295325, 0.0001159533640020527, 0.00011509262549225241, 0.00011490683391457424, 0.0001136428109020926, 0.00011411121522542089, 0.00011264241038588807, 0.00011166749754920602, 0.0001108165888581425, 0.00010998027573805302, 0.0001098700740840286, 0.00010993901378242299, 0.0001089822908397764, 0.00010820350871654227, 0.00010726493201218545, 0.0001076713451766409, 0.00010628092422848567, 0.00010669005860108882, 0.00010500595817575231, 0.00010466646926943213, 0.00010363510227762163, 0.00010399314487585798, 0.00010255603410769254, 0.00010194934293394908, 0.00010310826473869383, 0.00010095161997014657, 0.00010130239388672635, 0.00010113008465850726, 9.926957864081487e-05, 0.0001007469545584172, 9.847050387179479e-05, 9.745893476065248e-05, 9.727483848109841e-05, 9.714236512081698e-05, 9.647945989854634e-05, 9.687483543530107e-05, 9.60752586252056e-05, 9.520872845314443e-05, 9.41392790991813e-05, 9.312529437011108e-05, 9.253071766579524e-05, 9.296567441197112e-05, 9.142083581537008e-05, 9.16003409656696e-05, 9.106600191444159e-05, 9.008798224385828e-05, 8.918804087443277e-05, 8.869602606864646e-05, 8.879579399945214e-05, 8.803107630228624e-05, 8.771551074460149e-05, 8.682032785145566e-05, 8.744794467929751e-05, 8.541584247723222e-05, 8.622857421869412e-05, 8.449335291516036e-05, 8.460509707219899e-05, 8.484516729367897e-05, 8.427797729382291e-05, 8.352740405825898e-05, 8.252291445387527e-05, 8.18378830444999e-05, 8.141967555275187e-05, 8.053371857386082e-05, 8.14855593489483e-05, 8.034710481297225e-05, 7.979060319485143e-05, 7.934846507851034e-05, 7.866685336921364e-05, 7.846265361877158e-05, 7.795880810590461e-05, 7.750104850856587e-05, 7.740686851320788e-05, 7.6448151958175e-05, 7.625119178555906e-05, 7.592924521304667e-05, 7.54314532969147e-05, 7.488365372410044e-05, 7.443767390213907e-05, 7.394523709081113e-05, 7.435886072926223e-05, 7.45595752960071e-05, 7.320811710087582e-05, 7.35377034288831e-05, 7.248846668517217e-05, 7.165261922637001e-05, 7.114077743608505e-05, 7.064338569762185e-05, 7.046850078040734e-05, 7.024688966339454e-05, 6.975210271775723e-05, 6.912704702699557e-05, 6.891708471812308e-05, 6.82068130117841e-05, 6.7934874095954e-05, 6.80171069689095e-05, 6.741430843248963e-05, 6.719094380969182e-05, 6.680037040496245e-05, 6.699153163935989e-05, 6.607544491998851e-05, 6.48427230771631e-05, 6.494326225947589e-05, 6.449444481404498e-05, 6.478604336734861e-05, 6.367913010763004e-05, 6.34546740911901e-05, 6.347213638946414e-05, 6.382363790180534e-05, 6.217681948328391e-05, 6.277848297031596e-05, 6.2283463194035e-05, 6.180015770951286e-05, 6.110432150308043e-05]}, {\"name\": \"Validation\", \"type\": \"scattergl\", \"y\": [1.8325276374816895, 2.107595443725586, 1.8901751041412354, 1.762485384941101, 1.723910927772522, 1.7277920246124268, 1.572575569152832, 1.6116276979446411, 1.4561904668807983, 1.4416780471801758, 1.3938874006271362, 1.4343409538269043, 1.1835530996322632, 1.3130110502243042, 1.258724570274353, 1.1323518753051758, 1.0920746326446533, 1.174200177192688, 1.156903862953186, 0.9564534425735474, 1.0951855182647705, 1.015477180480957, 1.0152665376663208, 0.9511440396308899, 0.9129762649536133, 0.9316933155059814, 0.9793792963027954, 0.8586554527282715, 0.964785099029541, 0.9276838302612305, 0.833731472492218, 0.8268636465072632, 0.878304660320282, 0.8690224289894104, 0.7937456369400024, 0.8124315142631531, 0.7567111253738403, 0.8417626619338989, 0.7842229604721069, 0.767274022102356, 0.7687790393829346, 0.7286150455474854, 0.8165633082389832, 0.7590765357017517, 0.7077948451042175, 0.7032394409179688, 0.7294909954071045, 0.7638311982154846, 0.6712477803230286, 0.6975007057189941, 0.6745260953903198, 0.7148318886756897, 0.7261321544647217, 0.6148703694343567, 0.6176922917366028, 0.6741283535957336, 0.6537345051765442, 0.629037082195282, 0.63071608543396, 0.6625418663024902, 0.5891022086143494, 0.6244668960571289, 0.6083746552467346, 0.6050524711608887, 0.6388290524482727, 0.5710883140563965, 0.6322293281555176, 0.5649086236953735, 0.5685610175132751, 0.609175980091095, 0.589671790599823, 0.49893665313720703, 0.6149548292160034, 0.5897626876831055, 0.5086650848388672, 0.5234858393669128, 0.5626272559165955, 0.5532284379005432, 0.4984428286552429, 0.5515314340591431, 0.5396506190299988, 0.5035356283187866, 0.5153244733810425, 0.5670299530029297, 0.43870189785957336, 0.5023202300071716, 0.5191911458969116, 0.4406527578830719, 0.6018025279045105, 0.42987367510795593, 0.46800270676612854, 0.5051318407058716, 0.5121999979019165, 0.44007691740989685, 0.4360097050666809, 0.4675964415073395, 0.4737723171710968, 0.49943238496780396, 0.4254605770111084, 0.39476174116134644, 0.46964213252067566, 0.47421640157699585, 0.40322425961494446, 0.46186983585357666, 0.45023787021636963, 0.47274303436279297, 0.43079087138175964, 0.42017361521720886, 0.42864638566970825, 0.4441741704940796, 0.4325587749481201, 0.44646573066711426, 0.410307914018631, 0.38058528304100037, 0.41214337944984436, 0.4377328157424927, 0.4128212332725525, 0.379542738199234, 0.44051599502563477, 0.3938840627670288, 0.3870704770088196, 0.3861013948917389, 0.4089200794696808, 0.3594633936882019, 0.3825645446777344, 0.3415563106536865, 0.3726702034473419, 0.357086181640625, 0.37982386350631714, 0.3519059717655182, 0.31352123618125916, 0.3893420100212097, 0.37295442819595337, 0.37305670976638794, 0.27919670939445496, 0.33481818437576294, 0.37581175565719604, 0.3607766330242157, 0.3193906247615814, 0.33883193135261536, 0.36159294843673706, 0.3469488024711609, 0.26169952750205994, 0.34285902976989746, 0.37154421210289, 0.30280041694641113, 0.324908047914505, 0.35323405265808105, 0.31826987862586975, 0.32285094261169434, 0.29480984807014465, 0.30274567008018494, 0.3152763247489929, 0.35185152292251587, 0.32772096991539, 0.28932735323905945, 0.2708946466445923, 0.2795283794403076, 0.29277703166007996, 0.28420814871788025, 0.2953217625617981, 0.3010600507259369, 0.27231523394584656, 0.2935846447944641, 0.2782551646232605, 0.28290656208992004, 0.2780395448207855, 0.291721910238266, 0.29995429515838623, 0.2722789943218231, 0.2536408603191376, 0.29694756865501404, 0.2551093101501465, 0.2835845649242401, 0.2940133810043335, 0.1997319608926773, 0.25302931666374207, 0.2774125635623932, 0.27273204922676086, 0.27324745059013367, 0.2576708495616913, 0.2501888573169708, 0.2718457877635956, 0.2620941698551178, 0.26355236768722534, 0.27515435218811035, 0.22926576435565948, 0.2670731842517853, 0.27242860198020935, 0.21767526865005493, 0.25095507502555847, 0.2566837966442108, 0.24107469618320465, 0.23582229018211365, 0.25534680485725403, 0.2552034854888916, 0.19195939600467682, 0.23783493041992188, 0.23512201011180878, 0.2326553910970688, 0.24108679592609406, 0.2259172946214676, 0.23732608556747437, 0.24625717103481293, 0.2062917947769165, 0.23293623328208923, 0.252887487411499, 0.23166470229625702, 0.2139974683523178, 0.24113766849040985, 0.19958940148353577, 0.22323504090309143, 0.22428272664546967, 0.2320854514837265, 0.22557267546653748, 0.23286545276641846, 0.22345198690891266, 0.18489348888397217, 0.2282497137784958, 0.19176223874092102, 0.2126314640045166, 0.21552488207817078, 0.2259272038936615, 0.20603814721107483, 0.206589937210083, 0.21418176591396332, 0.1869981735944748, 0.22418782114982605, 0.21010953187942505, 0.1759023219347, 0.17491783201694489, 0.22274932265281677, 0.22871267795562744, 0.20433281362056732, 0.2062561810016632, 0.20290347933769226, 0.18925756216049194, 0.19696135818958282, 0.20378313958644867, 0.20914047956466675, 0.20463550090789795, 0.18590016663074493, 0.21082857251167297, 0.16940701007843018, 0.19756487011909485, 0.21499225497245789, 0.18274138867855072, 0.19824612140655518, 0.18506118655204773, 0.18771865963935852, 0.18494904041290283, 0.19884054362773895, 0.18757052719593048, 0.17549638450145721, 0.1636638641357422, 0.1858690232038498, 0.189229354262352, 0.17565910518169403, 0.18357720971107483, 0.16597802937030792, 0.16948340833187103, 0.18422815203666687, 0.16784141957759857, 0.1737602949142456, 0.18149861693382263, 0.15936264395713806, 0.16845540702342987, 0.17796416580677032, 0.17225655913352966, 0.1640242338180542, 0.18146848678588867, 0.17297470569610596, 0.17445848882198334, 0.1631404459476471, 0.16553883254528046, 0.180082768201828, 0.17100656032562256, 0.14987103641033173, 0.16405296325683594, 0.17776213586330414, 0.1739015132188797, 0.16017021238803864, 0.155472531914711, 0.18617528676986694, 0.14633987843990326, 0.15310314297676086, 0.16651807725429535, 0.16677719354629517, 0.1660425364971161, 0.16713133454322815, 0.14950186014175415, 0.15666408836841583, 0.15667909383773804, 0.16497962176799774, 0.1607225239276886, 0.15937697887420654, 0.16636434197425842, 0.14762301743030548, 0.15618908405303955, 0.15032680332660675, 0.1568259447813034, 0.17178979516029358, 0.1361159086227417, 0.14518506824970245, 0.16751031577587128, 0.15692085027694702, 0.16336853802204132, 0.1539064645767212, 0.1522621363401413, 0.15227442979812622, 0.13559019565582275, 0.14029286801815033, 0.16813863813877106, 0.1497715413570404, 0.14706240594387054, 0.16058282554149628, 0.14374741911888123, 0.14735469222068787, 0.15709541738033295, 0.14750359952449799, 0.15252387523651123, 0.14564794301986694, 0.14734594523906708, 0.15479421615600586, 0.14778517186641693, 0.1428467482328415, 0.14956071972846985, 0.16785000264644623, 0.13344228267669678, 0.1466757208108902, 0.13026010990142822, 0.14795881509780884, 0.15523022413253784, 0.1327589601278305, 0.1525304913520813, 0.13123422861099243, 0.14213043451309204, 0.15143446624279022, 0.14915631711483002, 0.15298672020435333, 0.14440421760082245, 0.14524012804031372, 0.13653527200222015, 0.1404501050710678, 0.1442861109972, 0.14543646574020386, 0.13664047420024872, 0.13912884891033173, 0.14773088693618774, 0.13340991735458374, 0.13664276897907257, 0.13508421182632446, 0.13714593648910522, 0.13991515338420868, 0.14414457976818085, 0.1491267830133438, 0.1341705024242401, 0.13608640432357788, 0.1452232301235199, 0.1327274888753891, 0.1337919980287552, 0.13768251240253448, 0.12381107360124588, 0.11877135932445526, 0.14120641350746155, 0.12875618040561676, 0.12538819015026093, 0.13943664729595184, 0.1435522884130478, 0.12070342898368835, 0.12142763286828995, 0.1299642026424408, 0.12004456669092178, 0.139370396733284, 0.14373672008514404, 0.11986219137907028, 0.13114291429519653, 0.15792547166347504, 0.12529774010181427, 0.11772092431783676, 0.12631775438785553, 0.1330944448709488, 0.12349163740873337, 0.11985258758068085, 0.134043350815773, 0.14288941025733948, 0.12693321704864502, 0.12659041583538055, 0.1358194500207901, 0.13423684239387512, 0.11519905179738998, 0.12927678227424622, 0.13004720211029053, 0.13415707647800446, 0.1393599957227707, 0.12361077964305878, 0.11563020199537277, 0.12203372269868851, 0.1302352249622345, 0.13389520347118378, 0.11929138749837875, 0.12321072816848755, 0.12824282050132751, 0.13059458136558533, 0.11774692684412003, 0.1264002025127411, 0.1229066252708435, 0.1271222084760666, 0.13161255419254303, 0.12759742140769958, 0.1093011274933815, 0.11792854964733124, 0.12010973691940308, 0.1247105747461319, 0.1275998204946518, 0.12790358066558838, 0.1181839108467102, 0.12043226510286331, 0.11364786326885223, 0.12857665121555328, 0.12600485980510712, 0.11966314166784286, 0.12222719192504883, 0.11219829320907593, 0.13119718432426453, 0.1312558352947235, 0.12174107134342194, 0.1147584393620491, 0.11267022043466568, 0.11205368489027023, 0.11944518238306046, 0.13442976772785187, 0.12238209694623947, 0.11193794757127762, 0.127968892455101, 0.118180051445961, 0.11350230127573013, 0.12079784274101257, 0.12540796399116516, 0.10479200631380081, 0.11208418011665344, 0.12406360357999802, 0.12103937566280365, 0.1174270510673523, 0.12915314733982086, 0.10076234489679337, 0.10570885241031647, 0.12489770352840424, 0.12988272309303284, 0.12418926507234573, 0.11147119849920273, 0.11066056042909622, 0.11531472951173782, 0.12537230551242828, 0.13048076629638672, 0.09832339733839035, 0.11268920451402664, 0.12268147617578506, 0.11905449628829956, 0.12374728173017502, 0.12108497321605682, 0.12772230803966522, 0.10787102580070496, 0.11199736595153809, 0.11353830993175507, 0.11926347762346268, 0.12198609113693237, 0.1236458420753479, 0.11542840301990509, 0.11260528117418289, 0.12771569192409515, 0.11962825059890747, 0.11661861836910248, 0.11571035534143448, 0.11149456351995468, 0.12478577345609665, 0.12644794583320618, 0.11532376706600189, 0.12095874547958374, 0.11111074686050415, 0.10945998132228851, 0.1147465705871582, 0.1178930252790451, 0.11676043272018433, 0.1136193722486496, 0.10830497741699219, 0.12486404180526733, 0.09744944423437119, 0.10243654251098633, 0.11627524346113205, 0.09569484740495682, 0.10889621078968048, 0.12055889517068863, 0.12459174543619156, 0.11054769903421402, 0.12016142159700394, 0.11247923225164413, 0.11758599430322647, 0.11703351140022278, 0.11168273538351059, 0.12090568244457245, 0.11675786226987839, 0.1035296693444252, 0.11274870485067368, 0.11394873261451721, 0.11757593601942062, 0.11245738714933395, 0.11433775722980499, 0.11605586856603622, 0.11701047420501709, 0.10404858738183975, 0.12359530478715897, 0.11086005717515945, 0.10515845566987991, 0.11718941479921341, 0.11547596007585526, 0.11394013464450836, 0.112583227455616, 0.10761169344186783, 0.11848459392786026, 0.11544687300920486, 0.1092589870095253, 0.1165684387087822, 0.09588822722434998, 0.10683389753103256, 0.10295185446739197, 0.11482111364603043, 0.12253537029027939, 0.118288055062294, 0.10954251140356064, 0.11202818900346756, 0.1209394633769989, 0.11613921076059341, 0.11480231583118439, 0.10496161878108978, 0.11205840855836868, 0.11612095683813095, 0.11392290145158768, 0.11463268101215363, 0.10701140016317368, 0.11948021501302719, 0.11028149724006653, 0.11114984005689621, 0.11870357394218445, 0.1087350845336914, 0.10950880497694016, 0.11586190015077591, 0.1151348352432251, 0.1042213886976242, 0.10858362913131714, 0.10727401822805405, 0.11278864741325378, 0.11616843938827515, 0.11334581673145294, 0.1179266944527626, 0.11416562646627426, 0.10413501411676407, 0.10923677682876587, 0.11550947278738022, 0.11638098210096359, 0.10182841122150421, 0.10340548306703568, 0.12331704795360565, 0.1188681423664093, 0.10987360030412674, 0.10745479166507721, 0.12127440422773361, 0.11945163458585739, 0.11771103739738464, 0.10802682489156723, 0.10872264206409454, 0.11385012418031693, 0.11716752499341965, 0.1201753318309784, 0.11039930582046509, 0.11845413595438004, 0.1078295037150383, 0.11017829179763794, 0.11284410208463669, 0.11517661809921265, 0.11480731517076492, 0.11046702414751053, 0.11523362249135971, 0.10869443416595459, 0.114530049264431, 0.11046427488327026, 0.11526206880807877, 0.109286829829216, 0.10347873717546463, 0.10968970507383347, 0.11696961522102356, 0.11585818231105804, 0.12229965627193451, 0.10084003210067749, 0.10101000964641571, 0.11128631234169006, 0.10568370670080185, 0.113382488489151, 0.11859286576509476, 0.10465923696756363, 0.10883205384016037, 0.11898676306009293, 0.10696161538362503, 0.11662910878658295, 0.11181408911943436, 0.10758611559867859, 0.11432898789644241, 0.12147440761327744, 0.10166400671005249, 0.11278770118951797, 0.10885467380285263, 0.11446516215801239, 0.1123930886387825, 0.11207742244005203, 0.10780759900808334, 0.10754990577697754, 0.11667440831661224, 0.11191575229167938, 0.11458100378513336, 0.11536487936973572, 0.11556638777256012, 0.10722237080335617, 0.11327871680259705, 0.11330387741327286, 0.1102621927857399, 0.11037234216928482, 0.1148533746600151, 0.11324244737625122, 0.09727704524993896, 0.10131802409887314, 0.10660427808761597, 0.1095045730471611, 0.11391393840312958, 0.11725126206874847, 0.11572588235139847, 0.10878212004899979, 0.10924538224935532, 0.11678355187177658, 0.10667864233255386, 0.10539820045232773, 0.1135048121213913, 0.10932371020317078, 0.09731365740299225, 0.10939722508192062, 0.11455093324184418, 0.11847314983606339, 0.1121782511472702, 0.10785132646560669, 0.1125229150056839, 0.10244269669055939, 0.10733862221240997, 0.10985289514064789, 0.11077845096588135, 0.11423990875482559, 0.10663904249668121, 0.10981127619743347, 0.10350051522254944, 0.11026543378829956, 0.11215859651565552, 0.11422324925661087, 0.1047414019703865, 0.10226525366306305, 0.10902148485183716, 0.10550548136234283, 0.11708163470029831, 0.1159047856926918, 0.10268274694681168, 0.1117640808224678, 0.10824547708034515, 0.10960108786821365, 0.10849052667617798, 0.11610158532857895, 0.1101977750658989, 0.09892624616622925, 0.10331179201602936, 0.11345850676298141, 0.09299655258655548, 0.10933912545442581, 0.11070609092712402, 0.10768818110227585, 0.09950486570596695, 0.10662374645471573, 0.11100446432828903, 0.10864204168319702, 0.10509266704320908, 0.10982134193181992, 0.1089828759431839, 0.10718244314193726, 0.10756517201662064, 0.10250230133533478, 0.10732446610927582, 0.10530116409063339, 0.10401910543441772, 0.10654344409704208, 0.10393130779266357, 0.10801836103200912, 0.10951332002878189, 0.10694581270217896, 0.10786225646734238, 0.11377399414777756, 0.10788775980472565, 0.08502621203660965, 0.09920240193605423, 0.10159658640623093, 0.09501868486404419, 0.10154695808887482, 0.10013537108898163, 0.11254486441612244, 0.09609435498714447, 0.10548950731754303, 0.11164113879203796, 0.10172903537750244, 0.10485512763261795, 0.10624094307422638, 0.11465297639369965, 0.10863567888736725, 0.10741966962814331, 0.10924826562404633, 0.11100015044212341, 0.1055675819516182, 0.10280420631170273, 0.10496369004249573, 0.10864801704883575, 0.10466988384723663, 0.11076899617910385, 0.10717672109603882, 0.10314387828111649, 0.1034284457564354, 0.10764449834823608, 0.10789969563484192, 0.1052115261554718, 0.09485919773578644, 0.10318661481142044, 0.10465431958436966, 0.10267402976751328, 0.10535526275634766, 0.09847866743803024, 0.09984581917524338, 0.09972542524337769, 0.10739146918058395, 0.11046849191188812, 0.09756419062614441, 0.102348692715168, 0.1027863398194313, 0.10142435133457184, 0.09759869426488876, 0.1038641706109047, 0.10528755187988281, 0.11265764385461807, 0.0934266746044159, 0.09821620583534241, 0.08981934934854507, 0.09610849618911743, 0.1033250018954277, 0.10437782853841782, 0.09958016872406006, 0.10277949273586273, 0.1061323806643486, 0.08398640155792236, 0.09334763884544373, 0.10284433513879776, 0.10840633511543274, 0.09899356961250305, 0.1020505428314209, 0.10701209306716919, 0.1034710630774498, 0.1024189218878746, 0.09914883226156235, 0.09736460447311401, 0.10227860510349274, 0.10392258316278458, 0.08771269023418427, 0.09017980098724365, 0.11121856421232224, 0.09323626011610031, 0.09546855837106705, 0.09842787683010101, 0.10264864563941956, 0.10183294862508774, 0.10207004100084305, 0.10355596989393234, 0.0951930582523346, 0.09788373857736588, 0.10247794538736343, 0.10620293766260147, 0.10286347568035126, 0.08444681018590927, 0.08601032197475433, 0.09521455317735672, 0.09693355858325958, 0.09380573034286499, 0.099248506128788, 0.10634523630142212, 0.10094945132732391, 0.09390435367822647, 0.09514355659484863, 0.09602035582065582, 0.09432941675186157, 0.10909829288721085, 0.09844623506069183, 0.09690207242965698, 0.0985456109046936, 0.09866449981927872, 0.09589167684316635, 0.102702796459198, 0.09942973405122757, 0.10097989439964294, 0.10103878378868103, 0.09913688153028488, 0.0989215224981308, 0.09815216809511185, 0.09752253443002701, 0.10038571804761887, 0.09582924842834473, 0.09605947881937027, 0.10325261950492859, 0.09539163112640381, 0.10026206821203232, 0.08780927211046219, 0.0948438048362732, 0.09460888057947159, 0.09890561550855637, 0.10045204311609268, 0.10279526561498642, 0.09606761485338211, 0.08898764103651047, 0.0930119976401329, 0.09752041101455688, 0.09554310142993927, 0.09965921193361282, 0.09826622158288956, 0.09511759877204895, 0.0968518927693367, 0.09662213176488876, 0.09992263466119766, 0.0905395895242691, 0.09582997113466263, 0.09849409013986588, 0.10095814615488052, 0.09876744449138641, 0.09718119353055954, 0.09525234997272491, 0.09596002101898193, 0.09726619720458984, 0.09857925027608871, 0.09567470103502274, 0.09261476248502731, 0.09390117973089218, 0.0974893644452095, 0.09600859135389328, 0.09657441824674606, 0.09677562862634659, 0.09009148180484772, 0.094697006046772, 0.09559748321771622, 0.09130445867776871, 0.08432044088840485, 0.08120610564947128, 0.09708935022354126, 0.10080789774656296, 0.0933295264840126, 0.08867309987545013, 0.09334353357553482, 0.0920010507106781, 0.09721951186656952, 0.08549576997756958, 0.08828224986791611, 0.08879809081554413, 0.09306103736162186, 0.08944513648748398, 0.0920524001121521, 0.09081190079450607, 0.09429658204317093, 0.0854480043053627, 0.087877057492733, 0.09779629111289978, 0.09213179349899292, 0.07840801775455475, 0.09244531393051147, 0.09767439216375351, 0.08706091344356537, 0.09743887186050415, 0.09217474609613419, 0.08733530342578888, 0.09448415040969849, 0.0898989737033844, 0.08668947964906693, 0.0897570550441742, 0.09284164011478424, 0.09292025864124298, 0.09265993535518646, 0.09275597333908081, 0.09145975112915039, 0.08841031044721603, 0.08963620662689209, 0.0925692617893219, 0.09235599637031555, 0.08293447643518448, 0.08744089305400848, 0.09041917324066162, 0.09234793484210968, 0.09127381443977356, 0.08366728574037552, 0.0860573947429657, 0.088264599442482, 0.09213583171367645, 0.09007654339075089, 0.09237868338823318, 0.08898258954286575, 0.08948585391044617, 0.08213900774717331, 0.0915704295039177, 0.09986329823732376, 0.09045702964067459, 0.09136396646499634, 0.08164860308170319, 0.08935539424419403, 0.08929288387298584, 0.09577502310276031, 0.09233026206493378, 0.08379363268613815, 0.09188878536224365, 0.08210714161396027, 0.09258207678794861, 0.09078747034072876, 0.09010446816682816, 0.08086191117763519, 0.08399192988872528, 0.0811421200633049, 0.07991785556077957, 0.09196251630783081, 0.08595268428325653, 0.08868738263845444, 0.09129197150468826, 0.09147854149341583, 0.090689517557621, 0.08622346073389053, 0.0920821875333786, 0.09328222274780273, 0.08208997547626495, 0.0812789797782898, 0.08737978339195251, 0.08732522279024124, 0.0960610881447792, 0.07908802479505539, 0.08201173692941666, 0.08661697804927826, 0.08542847633361816, 0.08352874219417572, 0.09132641553878784, 0.09399191290140152, 0.08510810136795044, 0.08770658075809479, 0.08297455310821533, 0.08649269491434097, 0.0868796557188034, 0.08778537064790726, 0.08924922347068787, 0.09416915476322174, 0.08674481511116028, 0.08360638469457626, 0.08579154312610626, 0.08982852101325989, 0.08412064611911774, 0.08608663082122803, 0.08298888802528381, 0.0796411782503128, 0.08710557222366333, 0.0819660872220993, 0.09023768454790115, 0.0856909230351448, 0.0841747522354126, 0.09029513597488403, 0.08661685883998871, 0.08390945196151733, 0.08771936595439911, 0.08058682084083557, 0.08620668202638626, 0.08889342099428177, 0.08062826097011566, 0.08133074641227722, 0.0924295112490654, 0.08853393793106079, 0.082662433385849, 0.08573651313781738, 0.07877497375011444, 0.08833872526884079, 0.08102770149707794, 0.08095642179250717, 0.08614402264356613, 0.09397610276937485, 0.08808594197034836, 0.08104100078344345, 0.0813266783952713, 0.07981488108634949, 0.08892416208982468, 0.09132659435272217, 0.07901281118392944, 0.08780251443386078, 0.08164750039577484]}],\n",
              "                        {\"height\": 500, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"width\": 700, \"xaxis\": {\"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"title\": {\"text\": \"Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1929b219-2484-459c-87df-f3d03b0f0de6');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_ZzB_1H3zhB"
      },
      "source": [
        "#Part 4: Evaluate model\n",
        "\n",
        "In order to properly assess if our model is capable of working on a real world scenario, we must then evaluate it using our test set. We do below by using the evaluate method along with the features and targets from the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWEui9IFH_x5"
      },
      "source": [
        "##4.1 Estimate MS and MA error\n",
        "\n",
        "Mean Square Error (MSE) and the Mean Absolute Error are computed using the X test set over the y test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IO13d9cieMjo",
        "outputId": "2425cd36-4765-49da-f061-3c6021d5e707"
      },
      "source": [
        "mse_nn, mae_nn = model.evaluate(X_test, y_test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 0s 2ms/step - loss: 0.1524 - mse: 0.0116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxEiCxJcH9TR"
      },
      "source": [
        "##4.2 Compute Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkSGdcGS3KX0"
      },
      "source": [
        "predictions = model.predict(X_test)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cdc1xdxJ5MB"
      },
      "source": [
        "predictions=np.around(predictions,0) \n",
        "predictions= predictions.astype(int) #one-hot representation uses binary\n",
        "#predictions"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VTBgR45NnFi",
        "outputId": "179431ef-acd5-4715-d376-b208df4e98c3"
      },
      "source": [
        "# Confusion matrix\n",
        "#sklearn.metrics.confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)\n",
        "confusion_matrix(np.argmax(y_test,axis=1),np.argmax(predictions,axis=1)) #decode one-hot encoding"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[57,  1,  0,  0],\n",
              "       [ 1, 26,  0,  0],\n",
              "       [ 0,  0, 28,  0],\n",
              "       [ 1,  0,  0, 25]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUy_Eyj6vA5-"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}